# Multiple Regression {#mlr}

This chapter covers material from chapters 9-12 of Sleuth. 

## The variables
Suppose we have a quantitative response variable $y$ that we want to relate to $p$ explantory variable (aka predictors, covariates) $x_1, \dotsc, x_p$. There is no restriction on the type of covariates, they can be both quantitative and categorical variables.


## The model form {#mlr-model}
This section describes the **multiple linear regression** (MLR) model for a particular **population** of interest. Another way to frame the model is that it describes a hypothetical **data generating process (DGP)** that was used to generate the sample of data that we have on hand. 

The major change in the MLR model compared to the SLR model is that now the **mean** function $\mu_{y\mid x_1, \dotsc, x_p}$ is a function of all covariates. The expression for $\mu$ must be linear with respect to the $\beta$ parameters even if  we used a function of a predictor like $\log(x)$. The expression for $\mu$ is  can even involve polynomial functions of $x$ like $x^2$ or interactions of predictors like $x_1 \times x_p$. In this section, we will describe the basic MLR with the simplest form it can take:
$$
\mu_{y\mid  x_1, \dotsc, x_p} = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \dotsm \beta_p x_{p,i}
$$
More complicated forms will be described later in this chapter. 

Let $Y_i$ be the response from unit $i$ that has explanatory (aka predictor) values $x_{1,i}, x_{2,i}, \dotsc, x_{p,i}$. There are two **equivalent** ways to express the SLR model for $Y$:

- Conditional normal model:
$$
Y_i \mid x_{1,i}, x_{2,i}, \dotsc, x_{p,i}\sim N(\mu_{y\mid x} = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \dotsm \beta_p x_{p,i}, \sigma)
$$

- Mean + error: 
\begin{equation}
Y_i = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \dotsm \beta_p x_{p,i} + \epsilon_i \ \ \ \ \ \epsilon_i \sim N(0, \sigma)
\end{equation}

**Both** expressions of the MLR model above say the same thing: 

- **Linear Mean:** $\mu_{y\mid x}$ describes the population mean value of $Y$ given all predictor values and it is linear with respect to the $\beta$ parametrs. (We still have a "linear" model even if we used $\log(x)$ or $x^2$!)
- **Constant SD:** $SD(Y\mid x)=\sigma$ describes the SD of $Y$'s in the population around a given mean value $\mu_{y\mid x}$. The fact that this SD **does not** depend on the value of $x$ is called the contant variance, or homoscedastic, assumption. 
- **Normality:** The shape of population response values around $\mu_{y\mid x}$ is described by a normal distribution model.  
- **Indepedence:** Given a predictor value of $x$, all responses $Y$ occur independently of each other. 

There are a total of **$\pmb{p+1}$ parameters** in this MLR model: 

- the $p$ mean parameters $\beta_0, \beta_1, \dotsc, \beta_p$ 
- the SD parameter $\sigma$

### Interpretation {#mlr-interpretation}

How a predictor influences the mean response is determined by the form of the $\mu$ function. Some common forms are discussed here.

#### Planar model
The mean model described above models the relationship between $y$ and all corvariates as a $(p+1)-$dimensional plane:
$$
\mu_{y\mid  x_1, \dotsc, x_p} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \dotsm \beta_p x_{p}
$$
When $p=1$, we have a SLR and this "plane" simplifies to a 2-d line. When $p=2$, the mean "surface" is a 3-D plane. The plot below displays an example of a sample of point triples $(x_{1,i}, x_{2,i}, y_i)$ that form a point "cloud" that is floating in the x-y-z coordinate system. The mean function is a plane that floats through the "middle" of the point cloud, hitting the mean value of $y$ for each combination of $x_1$ and $x_2$. Notice that when we "fix" one of the predictor values, the trend between $y$ and the other predictor is linear. E.g. Pick any place on the mean surface, then any place you "trace" along the surface will result in a line. 


```{r, echo=FALSE}
library(plotly)
set.seed(9282019)
x1 <- rnorm(100,0,1)
x2 <- rnorm(100,0,1)
y <- 1 + x1 + x2 + rnorm(100,0,.7)
my_df <- data.frame(x1,x2,y)
my_lm <- lm(y~x1+x2,my_df)
library(tidyr)
grid <- expand(data.frame(x1=seq(-2,2,by=1),x2=seq(-2,2,by=1)), x1,x2)
grid$yhat <- predict(my_lm, grid)
m <- matrix(grid$yhat, nrow = 5, ncol = 5)

my_plot <- plot_ly(my_df, 
                x = ~x1, 
                y = ~x2, 
                z = ~y, 
                type = "scatter3d", 
                mode = "markers", color=I("black"),opacity = .8 )

my_plot %>% add_surface(x = seq(-2,2,by=1), y = seq(-2,2,by=1), z = m, showscale = TRUE, type = "surface")
```

```{r, eval=FALSE, include=FALSE}
library(scatterplot3d)
s3d <- scatterplot3d(my_df$x1,my_df$x2, my_df$y, type = "p", color = "blue",
    angle=55, pch = 16)
# Add regression plane
s3d$plane3d(my_lm)
# Add supplementary points
s3d$points3d(seq(10, 20, 2), seq(85, 60, -5), seq(60, 10, -10),
    col = "red", type = "h", pch = 8)
```


How should we interpret the $\beta$'s in our planar mean function?

- $\beta_0$ is the mean response when all  predictor values are 0 since $\mu_{y \mid 0} = \beta_0 + \beta_1(0)  + \dotsm + \beta_p (0)= \beta_0$.
- $\beta_j$ tells us how the mean response changes for a one unit increase in $x_j$ **holding all other predictors fixed**. We can illustrate this for the the predictor $x_1$. The mean function at $x_1 + 1$, holding $x_2, \dotsc, x_p$ fixed, is
$$
\begin{split}
\mu(y \mid  x_1+1, x_2, \dotsc, x_p) &= \beta_0 + \beta_1 (x_{1}+1) + \beta_2 x_{2} + \dotsm + \beta_p x_p \\
& = \beta_0 + \beta_1 x_{1} + \beta_1 + \beta_2 x_{2}  + \dotsm + \beta_p x_p \\
& = \mu(y \mid  x_1, x_2, \dotsc, x_p) + \beta_1 
\end{split}
$$
This shows that a 1 unit increase in $x_1$ is associated with a $\beta_1$ change in the mean response **holding all other predictors fixed**. This holds in general too: a 1 unit increase in $x_j$ is associated with a $\beta_j$ change in the mean response **holding all other predictors fixed**.

#### Quadratic model
A model that incorporates *polynomial* functions of predictors, like $x^2, x^3$, etc,  is also a MLR model. Here is an example that says the mean of $y$ is a quadratic function of $x_1$ but a linear function of $x_2$:
$$
\mu_{y\mid  x_1, x_2} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{1}^2 + \beta_3 x_{2}
$$
Notice that this model has **two** covariates $x_1$ and $x_2$ but **four** mean function parameters $\beta_0 - \beta_4$ due to the extra quadratic term. An example of this model is visualized below (with all $\beta$'s equal to 1). You see the quadratic relationship with $x_1$ when you orient the $x_1$ axis to be the left-to-right axes with the $x_2$ axis coming out of the page. When these axes are flipped, you see a linear relationship. 


```{r, echo=FALSE}
library(plotly)
set.seed(92812209)
x1 <- rnorm(100,0,1)
x2 <- rnorm(100,0,1)
y <- 1 + x1 + x1^2 + x2 + rnorm(100,0,.7)
my_df <- data.frame(x1,x2,y)
my_lm <- lm(y~x1+x2 + I(x1^2),my_df)
library(tidyr)
grid <- expand(data.frame(x1=seq(-2,2,by=1),x2=seq(-2,2,by=1)), x1,x2)
grid$yhat <- predict(my_lm, grid)
m <- matrix(grid$yhat, nrow = 5, ncol = 5)

my_plot <- plot_ly(my_df, 
                x = ~x1, 
                y = ~x2, 
                z = ~y, 
                type = "scatter3d", 
                mode = "markers", color=I("black"),opacity = .8 )

my_plot %>% add_surface(x = seq(-2,2,by=1), y = seq(-2,2,by=1), z = m, showscale = TRUE, type = "surface")
```

How should we interpret the $\beta$'s in this quadratic mean function?

- $\beta_0$ is the mean response when all  predictor values are 0.
- $\beta_3$ tells us how the mean response changes for a one unit increase in $x_2$ **holding $\pmb{x_1}$ fixed**. 
- $\beta_1$ and $\beta_2$ tell us how the mean response changes as a function of $x_1$, but since it is quadratic the exact change in the mean response depends on the value of $x_1$. E.g. the closer the $x_1$ value is to the "top" or "bottom" of the quadratic curve, the smaller the changes in the mean response. We can illustrate this for the the predictor $x_1$. The mean function at $x_1 + 1$, holding $x_2$ fixed, is
$$
\begin{split}
\mu(y \mid  x_1+1, x_2) &= \beta_0 + \beta_1 (x_{1}+1)+ \beta_2 (x_{1}+1)^2 + \beta_3 x_{2}  \\
& = \beta_0 + \beta_1 x_{1} + \beta_1 + \beta_2 x_{1}^2 + \beta_2 2x_1 + \beta_2  + \beta_3 x_2 \\
& = \mu(y \mid  x_1, x_2) + \beta_1 + \beta_2(2x_1+1) 
\end{split}
$$
This shows that a 1 unit increase in $x_1$ is associated with a $\beta_1 + \beta_2(2x_1+1)$ change in the mean response **holding all other predictors fixed**.  For example, if $x_1 = 1$ the mean change is $\beta_1 + 3\beta_2$.

#### Interactions 
A model that incorporates predictor *interactions* says that the effect of one predictor is dependent on the value of the other predictor, and vica versa.  Here is an example that has the interaction of $x_1$ and $x_2$:
$$
\mu_{y\mid  x_1, x_2} = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + \beta_3 x_1x_{2}
$$
An example of this model is visualized below (with all $\beta$'s equal to 1). We obviously don't see a planar surface. It's a bit hard to see, but any "slice" you take from the surface along the $x_1$ axis will create a linear function along the $x_2$ axis. The trend, or steepness, of this line depends on the value you chose for $x_1$. Switching the $x_1$ and $x_2$ variables results in the same observation. The effect of each predictor is linear, but its slope, or *effect size*, is a function of the other predictor. 


```{r, echo=FALSE}
library(plotly)
set.seed(92812209)
x1 <- rnorm(100,0,1)
x2 <- rnorm(100,0,1)
y <- 1 + x1 + x1*x2 + x2 + rnorm(100,0,.7)
my_df <- data.frame(x1,x2,y)
my_lm <- lm(y~x1*x2 ,my_df)
library(tidyr)
grid <- expand(data.frame(x1=seq(-2,2,by=1),x2=seq(-2,2,by=1)), x1,x2)
grid$yhat <- predict(my_lm, grid)
m <- matrix(grid$yhat, nrow = 5, ncol = 5)

my_plot <- plot_ly(my_df, 
                x = ~x1, 
                y = ~x2, 
                z = ~y, 
                type = "scatter3d", 
                mode = "markers", color=I("black"),opacity = .8 )

my_plot %>% add_surface(x = seq(-2,2,by=1), y = seq(-2,2,by=1), z = m, showscale = TRUE, type = "surface")
```


How should we interpret the $\beta$'s in this interaction mean function?

- $\beta_0$ is the mean response when all  predictor values are 0.
- $\beta_1$ tells us how the mean response changes for a one unit increase in $x_1$ **when $\pmb{x_2=0}$**. 
- $\beta_2$ tells us how the mean response changes for a one unit increase in $x_2$ **when $\pmb{x_1=0}$**. 
- $\beta_3$  is the interaction effect that tells us how the effect of $x_1$ varies as a function of $x_2$, and vice versa.  We can illustrate this for the the predictor $x_1$. The mean function at $x_1 + 1$, holding $x_2$ fixed, is
$$
\begin{split}
\mu(y \mid  x_1+1, x_2) &= \beta_0 + \beta_1 (x_{1}+1)+ \beta_2 x_2+ \beta_3 (x_1+1)x_{2}  \\
& = \beta_0 + \beta_1 x_{1} + \beta_1 + \beta_2 x_{2} + \beta_3x_1x_2 + \beta_3x_2 \\
& = \mu(y \mid  x_1, x_2) + \beta_1 + \beta_3x_2 
\end{split}
$$
This shows that a 1 unit increase in $x_1$ is associated with a $\beta_1 + \beta_3x_2$ change in the mean response **holding all other predictors fixed**.  For example, if $x_2 = 5$ the mean change is $\beta_1 + 5\beta_3$. Similary, a 1 unit increase in $x_2$ is associated with a $\beta_2 + \beta_3x_1$ change in the mean response **holding all other predictors fixed**. 






## Example: MLR fit and visuals {#mlr-example1}


### `lm` fit
We fit a MLR model in R using the same command as a SLR model, but we add model predictors on the right-hand side of the formula. Examples include:

- planar model: `lm(y ~ x1 + x2 + x3, data=, subset=)`
- interaction model: `lm(y ~ x1 + x2 + x1:x2 + x3, data=, subset=)` or `lm(y ~ x1*x2 + x3, data=, subset=)` 
- quadratic model: `lm(y ~ x1 + I(x1^2) + x2, data=, subset=)`. This uses the "as is" operator `I()` that tells R that `^` is interpreted as a power rather than as its symbolic use in a formula (see `?formula` for more details) 

Here is the multiple linear regression of brain weight (g) on gestation (days), body size (kg) and litter size from **Case Study 9.2**:
```{r}
library(Sleuth3)
brain <- case0902
brain.lm <- lm(Brain ~ Gestation + Body + Litter, data=brain )
brain.lm
```

The estimated mean function is 
$$
\hat{\mu}(brain \mid gest,body,litter) =  -225.2921    +   1.8087 Gestation     + 0.9859  Body   + 27.6486  Litter
$$
Holding gestation length and body weight fixed, increasing litter size by one baby increases estimated mean brain weight by 27.6g. But is this an appropriate model to use? **This interpretation is meaningless if the model doesn't fit the data!** We need to check this with scatterplots and residual plots.

### Graphics for MLR 
If we have $p$ predictors in our model, then the MLR model can be viewed in a (at least) $p$-dimensional picture! Viewing this is difficult, if not impossible. The best we can do is look at 2-d scatterplots of $y$ vs. all the predictor variables. (But, unfortunately, what we see in these 2-d graphs doesn't always explain to us what we will "see" in the MLR model.)

#### Scatterplot matrix
A scatterplot matrix plots all pairs of variables used in a model. The primary plots of interest will have the response $y$ on the y-axis and the predictors on the x-axis. But the predictor plots (e.g. $x_1$ vs. $x_2$) are useful to see if any predictors are related, which is a topic dicussed in more detail later in these notes. 

The basic scatterplot matrix command `pairs` takes in a data frame, minus any columns you don't want plotted:
```{r}
names(brain)
# we want to omit column 1 (Species) from our graph:
pairs(brain[,-1])
```

The top row shows scatterplots of the response `Brain` vs. the three predictors. All three indicate that transformations of all variables should be explored. 

A slightly nicer version that includes univariate density curves and correlation coefficients is made using `ggpairs` in the `GGally` package. This option fits a smoother curve to the scatterplots: 
```{r}
library(GGally)
ggpairs(brain, columns = c("Body","Gestation", "Litter","Brain"), 
        lower = list(continuous = "smooth"))
```

In this plot command, we select variables from `brain` in the `columns` argument with the response `Brain` listed **last** to make the lower row of the plot show the response `Brain` on the y-axis vs. all three predictors on the x-axes. 

Conclusion: transformations are likely needed. Since all variables have positive values, we can try logs first. 

We can't use a `scale` argument in a scatterplot matrix to explore transformations. Instead, we can use the `dplyr` package to transform all variables (except `Species`) using the `mutate_all` function, then we plot using `ggpairs` again. Here we also added the `ggpairs` argument `columnLabels` to remind us what transformations were made:
```{r}
library(dplyr)
brain %>% select(-Species) %>%  # omit species
  mutate_all(.funs = log) %>%   # apply the log function to all variables
  ggpairs(columns = c("Body","Gestation", "Litter","Brain"),   
          lower = list(continuous = "smooth"), 
          columnLabels = c("log(Body)","log(Gestation)", "log(Litter)","log(Brain)"))
```

Does `Litter` need to be logged? If we don't want to log all variables, we can use `mutate_at`  instead of `mutate_all`:
```{r}
brain %>% select(-Species) %>%  # omit species
  mutate_at(.vars = c("Brain","Body","Gestation"), .funs = log) %>%  # pick variables to log
  ggpairs(columns = c("Body","Gestation", "Litter","Brain"), 
          lower = list(continuous = "smooth"), 
          columnLabels = c("log(Body)","log(Gestation)", "Litter","log(Brain)"))
```


#### Jittered scatterplot: 
Jittered plots are useful when data points overlap (discrete variables like litter) and your sample size isn't huge. Change the `alpha` transparency value with large data sets. Here we compare `geom_point()` against `geom_jitter`, using `grid.arrange` from the `gridExtra` package to put these plots side-by-side:

```{r}
base <- ggplot(brain, aes(x=Litter, y=Brain)) + scale_y_log10()
plotA <- base + geom_point() + labs(title="unjittered")
plotB <- base + geom_jitter(width=.1) + labs(title="jittered")
library(gridExtra)
grid.arrange(plotA,plotB, nrow=1)
```

The `width=.1` amount specifies how much to jitter the points (we changed it here because the default amount didn't display enough of a change). We can see the jittering most in the points clustered around `Litter=0`. 




### Residual plots for MLR
If we have $p$ predictors, then there are $p+1$ residual plots to check:

- plot $r_i$ against all predictors $x_1, \dotsc, _p$. The motivation for these plots is the same as SLR (residuals should not be related to the $x$'s)
- plot $r_i$ against the *fitted* values $\hat{y}_i$. The motivation for this may be less clear, but the fitted values are simply a linear function of the predictor values:
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1,i}  + \hat{\beta}_2 x_{2,i} + \dotsm  + \hat{\beta}_p x_{p,i}
$$
so the residuals should not be related to the fitted values if the model fits.

All $p+1$ plots should be checked. A quick starting point is the fitted value plot which is the first plot when `plot`-ing the `lm`. Here is the residuals vs. fitted plot for the untransformed variable model:
```{r, fig.width=4,fig.height=4}
# check residuals vs. fitted (y-hat) values:
plot(brain.lm, which=1)
```

This plot indicates non-linearity and non-constant variance. Here is the same residual plot for the model will all variables logged except `Litter`:
```{r, fig.width=4,fig.height=4}
brain.lm2 <- lm(log(Brain) ~ log(Gestation) + log(Body) + Litter, data=brain)
plot(brain.lm2, which=1)
```

We can use the `ggnostic` function in `GGally` to get plots of residuals vs. all predictors:
```{r}
ggnostic(brain.lm2, columnsY = ".resid")  # from GGally
```

The residual plots reveal some outliers that should be explored but the overall fit, while not perfect, is much better than the untransformed version.

```{r}
library(broom)
library(knitr)
kable(tidy(brain.lm2), digits=4)
```
The estimated median function for this model is
$$
\widehat{median}(y \mid x) = e^{0.8234}(Gest)^{0.4396}(Body)^{0.5745}e^{-0.1104(Litter)} 
$$

The effect of `Gestation` is interpreted as you would in a **power** model (both variables logged). For example, doubling gestational days is associated with an estimated $(2^{0.4396}-1)100%=$35.6% increase in median brain weight, holding body weight and litter size constant. 

The effect of `Litter` is interpreted as you would in an **exponential** model (with just the response logged). For example, after controlling for body size and gestation time, each additional offspring decreases estimated median brain weight by 10.5% (work is $(e^{-0.1104}-1)100%=$).

### EDA for interactions 
We visualize interactions by using a graphic that looks at the relationship between $y$ and $x_1$ while holding the value of $x_2$ fixed (or almost fixed), and vice versa for flipping the role of $x$ variables. An interaction may be needed if the relationship between $y$ and $x_1$ depends on the value of $x_2$.

#### Predictors: one quantitative and one categorical
For example, if $x_1$ is quantitative and $x_2$ is categorical, then we use `facet_wrap` (or `facet_grid`) to split the scatterplot of $y$ vs. $x_1$ by group of $x_2$:
```{r, eval=FALSE}
ggplot(mydata, aes(x=x1, y=y)) + 
  geom_point() + 
  facet_wrap(~x2)
```

#### Predictors: both quantitative
We use the same `facet_wrap` function, but we need to group the data into similar cases with respect to their $x_2$ value, which is now assumed to be quantitative. One way to do this is to use the `ntile(n=)` function from `dplyr` package where `n` determines how many groups the data will be divided into. 

Is there an interaction between body weight and gestation?? **Does the relationship between brain weight and gestation length change as we vary body weight?** If yes, then we should include an interaction term between gestation and body weight.

Here we hold body weight "fixed" by using the `ntile` command from the `dplyr` package to divide the data into chunks of animals with similar body weights. Here we pick `n=4` for this function which divides the cases into 4 equal sized chunks based on the quartiles (4) of `Body`. In the plot below the cases in "1" are the lower 25% of body weights, "2" are the 25-50th percentile values of Body weights, etc. 

```{r}
ggplot(brain, aes(x=Gestation, y=Brain)) + 
  geom_point() + geom_smooth(method="lm", se=FALSE) +
  scale_x_log10() + scale_y_log10() + 
  facet_wrap(~ ntile(Body, n=4))
```

Conclusion: the trend within each level of body weight is about the same. No obvious interactive effect of body weight and gestation length on brain weight. Of course we can always check significance by adding interaction term to model:

```{r}
brain.lm3 <- lm(log(Brain) ~ log(Gestation) + log(Body) + Litter + log(Gestation):log(Body), data=brain)
summary(brain.lm3)
```


### Quadratic models: Corn yields (exercise 9.15)
This final example illustrates a quadratic model fit and scatterplot. Consider the corn yield data in textbook exercise 9.15. How is corn yield (measured bushels/acre) in a year related to the amount of rainfall (inches) in that summer?

A linear model for yield against rainfall is not appropriate:
```{r}
corn <- ex0915
summary(corn)
ggplot(corn, aes(x=Rainfall, y=Yield)) + 
  geom_point() + 
  geom_smooth(method="lm", se=FALSE) + 
  labs(title="SLR model")
corn.lm1<-lm(Yield ~ Rainfall, data =corn)
plot(corn.lm1, which=1)
```

We can add a quadratic term (using the `I()` operator):
```{r}
corn.lm2 <- lm(Yield ~ Rainfall + I(Rainfall^2), data =corn)
```

Alternatively, we can update model 1 (SLR) using the `update` command:  on right side of formula the  `~ . + newstuff` says to add the `newstuff` to the old model formula which is denoted with the period `.` .
```{r}
corn.lm2 <- update(corn.lm1, ~ . + I(Rainfall^2))
summary(corn.lm2) 
plot(corn.lm2, which=1)
```

This residual plot looks much better for this quadratic model compared to the linear model. 

We can visualize this quadratic model using the `geom_smooth(method="lm")` function but we have to specify this model form since the default is a SLR. This is done in the `formula` argument, using `y` and `x` to denote the `x` and `y` that you specify in the `aes` argument. 
```{r}
ggplot(corn, aes(x=Rainfall, y=Yield)) + 
  geom_point() + 
  geom_smooth(method="lm", formula= y ~ x + I(x^2),  se=FALSE) + 
  labs(title="quadratic fit!")
```

The model fit is 
```{r}
kable(tidy(corn.lm2), digits = 4)
```
so 
$$
\hat{mu}(yield \mid rain) = -5.0147 + 6.0043(rain)- 0.2294(rain)^2
$$
An increase from 9 to 10 inches of rainfall is associated with a mean yield increase of 1.646 bushels per acre. 
$$
6.0043- 0.2294(2\times 9 + 1) = `r round(6.0043- 0.2294*(2*9 + 1),3)`
$$
An increase from 14 to 15 inches of rainfall is associated with a mean yield decrease of 0.648 bushels per acre. 
$$
6.0043- 0.2294(2\times 14 + 1) = `r round(6.0043- 0.2294*(2*14 + 1),3)`
$$

