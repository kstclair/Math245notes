# Logistic Regression {#logistic}

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
knitr::opts_chunk$set(echo = TRUE, eval=TRUE, prompt=TRUE, collapse = TRUE, message = FALSE, warning = FALSE)
```


This chapter covers material from chapters ?? of Sleuth. 

## The variables
Suppose we have a **categorical** response variable $Y$ that can take one of two values, which we will generically call a **success** or **failure**. We want to relate the **probability of success** to $p$ explantory variables (aka predictors, covariates) $x_1, \dotsc, x_p$. There is no restriction on the type of covariates, they can be both quantitative and categorical variables.

## The Bernoulli distribution {#logistic-bernoulli}

The Bernoulli distribution is a probability model for a random trial that has two possible outcomes: success or failure. A Bernoulli **random variable** $Y$ "counts" the number of successes in a Bernoulli random trial. If a "success" occurred then $Y=1$ and if a "failure" occurred then $Y=0$. 

We will let $\pi$ be the probability of success:
$$
\pi = P(Y=1) = P(success), \ \ \ \ \ 1-\pi = P(Y=0) = P(failure) 
$$
If $Y$ is a Bernoulli random variable, then we can use the shorthand notation $Y \sim Bern(\pi)$ to denote this. 

The **expected value**, or mean, of $Y$ is equal to 
$$
E(Y) = \mu = \pi
$$
and the standard deviation of $Y$ is equal to 
$$
SD(Y) = \sigma = \sqrt{\pi(1-\pi)}
$$
The expected value (or mean) of a random variable measures the "long run" average value that we would see from $Y$ if we were to repeat the random trial many, many times. The standard deviation tells us how these values of $Y$ will vary over these repeated trials. 



## The logistic model form {#logistic-model}

The population, or data generating, model for a **logistic** regression model for $Y$ assumes that each $Y_i$ is a Bernoulli random variable whose probability of success **depends on covariates** $\pmb{x_{1,i}, \dotsc, x_{p,i}}$. Specifically,

- $Y_i \mid X_i \overset{indep.}{\sim} Bern(\pi(X_i))$
    - binary response: $Y_i$'s are categorical with only two options
    - independence: Given $X_i$ values, $Y_i$'s are independent

We connect the linear combination of predictors 
$$
\eta_i = \beta_0 + \beta_1 x_{1,i} + \dotsm +  \beta_p x_{p,i}
$$
to the probability of success using the logistic function form:
$$
\pi(X_i) = \dfrac{e^{\eta_i}}{1 + e^{\eta_i}} = \dfrac{e^{\beta_0 + \beta_1 x_{1,i} + \dotsm +  \beta_p x_{p,i}}}{1 + e^{\beta_0 + \beta_1 x_{1,i} + \dotsm +  \beta_p x_{p,i}}}
$$

This function form is used because its inverse is equal to 
$$
\eta_i =  \beta_0 + \beta_1 x_{1,i} + \dotsm +  \beta_p x_{p,i}=  \ln \left(  \dfrac{\pi(X_i)}{1-\pi(X_i)}\right)
$$
This function is called the **logit function**: $logit(\pi) = \ln \left(  \dfrac{\pi}{1-\pi}\right)$. This  means that a one unit increase in $x_1$ can be interpreted as an additive  $\beta_1$ change in in the logit function, holding other terms fixed.  But what does this mean?

The **odds of success** is defined as the ratio of the probability of success to the probability of failure:
$$
odds = \dfrac{\pi(X)}{1-\pi(X)}
$$
For example, if the probability of success is 0.6 then the odds of success is $0.6/0.4 = 1.5$. Meaning for every 6 successes, we see 4 failures. If the probability of success is 0.1, then the odds of success is $0.1/0.9 \approx 0.111$, meaning for every 1 success we see 9 failures. Odds greater than 1 indicate the probability of success is above 50% while odds less than 1 indicate the probability of success is less than 50%.

So, we now can see that the logit function equals the **log-odds of success**:
$$
\ln \left(  \dfrac{\pi(X_i)}{1-\pi(X_i)}\right) = \beta_0 + \beta_1 x_{1,i} + \dotsm +  \beta_p x_{p,i}
$$
This model form is an example of a **generalized linear model** which relates the response $Y$ to predictors through a linear combination $\eta$ of predictors. Is does this by defining the following functions:

- The **kernel mean function** defines the expected value (mean) of $Y$ as a function of $\eta$.
    - in a logistic model, the kernel mean function is the logistic function $E(Y \mid X) = \pi(X) = \dfrac{e^{\eta}}{1+e^{\eta}}$
- The **link function** defines the linear combination $\eta$ as a function of the mean of $Y$.
    - in a logistic model, the link function is the logit function $\eta = \ln(\pi/(1-\pi))$
- These two functions are inverses of one another. 


```{r, fig.height=4, fig.width=9, echo=FALSE}
par(mfrow=c(1,2))
curve(exp(x)/(1+exp(x)), from=-10, to=10, ylab="logistic(eta)", xlab="eta")
title("logistic")
curve(log(x/(1-x)), from=0, to=1,  xlab="p", ylab="logit(p)")
title("logit")
```

### Interpretation
Changes in predictors can be interpreted as changes in the **odds** of success (we can't make general statement about changes in the **probability** of success). Specifically, we  "unlog" the logit equation to get an expression for the odds of success for predictors $x_1, \dotsc, x_p$:
$$
odds(x_1, \dotsc, x_p) = \dfrac{\pi(X)}{1-\pi(X)} = e^{\beta_0 + \beta_1 x_{1} + \dotsm +  \beta_p x_{p}}
$$

What happens if we increase $x_1$ by one unit, holding other predictors fixed? 
$$
odds(x_1+1, \dotsc, x_p)  = e^{\beta_0 + \beta_1 (x_{1}+1) + \dotsm +  \beta_p x_{p}} = e^{\beta_0 + \beta_1 x_{1} + \dotsm +  \beta_p x_{p}} \times e^{\beta_1}
$$
Increasing $x_1$ by one unit has a **multiplicative** change of  $e^{\beta_1}$ in the **odds of success**. Note that this is a similar interpretation to the exponential model in SLR or MLR. 

What is we have a predictor that is logged? 
$$
odds(x_1, \dotsc, x_p) = e^{\beta_0 + \beta_1 \ln(x_{1}) + \dotsm +  \beta_p x_{p}} =   e^{\beta_0}x_1^{\beta_1} e^{\beta_2 x_2 + \dotsm +  \beta_p x_{p}}
$$

Then our interpretation is similar to a power model. Changing $x_1$ by a factor of $m$:
$$
odds(mx_1, \dotsc, x_p) =    e^{\beta_0}(mx_1)^{\beta_1} e^{\beta_2 x_2 + \dotsm +  \beta_p x_{p}} =    e^{\beta_0}x_1^{\beta_1} e^{\beta_2 x_2 + \dotsm +  \beta_p x_{p}} \times m^{\beta_1}
$$
results in a multiplicative change of $m^{\beta_1}$ in the odds of success.