[
["index.html", "Math 245 Notes Getting Started Getting help", " Math 245 Notes Katie St. Clair, Carleton College 2020-08-19 Getting Started This manual is designed to provide students in Math 245 (Applied Regression Analysis) with supplemental notes and examples. These notes are not intended to replace notes that you take by hand or computer during class or when reading assigned material. Most examples/activities will have an .Rmd available that you can download to use as a template for completing the activity. Save these files to your class project folder. Getting help Try to start your homework or projects early enough to seek help from me! You can do this in a variety of ways: Office visit: Stop by office hours or drop by anytime my door is open. This is the easiest way for me to answer any class questions that you have or help you debug troublesome R code. Email: I can usually clarify any homework questions over email. If you are running in an R problem, you should snapshot your R code and error and send it to me via email. I can often figure out your code issue this way! Stats Lab: Visit the stats lab assistants in CMC 201 for help using R. "],
["review.html", "Chapter 1 Review of Statistical Inference 1.1 Sampling Distribution 1.2 Central Limit Theorem 1.3 Hypothesis testing 1.4 Confidence Intervals 1.5 Review activity (day 2)", " Chapter 1 Review of Statistical Inference 1.1 Sampling Distribution Suppose we take a random sample of size \\(n\\) from a large population of individuals. We record a variable and compute a statistic like a sample mean or sample proportion from this data. The sampling distribution of the statistic describes how the stat is distributed over many random samples: take a random sample of size \\(n\\) record the variable of interest (data) for the \\(n\\) sampled cases compute the statistic from the data repeat 1-3 many, many times plot the distribution of your statistics from step 4. 1.2 Central Limit Theorem Under the right conditions, the distribution of many sample means \\(\\bar{x}\\) or proportions \\(\\hat{p}\\) will look like a normal distribution that is centered at the true population parameter (mean \\(\\mu\\) or proportion \\(p\\)) value. Shape: normally distributed (“bell-shaped”) Center: true population parameter value Variation: called the standard error of the statistic which measures the standard deviation of your statistic over many samples The conditions for “normality” (symmetry) have to do with both the sample sized and the distribution of your variable. If your measurements are very symmetric (e.g. heights of woman in an adult population) then the sample size \\(n\\) can be very small and the sample mean will behave normally. As your measurements get more skewed (e.g. income per person in a large city), then the sample size \\(n\\) needs to get larger for the sample mean to behave normally. Outliers are always a problem, even if you have a large sample size, since means can be easily influenced by one or two very unusual cases. 1.2.1 Standard error The standard error of the sample mean \\(\\bar{x}\\) measures the variability of this statistic and is the standard deviation of the sampling distribution for \\(\\bar{x}\\). Roughly, the SE of any statistic tells us how it will vary from sample to sample. For the sample mean statistic, the estimated SE is equal to \\[ SE(\\bar{x}) = \\dfrac{s}{\\sqrt{n}} \\] where \\(s\\) is the sample standard deviation of your variable. For the difference of two sample means, \\(\\bar{x}_1 - \\bar{x}_2\\), the standard error is \\[ SE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\dfrac{s^2_1}{n_1} + \\dfrac{s^2_2}{n_2}} \\] where \\(n_i\\) is the sample size of sample \\(i\\) and \\(s_i\\) is the sample SD of population \\(i\\). For the sample proportion \\(\\hat{p}\\), the standard error is \\[ SE(\\hat{p}) = \\sqrt{\\dfrac{p(1-p)}{n}} \\] where \\(p\\) is the true population proportion. To estimate this SE we just use the sample proportion \\(\\hat{p}\\) in the SE formula. 1.3 Hypothesis testing Your null hypothesis \\(H_0\\) is a specific claim about a population parameter of interest. Examples include: \\(H_0: p = 0.5\\) (population proportion is 0.50, or 50%) \\(H_0: \\mu_1 - \\mu_2 = 0\\) (the mean of population 1 \\(\\mu_1\\) is equal to the mean of population 2 \\(\\mu_2\\)) The alternative hypothesis specifies another, more general, scenario for the population(s). Examples include: \\(H_0: p &gt; 0.5\\) (population proportion greater than 0.50) \\(H_0: \\mu_1 - \\mu_2 \\neq 0\\) (the two populations have difference means) You then use a test statistic to measure how far the data (e.g. sample statistic) deviates from what is expected assuming the null is true. Often these test stats take form of a standardized values so large absolute values indicate data that deviates a lot from what is expected if the null is true. For a hypothesis about one population mean \\(H_0: \\mu = \\mu_0\\), we use a t-test statistic: \\[ t = \\dfrac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\] To test for a difference in two means, we use another t-test stat: \\[ t = \\dfrac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\dfrac{s^2_1}{n_1} + \\dfrac{s^2_2}{n_2}}} \\] We then construct the sampling distribution of the test statistic under the assumption that the null hypothesis is true. Using this model, we compute a p-value by finding the probability of getting a test stat as, or more, extreme as the observed test stat. The p-value measures the probability of observing data as unusual, or more usual, than the observed data if the null is true. Small p-values indicate data that would rarely be seen if the null is true which means we have evidence (data) that supports the alternative hypothesis. If we use a t-test statistic, then we use a t-distribution to compute a p-value. If t is the test stat value then either command below will give a one-sided (\\(&lt;\\) or \\(&gt;\\)) p-value: &gt; pt(abs(t), df = , lower.tail = FALSE) # method 1 (gives right tail value) &gt; pt(-abs(t), df = ) # method 2 (gives left tail value) If your alternative is two-sided, you need to double the value given by either command above. The degrees of freedom depends on the type of estimate: for a one-sample mean problem, use \\(df = n-1\\) for a two-sample difference in means problem, let technology get the value! 1.4 Confidence Intervals A confidence interval for a population parameter gives us a range of likely values of the parameter. Most confidence intervals we use in this class are of the form: \\[ \\textrm{estimate} \\pm \\textrm{ margin of error} \\] The idea behind constructing a confidence interval starts with our estimate’s (statistic’s) sampling distribution and margin of error for some level of confidence: The sampling distribution tells us how an estimate varies from sample to sample around the true population parameter. The margin of error for a “C%” confidence interval is computed so that the estimate will be within the margin of error of the true parameter value C% of the time. Another “confidence” interpretation: of all possible samples, C% will yield a confidence interval (using the C% margin of error) that contains the true parameter value. Examples: E.g. for 95% confidence: 95% of all possible samples will give an estimate that is within the (95% level) margin of error of the truth. E.g. for 90% confidence: 90% of all possible samples will give 90% confidence interval that contains the truth. Normally distributed estimates: when a sampling distribution is roughly normally distributed, we can approximately say that 95% margin of error \\(\\approx 1.96 \\times SE\\) 90% margin of error \\(\\approx 1.65 \\times SE\\) 99% margin of error \\(\\approx 2.58 \\times SE\\) A higher level of confidence will lead to a larger margin of error: We need a larger margin of error to get a higher fraction of samples that close to the population parameter. The margin of errors given above are ballpark values. We will mostly be using a more trustworthy distribution in our class, the t-distribution, to compute how many SE’s we go to be a certain level of confidence: \\[ \\textrm{margin of error } = t^*_{df} \\times SE \\] where \\(t^*\\) is the \\((100-C)/2\\) percentile from the t-distribution with \\(df\\) degrees of freedom. Examples of confidence levels: for 95% confidence, \\(t^*\\) is the 2.5% percentile (or 97.5%) &gt; qt(.025, df= ) # for 95% confidence for 90% confidence, \\(t^*\\) is the 5% percentile (or 95%) &gt; qt(.05, df= ) # for 90% confidence Examples of degrees of freedom: for a one-sample mean problem, use \\(df = n-1\\) for a two-sample difference in means problem, let technology get the value! 1.5 Review activity (day 2) A Markdown of this activity with some code included is linked here. 1.5.1 General questions What is the difference between a parameter and a statistic? Give an example of a parameter and a statistic, using “common” notation for each. What is a random sample? What is statistical inference? Suppose you are told a 95% confidence interval for the proportion of registered voters for Trump just before the 2016 election is 0.38 to 0.44. What does this mean? What is the margin of error for this CI? What does “95% confidence” mean? A study found that participants who spent money on others instead of themselves had significantly lower blood pressure (two-sided p-value=0.012). What are the hypotheses for this test? What does the p-value mean? Inference methods rely on understanding the sampling distribution of the statistic that we used to estimate our unknown parameter. What does the sampling distribution of the sample mean look like? What does the standard error of the sample mean measure? How is the sampling distribution used in a hypothesis test? How is it used with confidence intervals? 1.5.2 Comparing two means The data set agstrat.csv contains a stratified random sample of 300 US counties Sampling units: counties Strata: region (North Central, Northeast, South, West) response: number of farms in 1992 (farms92) How can we answer the question: Do the average number of farms per county differ in the western and north central regions in 1992? Our basic plan for analysis looks like: Data: load and clean/manipulate (if needed) EDA: exploratory data analysis Inference: run tests/CIs and check assumptions! Conclusion: interpret results, avoid lots of stats jargon! 1.5.2.1 Data Read in the data and check variable names: &gt; agstrat&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/agstrat.csv&quot;) &gt; names(agstrat) (1a) How many counties were sampled from each region? &gt; table(agstrat$region) We want to focus on W and NC for our comparison. I will do a lot of data manipulation in this class using the R package dplyr. This package is already available on the mirage server, but if you are using a newly installed version of Rstudio on your computer you will need to install the package first. (See Appendix A.4). The filter command from the dplyr package selects rows from a data set that we want to keep (Section C.2.4). E.g. only rows from the W or NC regions. The argument used in this command is a logical statement and the row of a given case is kept in the data set when the statement is TRUE. &gt; library(dplyr) # need to load the package commands &gt; agstrat2 &lt;- filter(agstrat, region %in% c(&quot;W&quot;, &quot;NC&quot;)) &gt; table(agstrat2$region) Notice that filter it does not drop the unused levels (S and NE) from the region factor variable. We can use droplevels on the new data frame agstrat2 to get rid of these unused levels. We assign this new and improved data set the name agstrat2 again. (This overwrites the previous version without the dropped levels.) &gt; agstrat2 &lt;- droplevels(agstrat2) &gt; table(agstrat2$region) 1.5.2.2 EDA (2a) In 2-3 sentences, compare the distributions (shape, center, spread) of farms92 by region (W vs. NC). Note that the commands below give basic R commands for comparing distributions graphically and numerically. You do not necessarily need to use or show all output below when answering this type of question for homework, exams or reports. Graphical options (Section C.3.4) include boxplots: &gt; boxplot(farms92 ~ region, data = agstrat2, horizontal=TRUE, main=&quot;Number of farms (1992) by region&quot;, + ylab = &quot;region&quot;, xlab = &quot;# of farms per county in 1992&quot;) or faceted histograms from the ggplot2 package: &gt; library(ggplot2) # load ggplot2 package &gt; ggplot(agstrat2, aes(x=farms92)) + + geom_histogram() + + facet_wrap(~region) + + labs(title = &quot;Number of farms by region&quot;) Summary stats, by region, can be found using multiple tools in R (Section C.3.3). The tapply command is an option: &gt; tapply(agstrat2$farms92, agstrat2$region, summary) So is using the group_by and summarize combination in dplyr. Here we get the mean and SD of farms92 for both regions: &gt; agstrat2 %&gt;% + group_by(region) %&gt;% + summarize(mean(farms92), sd(farms92)) (2b) What county has the unusually high number of farms in the western region? How many farms do they have? Any ideas why it is so large? How do the summary stats change when this county is omitted? Here we find which row in agstrat2 has a farms92 value greater than 3000 (Section C.2.4): &gt; which(agstrat2$farms92 &gt; 3000) What county is this? We need to access row 118 in agstrat2. We use use [] subsetting or the slice command from dplyr: &gt; agstrat2[118,] # see row 118 &gt; slice(agstrat2, 118) # another way to see row 118 We can use the dplyr commands for (2a) with the slice command added to get stats without row 118 &gt; agstrat2 %&gt;% slice(-118) %&gt;% # slice removes row 118 + group_by(region) %&gt;% # then get stats by region + summarize(mean(farms92), sd(farms92), median(farms92), IQR(farms92)) 1.5.2.3 Inference (3a) What hypotheses are being tested with the t.test command below? What are the p-value and test statistics values for this test? How do you interepret the test statistic? How do you interpret the p-value? Are the assumptions met for this t-test? &gt; # run t test to compare mean number of farms &gt; t.test(farms92 ~ region, data = agstrat2) (3b) How and why do the test stat and p-value change when we omit row 118? Does your test conclusion change when omitting 118? Why? &gt; # redo without row 118 outlier &gt; t.test(farms92 ~ region, subset = -118, data = agstrat2) 1.5.2.4 Conclusion (4a) Do the average number of farms per county differ in the western and north central regions in 1992? If they do differ, by how much? Answer these question in context using numbers from the results above to support your conclusions. "],
["slr.html", "Chapter 2 Simple Linear Regression 2.1 The variables 2.2 The model form 2.3 Theory: Estimation 2.4 SLR model simulation 2.5 Inference for mean parameters 2.6 Inference for average or predicted response 2.7 Example: SLR model (day 3) 2.8 Checking model assumptions and fit 2.9 Example: SLR assumptions (day 4/5) 2.10 Transformations 2.11 Examples: Transformations (day 6) 2.12 \\(R^2\\) and ANOVA for SLR", " Chapter 2 Simple Linear Regression This chapter contains content from Sleuth Chapters 7 and 8. 2.1 The variables Suppose we have a quantitative response variable \\(y\\) that we want to relate to an explantory (aka predictor) variable \\(x\\). For now, we will assume that \\(x\\) is also quantitative. 2.2 The model form This section describes the SLR model for a particular population of interest. Another way to frame the model is that it describes a hypothetical data generating process (DGP) that was used to generate the sample of data that we have on hand. Let \\(Y_i\\) be the response from unit \\(i\\) that has explanatory (aka predictor) value \\(x_i\\). There are two equivalent ways to express the SLR model for \\(Y\\): Conditional normal model: Given a value \\(x_i\\), the response \\(Y_i\\) follows a normal model with mean and SD given below: \\[ Y_i \\mid x_i \\sim N(\\mu_{y\\mid x} = \\beta_0 + \\beta_1 x_i, \\sigma) \\] Mean + error: Statisticians are more likely to use a model specification that expresses \\(Y\\) as a function of the expected value/mean of \\(Y\\) plus an error term that models variation in responses around the mean: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\end{equation}\\] Both expressions of the SLR model above say the same thing: Linear Mean: \\(\\mu_{y\\mid x} = E(Y \\mid x) = \\beta_0 + \\beta_1 x\\) describes the population mean value of \\(Y\\) given a predictor value \\(x\\). This mean value varies linearly with \\(x\\) and the population parameters are \\(\\beta_0\\) and \\(\\beta_1\\). Constant SD: \\(SD(Y\\mid x)=\\sigma\\) describes the SD of \\(Y\\)’s in the population around a given mean value \\(\\mu_{y\\mid x}\\). The fact that this SD does not depend on the value of \\(x\\) is called the contant variance, or homoscedastic, assumption. Normality: The shape of population response values around \\(\\mu_{y\\mid x}\\) is described by a normal distribution model. Finally, one last assumption is made for the SLR model: Indepedence: Given a predictor value of \\(x\\), all responses \\(Y\\) occur independently of each other. There are a total of three parameters in the SLR model: the two mean parameters \\(\\beta_0\\) and \\(\\beta_1\\) the SD parameter \\(\\sigma\\) 2.2.1 Interpretation For a SLR model: \\(\\beta_0\\) is the mean response when the predictor value is 0 since \\(\\mu_{y \\mid 0} = \\beta_0 + \\beta_1(0) = \\beta_0\\). \\(\\beta_1\\) tells us how the mean response changes for a one unit increase in \\(x\\) 2.2.2 Example: Woodpecker nests We want to model nest depth (cm) in a tree cavity as a function of ambient air temperature (Celsius). Our SLR model for this relationships says that, given an ambient air temp \\(x_i\\), a randomly selected nest will have a depth \\(Y_i\\) that is modeled as \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\] This means that depth is normally distributed with a mean of \\(\\mu_{depth \\mid temp} = \\beta_0 + \\beta_1 (temp)\\) and a SD of \\(\\sigma\\). 2.3 Theory: Estimation Let’s consider taking a random sample of size \\(n\\) of responses and predictor values from our population (or DGP) for which the SLR model holds: \\((x_1, Y_1), \\dotsc, (x_n, Y_n)\\). The notation here, \\((x_i, Y_i)\\) implies that the predictor value \\(x_i\\) is fixed, but \\(Y_i\\) is a random variable that is generated from the SLR model described in Section 2.2. The estimation problem is that we need to use the sample of size \\(n\\) to estimate the SLR model parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\). Once we observe a sample of size \\(n\\), then we can use the SLR model to determine the probability of observing the sample. This probability, which depends on the actual model parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\), is called a likelihood function. We plug the observed data into this function, then find the parameters values that maximize the function using calculus. For a SLR model, this process yields the following maximum likelihood estimators (MLE) of our parameters: \\[ \\hat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\ \\ \\ \\ \\ \\hat{\\beta}_0 = \\bar{Y} - \\bar{\\beta}_1 \\bar{x} \\ \\ \\ \\ \\ \\hat{\\sigma} = \\sqrt{\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{y}_i)^2}{n-2}} \\] where \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) is the predicted value of \\(y\\) given the value \\(x_i\\). 2.3.1 Sampling Distributions for SLR estimates The sampling distribution of a model estimate (\\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\)) is constructed by: fix a set of predictor values: \\(x_1, \\dotsc, x_n\\) for each fixed \\(x_i\\), generate a response \\(Y_i\\) from \\(N(\\beta_0 + \\beta_1 x_i, \\sigma)\\) compute the MLE’s \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) from the observed sample from (2) repeat 2-3 lots of times, then the distribution of the estimates from part (3) show the sampling distribution of the slope or intercept estimate. Using probability theory, we can show that the sampling distributions of both \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are approximately normal when \\(n\\) is “large enough” (thanks to the CLT): \\[ \\hat{\\beta}_i \\sim N(\\beta_i, SD(\\hat{\\beta}_i)) \\] Unbiased: We see that the expected value (mean) of \\(\\hat{\\beta}_i\\) is the parameter \\(\\beta_i\\), meaning it is an unbiased estimator. (It doesn’t systematically over- or under-estimate the parameter of interest.) Standard error: We end up estimating the SD in the sampling distribution given above. The SEs for each mean parameter estimate are \\[ SE(\\hat{\\beta}_1) = \\hat{\\sigma}\\sqrt{\\dfrac{1}{(n-1)s^2_x}} \\ \\ \\ \\ \\ SE(\\hat{\\beta}_0) = \\hat{\\sigma}\\sqrt{\\dfrac{1}{n} + \\dfrac{\\bar{x}^2}{(n-1)s^2_x}} \\] 2.4 SLR model simulation Download the Markdown of this activity: .Rmd. 2.4.1 Simulation function This chunk contains code that defines our function reg.sim that simulates \\(n\\) responses from a given regression model and given set of \\(n\\) predictor values \\(x\\). I’ve excluded it from our compiled document so see the .Rmd file to take a look at how this was created. 2.4.2 Run the function once Let’s use the function from (1) above. We will use the \\(n=12\\) temps (x-values) from the woodpeckers data and assume that the true model is: \\[\\mu(y \\mid x) = 20 - 0.4x\\] (red line below) with \\(\\beta_0=20\\), \\(\\beta_1=-0.4\\), and \\(\\sigma=2\\). In the code below I use the set.seed() command to “fix” the random number generator so I get the same answer each time this is run (so my answer in the handout is reproduced each time this file is compiled). &gt; wpdata&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; set.seed(77) &gt; reg.sim(x=wpdata$temp, beta0=20, beta1=-.4, sigma=2) ## $b0 ## [1] 20.8893 ## ## $b1 ## [1] -0.4682368 For this simulated sample (with seed of 77), the estimated regression line is \\(\\hat{\\mu}(y \\mid x) = 20.889 - 0.468x\\) (black line). 2.4.3 Simulated sampling distribution for \\(\\hat{\\beta}_1\\) We will now use the replicate command to generate 1000 different samples which create 1000 different estimates of \\(\\beta_1\\). A histogram of these estimates simulates the sampling distribution of estimated slope. What is the shape of the sampling distribution? Where is the distribution centered? How variable are these estimated slopes. &gt; set.seed(7) # just makes simulation reproducible &gt; slopes&lt;- replicate(1000,reg.sim(x=wpdata$temp, beta0=20, beta1=-.4, sigma=2,grph=F)$b1) &gt; hist(slopes); abline(v=-0.4,col=&quot;red&quot;, lwd=2) &gt; mean(slopes); sd(slopes) ## [1] -0.3974248 ## [1] 0.05227315 2.4.4 Are slope and intercept estimates correlated? In regression, it is not unusual to be interested in estimating a linear combination of our model parameters. An easy example of such a combination of parameters is the mean response for a given value \\(x_0\\) of the predictor: \\[ \\mu(y \\mid x=x_0) = \\beta_0 + \\beta_1x_0 \\] For a specific example, we may want to estimate the mean response (depth) for a temp of \\(x_0=5\\) degrees: \\(\\mu(y \\mid x=5) = \\beta_0 + \\beta_1 5\\). If we don’t know \\(\\beta_0\\) and \\(\\beta_1\\), then this mean parameter is a linear combination of two unknown parameters which we need to estimate. The natural estimate of this is just the estimated mean response: \\(\\hat{\\mu}(y \\mid x=x_0)=\\hat{\\beta}_0 + \\hat{\\beta}_1x_0\\). To assess how variable this estimate is (i.e. to get its SE) we need to understand how (if) the individual estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are correlated. We can use a simulation to look at this issue by generating 1000 samples from our model and plotting each \\((\\hat{\\beta}_0,\\hat{\\beta}_1)\\) pair for each sample. How are estimated slope and intercept associated? Any ideas why? &gt; set.seed(7) # this seed MUST match the seed used to get slopes! &gt; intercepts&lt;- replicate(1000,reg.sim(x=wpdata$temp, beta0=20, beta1=-.4, sigma=2,grph=F)$b0) &gt; plot(intercepts,slopes); abline(h=-.4,col=&quot;red&quot;); abline(v=20,col=&quot;red&quot;) &gt; title(&quot;Estimated slopes and intercepts&quot;) &gt; cor(intercepts, slopes) # correlation between estimates ## [1] -0.6928231 &gt; cov(intercepts, slopes) # covariance between estimates ## [1] -0.02869504 2.5 Inference for mean parameters In intro stats, you used \\(t\\) inference procedures for inference about population means since: (1) the sampling distribution of the sample mean was normally distributed and (2) we had to estimate its variability with a SE. The same goes for inference about the mean response parameters in a SLR model: \\[ t = \\dfrac{\\hat{\\beta}_i - \\beta_i}{SE(\\hat{\\beta}_i)} \\sim t_{df=n-2} \\] Use a t-distribution with \\(n-2\\) degrees of freedom for inference about the mean parameters. The degrees of freedom are calculated as the sample size \\(n\\) minus the number of terms in \\(\\mu_{y \\mid x}\\) that you have to estimate with the data. 2.5.1 Confidence Intervals To estimate either mean parameter with \\(C\\)% confidence, we have the general form \\[ \\hat{\\beta}_i \\pm t^* SE(\\hat{\\beta}_i) \\] where \\(t^*\\) is the \\((100-C)/2\\) percentile from the t-distribution with \\(df=n-2\\) degrees of freedom. 2.5.2 Hypothesis tests We can test the hypothesis \\[ H_0: \\beta_i = \\beta^*_i \\] with the following t-test statistic: \\[ t =\\dfrac{\\hat{\\beta}_i - \\beta^*_i}{SE(\\hat{\\beta}_i)} \\] where \\(\\beta^*\\) is our hypothesized value of \\(\\beta\\) (intercept or slope). The t-distribution with \\(n-2\\) degrees of freedom is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. The usual test results given by standard regression output tests whether a parameter value (intercept or slope) is equal to 0 vs. not equal to 0: \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with a test stat of \\[ t =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] 2.6 Inference for average or predicted response 2.6.1 Confidence intervals for \\(\\mu_{y \\mid x}\\) Here we are interested in estimating not just a \\(\\beta\\) with confidence, but we want to estimate the mean response for a given value of \\(x_0\\). For example, suppose we want to estimate the mean nest depth when the temp is \\(x_0=8\\) degrees. This mean parameter of interest is then: \\[ \\mu_{depth \\mid temp=8} = \\beta_0 + \\beta_1 (8) \\] Our parameter of interest is \\(\\mu_{y \\mid x_0} = \\beta_0 + \\beta_1 x_0\\) where \\(x_0\\) is known and \\(\\beta\\)’s need to be estimated. The natural estimator is just the fitted equation: \\[ \\hat{\\mu}_{y \\mid x_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 \\] As will any estimator, we can measure the variability of this estimator with a SE: \\[ SE(\\hat{\\mu}_{y \\mid x_0}) = \\hat{\\sigma} \\sqrt{\\dfrac{1}{n} + \\dfrac{(x_0 - \\bar{x})^2}{(n-1)s^2_x}} \\] Note! This SE depends on \\(\\pmb{x_0}\\)! It is miminized when \\(x_0\\) equals the mean predictor value \\(\\bar{x}\\) and it grows as \\(x_0\\) gets further from \\(\\bar{x}\\). Estimation is most precise in the “middle” of the predictor range and becomes less precise at the “edges” (where we usually have less data). A 95% confidence interval for the mean response \\(\\mu_{y \\mid x_0}\\) looks like \\[ \\hat{\\mu}_{y \\mid x_0} \\pm t^*_{df=n-2}SE(\\hat{\\mu}_{y \\mid x_0}) \\] 2.6.2 Prediction intervals for new cases To predict one individual’s future response \\(Y\\) for the predictor value \\(x_0\\), we just use the fitted equation: \\[ pred_{y \\mid x_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 \\] (We use \\(pred_{y \\mid x_0}\\) to remind us which predictor value as used for prediction.) Recall that the SLR model assumes the \\(Y\\) values when \\(x=x_0\\) are normally distributed with mean \\(\\mu_{y \\mid x_0}\\) and SD \\(\\sigma\\). The SE of this prediction at \\(x_0\\) takes into account (1) uncertainty in estimating the mean \\(\\mu_{y \\mid x_0}\\) and (2) variation in \\(Y\\)’s around the mean response (\\(\\sigma\\)): \\[ SE(pred_{y \\mid x_0}) = \\hat{\\sigma} \\sqrt{1 + \\dfrac{1}{n} + \\dfrac{(x_0 - \\bar{x})^2}{(n-1)s^2_x}} = \\sqrt{\\hat{\\sigma}^2 + SE(\\hat{\\mu}_{y \\mid x_0})^2 } \\] A 95% prediction interval for a future individual response at \\(x=x_0\\) looks like \\[ pred_{y \\mid x_0} \\pm t^*_{df=n-2}SE(pred_{y \\mid x_0}) \\] Predictions intervals feel and look similar to the mean response intervals above, but there is a very important conceptual difference: prediction means we are trying to “guess” at one individual response as opposed to the mean response of a large group of individuals. For example, if we want to understand nest depths for all nest build when temp is 8 degrees, then we care about estimating a fixed (but unknown) mean depth \\(\\mu_{depth \\mid temp=8}\\). If we see a bird starting to build a nest at 8 degrees, then we care about predicting this one, randomly determined depth \\(Y\\) using \\(pred_{depth \\mid temp=8}\\) and would use a prediction interval. 2.7 Example: SLR model (day 3) We will revisit the woodpecker nesting data first described in Section 2.2.2. 2.7.1 Load data Let’s suppose these 16 nests are a random sample of all nests in the collection region. The command head(dataname) produces a view of the first 5 rows of data. &gt; wpdata &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; head(wpdata) ## temp depth ## 1 -6 21.1 ## 2 -3 26.0 ## 3 -2 18.0 ## 4 1 19.2 ## 5 6 16.9 ## 6 10 18.1 &gt; dim(wpdata) ## [1] 12 2 2.7.2 EDA Start with univariate exploration: &gt; summary(wpdata) ## temp depth ## Min. :-6.00 Min. :10.50 ## 1st Qu.: 0.25 1st Qu.:12.03 ## Median :10.50 Median :16.85 ## Mean :11.00 Mean :16.36 ## 3rd Qu.:21.75 3rd Qu.:18.38 ## Max. :26.00 Max. :26.00 &gt; par(mfrow=c(1,2)) &gt; hist(wpdata$temp) &gt; hist(wpdata$depth) &gt; par(mfrow=c(1,1)) Graphically explore the (“bivariate”) relationship between temp and depth with a scatterplot using the command plot(y,x). Here is the plot version (with pch point character changed to give filled circles): &gt; plot(depth ~ temp, data=wpdata, pch=16, xlab=&quot;air temperature (C)&quot;, + ylab=&quot;nest depth (cm)&quot;, main=&quot;woodpeckers scatterplot&quot;) Here is the ggplot2 version with labs added to change labels and title: &gt; library(ggplot2) &gt; ggplot(wpdata, aes(x=temp, y = depth)) + geom_point() + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, title= &quot;woodpeckers scatterplot&quot;) 2.7.3 The least squares line (the estimated SLR model): You fit the linear model with mean \\(\\mu_{Y \\mid x} = \\beta_0 + \\beta_1 x\\) with the linear model function lm(y ~ x, data=). &gt; wood.lm &lt;- lm(depth~temp, data=wpdata) &gt; wood.lm ## ## Call: ## lm(formula = depth ~ temp, data = wpdata) ## ## Coefficients: ## (Intercept) temp ## 20.1223 -0.3422 We have mean parameter estimates: \\[ \\hat{\\beta}_0 = 20.122 \\ \\ \\ \\ \\hat{\\beta}_0 = -0.342 \\] The object wood.lm is called a linear model object in R. We can add the regression line from this object to an existing base R plot of the data using the abline command. &gt; plot(depth ~ temp, data=wpdata, pch=16, xlab=&quot;air temperature (C)&quot;, + ylab=&quot;nest depth (cm)&quot;, main=&quot;regression of depth on temp&quot;) &gt; abline(wood.lm) The ggplot2 package contains a geom_smooth geometry to add this SLR line: &gt; ggplot(wpdata, aes(x=temp, y = depth)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;, se = FALSE) + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, title= &quot;woodpeckers scatterplot&quot;) With geom_smooth, you need to specify the type of method used to relate \\(x\\) to \\(y\\). Adding se=FALSE removes a confidence interval for the mean trend. Notice that the aes aesthetic given in the ggplot function specifies the x and y variables to plot. The geom’s that follow this part of the command use this aes to create points and an estimated line. 2.7.4 Inference for coefficients The summary command is used for statistical inference for the intercept \\(\\beta_0\\) and slope \\(\\beta_1\\): &gt; summary(wood.lm) ## ## Call: ## lm(formula = depth ~ temp, data = wpdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8066 -1.3321 -0.6529 0.6811 4.8512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.12228 0.94024 21.401 1.11e-09 *** ## temp -0.34218 0.05961 -5.741 0.000188 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.335 on 10 degrees of freedom ## Multiple R-squared: 0.7672, Adjusted R-squared: 0.7439 ## F-statistic: 32.96 on 1 and 10 DF, p-value: 0.0001875 This output gives the mean parameter estimates in the Estimate column of the main table: \\[ \\hat{\\beta}_0 = 20.122 \\ \\ \\ \\ \\hat{\\beta}_0 = -0.342 \\] The estimated model SD \\(\\sigma\\) is given by Residual standard error: \\[ \\hat{\\sigma} = 2.335 \\] You should know how to verify (or find) the test stats and p-values for \\(\\beta_0\\) and \\(\\beta_1\\) given by the summary command if you are given the estimates and SEs. For the slope test \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\neq 0\\) we get a \\(t\\) test stat of &gt; (-0.34218 - 0)/0.05961 ## [1] -5.740312 The p-value for this two sided test is the probability of being above +5.741 and below -5.741, or double the probability below -5.741 (since the t-distribution is symmetric around 0): &gt; pt(-5.741,10) ## [1] 9.373105e-05 &gt; 2*pt(-5.741,10) ## [1] 0.0001874621 A 95% confidence interval for the slope \\(\\beta_1\\) is computed from the t distribution with 10 degrees of freedom: &gt; qt(.975,10) ## [1] 2.228139 &gt; -.34218 + c(-1,1)*qt(.975,10)*.05961 ## [1] -0.4749994 -0.2093606 &gt; confint(wood.lm) ## 2.5 % 97.5 % ## (Intercept) 18.0272874 22.2172802 ## temp -0.4749868 -0.2093679 The confint function gives the most accurate interval (no rounding error) but you need to know how to compute these CIs ``by hand.\" 2.7.5 Additional lm information The function lm creates a linear model object in R that has lots of information associated with it. Information includes the coefficient values, fitted values (\\(\\hat{y}_i\\)), and residuals (\\(y_i - \\hat{y}_i\\)): &gt; attributes(wood.lm) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; ## ## $class ## [1] &quot;lm&quot; &gt; wood.lm$coefficients ## (Intercept) temp ## 20.1222838 -0.3421773 &gt; wood.lm$fitted.values # predicted y values (y-hat) ## 1 2 3 4 5 6 7 8 ## 22.17535 21.14882 20.80664 19.78011 18.06922 16.70051 16.35833 13.62091 ## 9 10 11 12 ## 12.93656 11.91003 11.56785 11.22567 &gt; wood.lm$residuals # residuals for each data point ## 1 2 3 4 5 6 7 ## -1.0753477 4.8511843 -2.8066384 -0.5801065 -1.1692199 1.3994894 0.4416667 ## 8 9 10 11 12 ## -1.8209148 -1.9365602 0.1899718 3.2321491 -0.7256736 There are also functions that act on lm objects like &gt; fitted(wood.lm) ## 1 2 3 4 5 6 7 8 ## 22.17535 21.14882 20.80664 19.78011 18.06922 16.70051 16.35833 13.62091 ## 9 10 11 12 ## 12.93656 11.91003 11.56785 11.22567 &gt; resid(wood.lm) ## 1 2 3 4 5 6 7 ## -1.0753477 4.8511843 -2.8066384 -0.5801065 -1.1692199 1.3994894 0.4416667 ## 8 9 10 11 12 ## -1.8209148 -1.9365602 0.1899718 3.2321491 -0.7256736 &gt; vcov(wood.lm) # variance and covariance matrix of beta estimates ## (Intercept) temp ## (Intercept) 0.88406062 -0.039081046 ## temp -0.03908105 0.003552822 2.7.6 broom package: Tidy lm output The broom package contains functions that convert model objects (like a lm object) into “tidy” data frames. The tidy command summarizes model results: &gt; library(broom) &gt; tidy(wood.lm) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 20.1 0.940 21.4 0.00000000111 ## 2 temp -0.342 0.0596 -5.74 0.000188 Notice that the output object is called a “tibble” which is a type of data frame in R. Here we can add confidence intervals for the model parameters to the output: &gt; tidy(wood.lm, conf.int=TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 20.1 0.940 21.4 0.00000000111 18.0 22.2 ## 2 temp -0.342 0.0596 -5.74 0.000188 -0.475 -0.209 The augment command augments the data frame used to create a lm with predicted values and residuals from the lm model: &gt; wpdata.aug &lt;- augment(wood.lm) &gt; head(wpdata.aug) ## # A tibble: 6 x 9 ## depth temp .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.1 -6 22.2 1.22 -1.08 0.272 2.42 0.0544 -0.540 ## 2 26 -3 21.1 1.07 4.85 0.211 1.66 0.732 2.34 ## 3 18 -2 20.8 1.03 -2.81 0.194 2.23 0.215 -1.34 ## 4 19.2 1 19.8 0.900 -0.580 0.149 2.45 0.00632 -0.269 ## 5 16.9 6 18.1 0.737 -1.17 0.0996 2.43 0.0154 -0.528 ## 6 18.1 10 16.7 0.677 1.40 0.0840 2.41 0.0180 0.626 The .fitted column gives the fitted values \\(\\hat{y}_i\\) for each case and the .resid gives the residuals \\(y_i - \\hat{y}_i\\). 2.7.7 Inference for the mean and predicted response We can get a 95% confidence interval for the mean nest depth at 8 degrees Celsius \\(\\mu(depth\\mid temp=8)\\) with the predict command with interval type confidence specified: &gt; predict(wood.lm, newdata = data.frame(temp=8), interval = &quot;confidence&quot;, se.fit=T) ## $fit ## fit lwr upr ## 1 17.38487 15.83132 18.93841 ## ## $se.fit ## [1] 0.6972406 ## ## $df ## [1] 10 ## ## $residual.scale ## [1] 2.33453 The argument se.fit=T provides the SE of the mean response estimate. The estimated mean depth of all nests built at 8 degrees is 17.38 cm with a SE of 0.697 cm. We are 95% confident that the mean depth at 8 degrees is between 15.83 to 18.9 cm. You can verify the computation of the SE and CI using summary stats for temp and the estimated parameter values from the summary command: \\[ \\hat{\\mu}_{temp \\mid 8} = \\hat{\\beta}_0 + \\hat{\\beta}_1 (8) = 20.122 + (-0.342 )(8) = 17.385 \\] \\[ SE(\\hat{\\mu}_{depth \\mid 8}) = \\hat{\\sigma} \\sqrt{\\dfrac{1}{n} + \\dfrac{(x_0 - \\bar{x})^2}{(n-1)s^2_x}} = 2.335 \\sqrt{\\dfrac{1}{12} + \\dfrac{(8-11)^2}{(12-1)11.809^2}} = 0.6972 \\] &gt; nrow(wpdata) ## [1] 12 &gt; mean(wpdata$temp) ## [1] 11 &gt; sd(wpdata$temp) ## [1] 11.80909 &gt; mn.est.se &lt;- 2.33453*sqrt(1/12 + (8-11)^2/((12-1)*11.80909^2)) &gt; mn.est.se ## [1] 0.6972407 &gt; mn.est &lt;- 20.12228 -0.34218*8 &gt; mn.est ## [1] 17.38484 &gt; mn.est + c(-1,1)*qt(.975,10)*mn.est.se ## [1] 15.83129 18.93839 You can also include more than one predictor value temp in this function: &gt; predict(wood.lm, newdata = data.frame(temp=c(8,20)), interval = &quot;confidence&quot;, se.fit=T) ## $fit ## fit lwr upr ## 1 17.38487 15.83132 18.93841 ## 2 13.27874 11.35950 15.19798 ## ## $se.fit ## 1 2 ## 0.6972406 0.8613639 ## ## $df ## [1] 10 ## ## $residual.scale ## [1] 2.33453 We can get a 95% prediction interval for the depth of one depth constructed at 8 degrees Celsius \\(pred_{depth \\mid temp=8}\\) with the predict command with interval type prediction specified: &gt; predict(wood.lm, newdata = data.frame(temp=8), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 17.38487 11.95617 22.81356 We are 95% confident that a new nest built at 8 degrees will have a depth between 11.96 to 22.81 cm. R does not give us the SE for prediction \\(SE(pred(Y \\mid x_0)) = \\sqrt{\\hat{\\sigma}^2 + SE(\\hat{\\mu})^2}\\) so we need to compute it by hand if we want its value: \\[ SE(pred_{depth \\mid 8}) = \\sqrt{\\hat{\\sigma}^2 + SE(\\hat{\\mu})^2} = \\sqrt{2.335^2 + 0.6972^2} = 2.436 \\] &gt; se.pred &lt;- sqrt(2.33453^2 + 0.6972406^2) &gt; se.pred ## [1] 2.436427 The predicted depth of one nest build at 8 degress is 17.38 cm with a SE of 2.44 cm. The 95% prediction interval produced above can be verified as follows: &gt; yhat &lt;- 20.12228 -0.34218*8 &gt; yhat ## [1] 17.38484 &gt; yhat + c(-1,1)*qt(.975,10)*se.pred ## [1] 11.95614 22.81354 2.7.8 Adding confidence bands to a scatterplot The geom_smooth function in ggplot2 adds a 95% confidence interval for \\(\\mu_{y \\mid x}\\) around the estimated mean line: &gt; ggplot(wpdata, aes(x=temp, y = depth)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;) + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, title= &quot;woodpeckers scatterplot with mean confidence interval&quot;) Adding prediction interval bands takes slightly more work. First, create a new version of the data set that includes prediction intervals for each case in the data: &gt; wpdata.pred &lt;- data.frame(wpdata, predict(wood.lm, interval=&quot;prediction&quot;)) &gt; head(wpdata.pred) ## temp depth fit lwr upr ## 1 -6 21.1 22.17535 16.30939 28.04131 ## 2 -3 26.0 21.14882 15.42438 26.87325 ## 3 -2 18.0 20.80664 15.12396 26.48932 ## 4 1 19.2 19.78011 14.20554 25.35468 ## 5 6 16.9 18.06922 12.61459 23.52385 ## 6 10 18.1 16.70051 11.28483 22.11620 Then add a geom_ribbon layer to the previous plot, with ymin and ymax determined by the prediction interval’s lower (lwr) and upper (upr) bounds. The fill arguments in both layers below are not really needed, but they are used here to provide a legend label for the plot: &gt; ggplot(wpdata.pred, aes(x=temp, y = depth)) + + geom_point() + + geom_ribbon(aes(x=temp, ymin = lwr, ymax = upr, fill = &quot;prediction&quot;), alpha = .1) + + geom_smooth(method = &quot;lm&quot;, aes(fill = &quot;confidence&quot;), alpha = .4) + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, + title= &quot;woodpeckers scatterplot&quot;, fill = &quot;Type&quot;) 2.7.9 Tools for displaying your model As described in the Formatting Tables in Markdown (Section D.5), you can use the package stargazer to create a nice table of model results in your pdf. The entire R chunk to do this in a pdf doc format is shown below (but not evaluated in this html book). You will need to add the R chunk option results='asis' to get the table formatted correctly. I also include the message=FALSE option in the chunk below that runs the library command to suppress the automatic message created when running the library command with stargazer. ```{r, results=&#39;asis&#39;, message=FALSE} library(stargazer) stargazer(wood.lm, header=FALSE, single.row = TRUE, title = &quot;SLR of depth on temp&quot;) ``` ```` The kable function (from the knitr package) works well in all output environments (e.g. pdf, html, word). The input to this function needs to be a data frame, so we can use the tidy version of our model summary: &gt; library(knitr) &gt; kable(tidy(wood.lm, conf.int = TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 20.122 0.94 21.401 0 18.027 22.217 temp -0.342 0.06 -5.741 0 -0.475 -0.209 2.8 Checking model assumptions and fit Recall the SLR model assumptions from 2.2: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\] Linearity: The mean response varies linearly with \\(x\\). Constant SD: \\(SD(Y\\mid x)=\\sigma\\) describes the SD of \\(Y\\)’s in the population around a given mean value \\(\\mu_{y\\mid x}\\). An equivalent statement of this assumption is that the model errors should not be associated with \\(x\\). Normality: The shape of population response values around \\(\\mu_{y\\mid x}\\) is described by a normal distribution model. Indepedence: Given a predictor value of \\(x\\), all responses \\(Y\\) occur independently of each other. An equivalent statement of this assumption is that the model errors are independent. 2.8.1 Residuals If all four model assumptions are met, then our model errors \\(\\epsilon_i\\)’s will be independent and distributed like \\(N(0,\\sigma)\\). Now we can’t actually “see” the model errors unless we know the true parameter values in the population since \\[ \\epsilon_i = y_i - (\\beta_0 + \\beta_1 x_i) \\] The closest thing we have to the model errors are the fitted model residuals computed using the estimated model parameters: \\[ r_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\] Residuals for a fitted model are a diagnostic tool to help check whether a model fit to data seems to match the true model form that generated the data. 2.8.2 Residual plot: linearity and constant variance A residual plot is constructed by plotting \\(r_i\\) (y-axis) against \\(x_i\\) (x-axis). A horizontal reference line at \\(y=0\\) is usually added (since the mean residual value is always 0). Linearity: This assumption is met if, at each x-value, you see similar scatter of points (residuals) above and below the 0-reference line. Constant variance: This assumption is met if you see a similar magnitude of point scatter around the 0-reference line as you move along the x-axis. A residual plot that meets both these conditions is called a null plot. Watch out for curvature which suggests the mean function relating \\(y\\) and \\(x\\) may not be linear nonconstant variation which is seem in “fan” shaped plots outliers which can have a large influence on the fitted model 2.8.2.1 Example: Residual plot Let’s revisit the woodpecker nesting depth model and use the broom package to add residuals to the wpdata data frame (Section 2.7.6): &gt; # model fit above in Section 2.7 example &gt; wpdata&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; wood.lm&lt;- lm(depth~temp, data=wpdata) &gt; library(broom) &gt; wpdata.aug &lt;- augment(wood.lm) &gt; head(wpdata.aug) ## # A tibble: 6 x 9 ## depth temp .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.1 -6 22.2 1.22 -1.08 0.272 2.42 0.0544 -0.540 ## 2 26 -3 21.1 1.07 4.85 0.211 1.66 0.732 2.34 ## 3 18 -2 20.8 1.03 -2.81 0.194 2.23 0.215 -1.34 ## 4 19.2 1 19.8 0.900 -0.580 0.149 2.45 0.00632 -0.269 ## 5 16.9 6 18.1 0.737 -1.17 0.0996 2.43 0.0154 -0.528 ## 6 18.1 10 16.7 0.677 1.40 0.0840 2.41 0.0180 0.626 The residual plot will put the predictor temp on the x-axis and .resid on the y-axis: &gt; library(ggplot2) &gt; ggplot(wpdata.aug, aes(x = temp, y = .resid)) + + geom_point() + + geom_hline(yintercept = 0, linetype= &quot;dashed&quot;) Don’t forget to use the augmented data frame wpdata.aug that contains the residuals. The layer geom_hline adds the horizontal reference line at 0. Interpretation: There are no majors trends seen in this residual plot. Generally, it is hard to prove or disprove model assumptions when we only observe 12 data points! 2.8.3 Residual normal QQ plot A normal QQ plot for a variable plots observed quartiles against the theoretical quartiles from a normal model. If these points follow a line then the data is approximately normal. Here is a general guide for interpreting non-linear trends that indicate a non-normal distribution: Concave up: the distribution is right skewed Concave down: the distribution is left skewed S-shaped: the distribution is symmetric but the tails are either too short (not enough variation) or too long (too much variation) to be normally distributed. More help! Check out this website for a deeper discussion of the interpretation of normal QQ plots We can check the normality assumption by plotting residuals with a normal QQ plot. You can also use a histogram of residuals to help interpret the normal QQ plot. 2.8.3.1 Example: Residual normal QQ plot Back to the augmented woodpecker data set. A histogram of the residuals shows a slightly right skewed distribution. &gt; hist(wpdata.aug$.resid) A quick way to get a normal QQ plot of residuals is to plug the lm object into the plot command and request plot number 2: &gt; plot(wood.lm, which = 2) Alternatively, you could use a ggplot. The aesthetic used for a QQ plot in a ggplot is sample = variable, then geom_qq and geom_qq_line are the layers used: &gt; ggplot(wpdata.aug, aes(sample = .resid)) + + geom_qq() + + geom_qq_line() Interpretation: The QQ plot also suggests a slightly longer right tail because it is (sort of) concave up. But, the QQ plot also clearly shows that this feature could just be due to two cases with residual values that are slightly higher than all others. Again, because there are only 12 data points these two cases could just be due to chance and we can conclude that there aren’t any strong indications of a non-normal distribution. 2.8.4 Independence Independence of errors is probably the hardest assumption to check because it often depends on how the data was collected. Two common scenarios that lead to data that violates this assumption are temporal correlation: This means correlation of errors because measurements were collected over time. Responses (and errors) measured close in time are more likely to be similar in value that responses measured further apart in time. For example, daily temperature readings in Northfield are temporally correlated. spatial correlation: This means correlation of errors because measurements were collected over a spatial region Responses (and errors) measured close together in space are more likely to be similar in value that responses measured further apart in space. For example, home values a sample of houses in St. Paul are likely spatially correlated since the value of a house is likely more similar to it’s neighbor than to a house across town. One method of checking for these two types of dependence is to plot the model residuals \\(r_i\\) (y-axis) against a variable that measures, or is associated, with time or space. For the time example, plot residuals against day of the year. For the spatial example, plot residuals against a categorical “neighborhood” variable. 2.8.5 Robustness against violations Robustness of a statistical method means the conclusions we make using the method aren’t all that senstive to assumptions used to construct the method. Can we trust our SLR model inferences if a model assumption is violated? Inference about \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\mu_{y \\mid x}\\) from a SLR model are only robust against violations of the normality assumption. If \\(n\\) is large enough, then the Central Limit Theorem tells us that t-tests and t-CIs are still accurate. SLR modeling is not robust against any other violations: Linearity: If your fitted mean model doesn’t match the form of the true mean then you will get incorrect inferences about predictor effects, mean responses, predicted responses, etc. Even your estimated model SD will be wrong! Constant variance and independence: If violated, the SE’s produced by the SLR model fit will not accuractely reflect the true uncertainty in our estimated parameters or predictions. Normality for prediction If the normality assumption is violated, then our prediction intervals will not capture the value of a new response “95% of the time”. (There is no CLT and “large n” to help us here when we are trying to predict one response!) 2.8.6 “Fixes” to violations Here are some suggestions if a particular assumption is violated. The first part to consider “fixing” is linearity. If the mean function is not correctly specified then that will likely cause the other assumptions to not hold. Linearity: transform one or both variables to a different scale (e.g. logarithm, square root, reciprocal), modify the mean function (e.g. add a quadratic term), try non-linear regression Constant variance: tranform the response variable, weighted regression Normality: transform the response variable Independence: add more predictors, use a model with correlated errors (e.g. mixed effects, time series, spatial, etc) 2.9 Example: SLR assumptions (day 4/5) 2.9.1 Drug offender sentences The data set DrugOffenses2.csv contains data on 24,011 individuals convicted on federal drug charges during 2013 and 2014. We will subset these individuals to look only at cases given non-life prison sentences (sentence2 &gt; 0), then look at the SLR of an individual’s sentence length (sent.nonlife) in months against their crimal history points (`CRIMPTS). 2.9.1.1 Basic EDA shows that both sentencing and points variables are right skewed, and there are non-responses in each variable. &gt; drug &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/DrugOffenses2.csv&quot;) &gt; dim(drug) ## [1] 24011 55 &gt; library(dplyr) &gt; drugPrison &lt;- filter(drug, sentence2 &gt; 0) &gt; par(mfrow=c(1,2)) &gt; hist(drugPrison$CRIMPTS, main=&quot;criminal points&quot;) &gt; hist(drugPrison$sent.nolife, main=&quot;sentence&quot;) &gt; par(mfrow=c(1,1)) &gt; summary(drugPrison$CRIMPTS) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 0.00 2.00 3.51 5.00 42.00 2012 &gt; summary(drugPrison$sent.nolife) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.03 21.00 48.00 65.41 87.00 960.00 45 2.9.1.2 Usual scatterplot The usual scatterplot suffers from overplotting when we have a large data sets or datasets with very discrete variables. It is hard to see where the majority of cases are in this plot and difficult to discern any trend in the response as we change the predictor variable. &gt; library(ggplot2) &gt; ggplot(drugPrison, aes(x=CRIMPTS, y=sent.nolife)) + + geom_point() + + geom_smooth(method= &quot;lm&quot;, se=FALSE) 2.9.1.3 Jittering and a loess smoother Two ways to minimize the effect of overplotting are to “jitter” the plotted points by adding a small amount of random noise. This can be done by using geom_jitter instead of geom_point use a more transparent plotting symbol. Then when points are overlapping, we will see darker colors which indicate a higher density of observations in that region of the graph. We do this by adding alpha= to the geom_jitter and make the alpha value less than 1 (smaller values mean more transparent points). We will also use a loess smoother line to the scatterplot to help reveal the true trend in sentencing as the point history changes. A smoother is a non-parameteric way to locally “average” the response as we change the predictor value. (Non-parametric means this isn’t a parameterized curve like a line or quadratic function, which means we don’t end up with a nice formula for \\(y\\) given \\(x\\) from such a model.) We fit the loess model to our plot by adding another layer of geom_smooth(method = \"loess\", se=FALSE). In the commands below, the aes(color=) just adds a color legend to the plot. &gt; ggplot(drugPrison, aes(x=CRIMPTS, y=sent.nolife)) + + geom_jitter(alpha = .2) + + geom_smooth(method= &quot;lm&quot;, se=FALSE, aes(color=&quot;lm&quot;)) + + geom_smooth(method=&quot;loess&quot;, se=FALSE, aes(color=&quot;smoother&quot;)) 2.9.1.4 Residual plot and loess smoother The scatterplot and loess smoother suggest there is slight quadratic relationship between points history and sentence length, with a positive trend up to about 20 points and a negative trend after. We can see this too in the residual plot with loess smoother added. The variation around the regression line seems fairly similar for any points value up to about 20 points, and after 20 points there are fewer cases which seem to be slightly less variable. (Note that “similar variation” means the spread around the line is of similar magnitude for any value of x, not that the spread around the line is symmetric. As we see next, the spread around the line (residuals) is skewed right.) &gt; drug.lm &lt;- lm(sent.nolife ~ CRIMPTS, data=drugPrison) &gt; library(broom) &gt; drugPrison.aug &lt;- augment(drug.lm) &gt; ggplot(drugPrison.aug, aes(x=CRIMPTS, y=.resid)) + + geom_jitter(alpha = .2) + + geom_smooth(method= &quot;lm&quot;, se=FALSE, aes(color=&quot;lm&quot;)) + + geom_smooth(method=&quot;loess&quot;, se=FALSE, aes(color=&quot;smoother&quot;)) 2.9.1.5 Checking normality The residuals for this model are not normally distributed, there is a strong skew to the right. If the linear model fit the trend of the data, this non-normality would not be a concern for inference about the mean parameters \\(\\beta\\). It would only be a concern if we wanted to predict one individual’s sentence length with confidence given their criminal history points. &gt; par(mfrow=c(1,2)) &gt; hist(resid(drug.lm), main=&quot;Histogram of residuals&quot;) &gt; plot(drug.lm, which = 2) 2.9.2 Case study 15.2 - Global Warming Let’s look at the regression of global mean temperature deviation (Celsius) on year (1850 through 2010). The scatterplot suggests a curved relationship with time. Temperature deviation is the mean yearly temp minus the mean temp from all 161 years. &gt; library(Sleuth3) &gt; temps &lt;- case1502 &gt; summary(temps) ## Year Temperature ## Min. :1850 Min. :-0.6060 ## 1st Qu.:1890 1st Qu.:-0.3000 ## Median :1930 Median :-0.1740 ## Mean :1930 Mean :-0.1153 ## 3rd Qu.:1970 3rd Qu.: 0.0260 ## Max. :2010 Max. : 0.6200 &gt; ggplot(temps, aes(x = Year, y = Temperature)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;, se = FALSE) + + geom_smooth(method = &quot;loess&quot;, se = FALSE, linetype = &quot;dashed&quot;) 2.9.2.1 Independent residuals? After fitting our regression of Temperature on Year and Year^2 (due to curvature), a check of the residual plot (residuals vs. Year) shows no obvious signs of curvature in residuals so the quadratic mean function with time seems adequate. &gt; temps.lm &lt;- lm(Temperature ~ Year + I(Year^2), data=temps) &gt; temps.aug &lt;- augment(temps.lm) &gt; ggplot(temps.aug, aes(x = Year, y = .resid)) + + geom_point() + + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) The independence assumption will be violated for this model if residuals closer in time (x) are more similar than residuals further apart in time. This is hard to see in the basic residual plot above. To see this idea better, we can look at line plot of residuals vs. time: &gt; ggplot(temps.aug, aes(x = Year, y = .resid)) + + geom_point() + + geom_line() + + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) With this plot is it clear that if one year has a negative residual (lower than expected temps), the next year’s residual is often also negative. Same is true for positive residuals. This indicates that there is a temporal association in these residuals, which means that they are not indepedent. A SLR model is not appropriate for this data because the SEs will not accurately reflect the uncertainty in our estimates when independence is violated. 2.9.2.2 Autocorrelation The correlation statistic is used to assess the strength and direction of a linear relationship between two quantitative variables. The autocorrelation is used to assess how correlated values are for the same quantitative variable. Autocorrelation is computed by pairing responses by a lag amount. Lag of 1 means we are computing correlation for responses that differ by one spot in the vector of responses. For our time and temp data, this means looking at the correlation between responses that differ by a year. Lag 2 means looking at correlation between responses that differ by 2 years, and so on. We can use autocorrelation to verify that residuals closer together in time are more similar than those further apart in time: &gt; acf(resid(temps.lm)) &gt; acf(resid(temps.lm), plot=FALSE) ## ## Autocorrelations of series &#39;resid(temps.lm)&#39;, by lag ## ## 0 1 2 3 4 5 6 7 8 9 10 ## 1.000 0.592 0.446 0.336 0.380 0.313 0.270 0.261 0.272 0.287 0.236 ## 11 12 13 14 15 16 17 18 19 20 21 ## 0.164 0.083 -0.004 0.014 -0.007 -0.067 -0.022 -0.038 -0.069 -0.065 -0.140 ## 22 ## -0.162 Residuals that differ by one year have a correlation of 0.592, indicating a moderate positive correlation. This is what were able to see in the line plot above. Residuals that differ from 2-9 years have correlations between about 0.45 to 0.25. After years there is little to no autocorrelation between residuals. 2.10 Transformations Transformations of one or both variables in SLR is done to fix one, or both, of these assumption violations: linearity and constant variance. Tranformations are often explored via trial-and-error, use plots of transformed variables and residual plots from potential transformed SLR models to find the “best” SLR model. “Best” is determined by a model that most satisfies the SLR assumptions, not a model that yields the “strongest” relationship between variables. A transformation choice can also be determined from a theoretical model that we have from a specific application. 2.10.1 Transformation choices We need to look at non-linear functions of a variable. A linear transformation, like changing a measure from cm to feet would only change the numbers you see on the axes label and not the relative location of points on the plot. Common transformations to consider are the following: logarithms: Any log-base can be explored but base choice will only affect how you interpret a model, not affect the linearity or constant variance assumptions. E.g. if \\(\\log_2(y)\\) vs. \\(x\\) looks nonlinear, then \\(\\log_{10}(y)\\) vs. \\(x\\) will also be nonlinear. Logarithms are nice because they are easily interpretable in a SLR model (more to come). In R, &gt; log(2) # the natural log function (ln) ## [1] 0.6931472 &gt; log10(2) # base 10 ## [1] 0.30103 &gt; log2(2) # base 2 ## [1] 1 square root: A square root transformation is an “in between” no transformation and a logarithm transformation (see the Figure below) &gt; sqrt(4) # square root function ## [1] 2 2.10.2 Transformations in R We can visualize square root and logarithms in aggplot2 scatterplot by adding a layer that changes the scale of an axes. For example, scale_x_sqrt() will change the geom_point() scatterplot to a plot of \\(y\\) aganist \\(\\sqrt{x}\\) but the axes numeric labels on the x-axis will still measure \\(x\\). The layer scale_y_log10() will convert \\(y\\) to the base-10 logarithm. If you want a transformation that is not one of these two, then you can transform the variable in the aesthetic: ggplot(data, aes(x = 1/x, y = y)) would plot \\(y\\) against the reciprocal \\(1/x\\). You can apply transformations directly in the lm command too, e.g. lm(log(y) ~ log(x), data) fits the regression of \\(\\log(y)\\) against \\(\\log(x)\\). If a transformation involves a mathematics function (like power ^ or division /) on the right side of the formula symbol ~, then you need to use the “as is” operator function I(). E.g. the regression of \\(y\\) against the reciprocal \\(1/x\\) is lm(y ~ I(1/x), data). 2.10.3 Interpretation In general, models with transformed variables should be interpreted as a usual SLR but with the transformed variable scale. For example, the SLR for \\(\\sqrt{y}\\) against \\(\\sqrt{x}\\) has a mean function that looks like \\[ \\mu_{\\sqrt{y} \\mid x} = \\beta_0 + \\beta_1 \\sqrt{x} \\] so a one unit increase in the square root of \\(x\\) in associated with a \\(\\beta_1\\) change in the mean of the square root of \\(y\\). Models that use logarithms have a nicer interpretation. Section 2.10.4 has more review of logarithms but the three common SLR models that use logarithms are given here. Logarithmic model: The regression of \\(Y\\) on \\(\\log(x)\\) has the mean function \\[ \\mu(Y \\mid \\log(x)) = \\beta_0 + \\beta_1 \\log(x) \\] Multiplying \\(x\\) by a factor \\(m\\) is associated with a mean function change of \\[ \\mu(Y \\mid \\log(mx)) - \\mu(Y \\mid \\log(x)) = \\beta_1(\\log(m) + \\log(x)) - \\beta_1 \\log(x) = \\beta_1 \\log(m) \\] If base-2 is used for the tranformation of \\(x\\) then a doubling of \\(x\\) (so \\(m=2\\)) is associated with a change in mean \\(y\\) of \\(\\beta_1 \\log_2(2) = \\beta_1\\). Exponential model: The regression of \\(\\log(Y)\\) on \\(x\\) has the mean function \\[ \\mu(\\log(Y) \\mid x) = median(\\log(Y) \\mid x) = \\beta_0 + \\beta_1 x \\] Since the median of a logged-variable equals the log of the median of the variable, we can “untransform” this model and write it in terms of the median of \\(Y\\) (assuming here that log is the natural log): \\[ median(Y \\mid x) = e^{\\log(median(Y \\mid x))} = e^{median(\\log(Y) \\mid x)} = e^{\\beta_0}e^{\\beta_1 x} \\] Note that the mean of a logged-variable does not equal the log of the mean of the variable, so we can’t express the untransformed model in terms of the median of \\(Y\\). A one unit increase in \\(x\\) is associated with a \\(e^{\\beta_1}\\)-factor change in the median function since \\[ median(Y \\mid x+1) = e^{\\beta_0}e^{\\beta_1 (x+1)} = e^{\\beta_0}e^{\\beta_1 x}e^{\\beta_1} = median(Y \\mid x) e^{\\beta_1} \\] - - Power model: The regression of \\(\\log(Y)\\) on \\(\\log(x)\\) has the mean function \\[ \\mu(\\log(Y) \\mid \\log(x)) = median(\\log(Y) \\mid\\log(x)) = \\beta_0 + \\beta_1 \\log(x) \\] Using the same logic as the exponential model, inference on the untransformed scale of \\(Y\\) is about the median of \\(Y\\): \\[ median(Y \\mid x) = e^{\\log(median(Y \\mid x))} = e^{median(\\log(Y) \\mid x)} = e^{\\beta_0}e^{\\beta_1 \\log(x)} = e^{\\beta_0}(e^{\\log(x)})^{\\beta_1} = e^{\\beta_0}x^{\\beta_1} \\] An m-fold (multiplicative) change in \\(x\\) is associated with a \\(m^{\\beta_1}\\)-factor change in the median function since \\[ median(Y \\mid mx) = e^{\\beta_0}e^{\\beta_1 \\log(mx)} = e^{\\beta_0}(e^{\\log(x)})^{\\beta_1}(e^{\\log(m)})^{\\beta_1} = median(Y \\mid x) m^{\\beta_1} \\] 2.10.4 Review: Logarithms Let \\(b&gt;0\\) and \\(x&gt;0\\). The logarithm (base-\\(b\\)) of \\(x\\) is denoted \\(\\log_b(x)\\) and equal to \\[ \\log_b(x) = a \\] where \\(a\\) tells us what power we must raise \\(b\\) to to obtain the value \\(x\\): \\[ b^a = x \\] Easy examples are: \\(b=2\\), \\(x=8\\) and \\(a=3\\), \\[ \\log_2(8) = 3 \\] since \\(2^3 = 8\\). Or using base \\(b=10\\), then \\[ \\log_{10}(0.01) = -2 \\] since \\(10^{-2} = 0.01\\). Some basic facts logarithm facts are \\[ \\log_b(b) = 1 \\] since \\(b^1 = b\\) and \\[ \\log_b(1) = 0 \\] since \\(b^0 = 1\\). 2.10.4.1 Interpreting logged variables Multiplicative changes in \\(x\\) result in additive changes in \\(\\log_b(x)\\). If \\(m&gt;0\\), then \\[ \\log_b(mx) = \\log_b(m) + \\log_b(x) \\] For example, \\[ \\log_2(16) = \\log_2(2\\times 8) = \\log_2(2) + \\log_2(8) = 1 + 3 = 4 \\] 2.10.4.2 Inverse (i.e. reversing the log, getting rid of the log, …) The logarithm and exponential functions are inverses of one another. This means we can “get rid” of the log by calculating \\(b\\) raised to the logged-function: \\[ b^{\\log_b(x)} = x \\] This will be useful in regression when we have a linear relationship between logged-response \\(y\\) and a set of predictors. We need to For example, suppose we know that \\[ \\log_2(y) = 3 + 5x \\] To return this to an expression on the original (unlogged) scale of \\(y\\), we need take both sides raised to the base 2: \\[ 2^{\\log_2(y)} = 2^{3 + 5x} \\] Simplifying both sides gives \\[ y = 2^3 \\times 2^{5x} \\] 2.10.4.3 Logarithm Practice Questions (day 6) Solutions are posted on the class Moodle site. Write the following as the sum of two logarithms. Simplify as much as possible: \\(\\log_2(2x)\\) \\(\\log_2(0.5x)\\) \\(\\ln(2x)\\) where \\(\\ln\\) is the natural log (base-\\(e\\)) Write the following expressions in terms of \\(y\\), not \\(\\log(y)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3x\\) \\(\\log_{10}(y) = -2 + 0.4x\\) \\(\\ln(y) = 1 - 3x\\) Write the following expressions in terms of \\(y\\) and \\(x\\), not \\(\\log(y)\\) and \\(\\log(x)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3\\log_2(x)\\) \\(\\ln(y) = -2 + 0.4\\ln(x)\\) \\(\\ln(y) = 1 - 3\\log_2(x)\\) Logarithmic model: Regression of \\(Y\\) on \\(\\log(x)\\) obtains the following estimated mean of \\(Y\\): \\[ \\hat{\\mu}(Y \\mid x) = 1 - 3 \\log_2(x) \\] What is the change in estimated mean response if we double the value of \\(x\\)? What is the change in estimated mean response if we triple the value of \\(x\\)? What is the change in estimated mean response if we reduce the value of \\(x\\) by 20%? Exponential model: Regression of \\(\\log_2(Y)\\) on \\(x\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = -2 + 0.4x \\] Write the median in terms of \\(Y\\) instead of \\(\\log_2(Y)\\). Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 1 unit? What is the percent change in estimated median response if we increase \\(x\\) by 1 unit? What is the multiplicative change in estimated median response if we decrease \\(x\\) by 2 units? What is the percent change in estimated median response if we decrease \\(x\\) by 2 units? Power model: Regression of \\(\\log_2(Y)\\) on \\(\\log_2(x)\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = 1 -3\\log_2(x) \\] Write the median in terms of \\(Y\\) and \\(x\\) instead of \\(\\log\\)s. Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 50%? What is the percent change in estimated median response if we increase \\(x\\) by 50%? What is the multiplicative change in estimated median response if we reduce the value of \\(x\\) by 20%? What is the percent change in estimated median response if we reduce the value of \\(x\\) by 20%? 2.11 Examples: Transformations (day 6) A solution to these examples is found on the class Moodle page. 2.11.1 Cars 2004 This is a dataset with stats taken from 230 car makes and models from 2004. (1a) Is city MPG a linear function of car weight (lbs)? &gt; cars &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/cars2004.csv&quot;) &gt; summary(cars[, c(&quot;city.mpg&quot;,&quot;weight&quot;)]) ## city.mpg weight ## Min. :10.00 Min. :1850 ## 1st Qu.:16.00 1st Qu.:3185 ## Median :18.00 Median :3606 ## Mean :19.22 Mean :3738 ## 3rd Qu.:21.00 3rd Qu.:4237 ## Max. :60.00 Max. :7608 &gt; library(ggplot2) &gt; ggplot(cars, aes(x= weight, y = city.mpg)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;, se = FALSE) + + geom_smooth(method = &quot;loess&quot;, se = FALSE, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + + labs(title=&quot;City MPG vs. car weight&quot;) (1b) Explore some transformations: which gives the most linear relationship? Use layers of the type scale_x_log10() or scale_y_sqrt() to change a particular axes (1c) Fit the regression of 1/mpg on weight and check residuals 2.11.2 2005 Residential Energy Survey (RECS) RECS surveys households across the US. We are treating this sample of 4,382 households as an equally-weighted sample of US households. Our goal now is to model total energy costs (CostTotal) as a function of the size of the housing unit (SqftMeasure). (2a) Total energy cost vs. size With over 4,000 cases, we reduce point transparency with alpha to avoid overplotting. Assess the scatterplot in terms of (1) linearity, (2)constant variance, and (3) normal errors. &gt; energy &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/EnergySurvey.csv&quot;) &gt; dim(energy) ## [1] 4382 16 &gt; summary(energy[,c(&quot;CostTotal&quot;,&quot;SqftMeasure&quot;)]) ## CostTotal SqftMeasure ## Min. : 57 Min. : 167 ## 1st Qu.: 1138 1st Qu.: 1056 ## Median : 1673 Median : 1848 ## Mean : 1841 Mean : 2284 ## 3rd Qu.: 2331 3rd Qu.: 3042 ## Max. :10346 Max. :11383 ## NA&#39;s :1 &gt; library(ggplot2) &gt; ggplot(energy, aes(x = SqftMeasure, y = CostTotal)) + + geom_point(alpha = .1) + + geom_smooth(method = &quot;loess&quot;, se = FALSE) (2b) Find a transformation that yields a linear model As done in example 1, use trial-and-error to find a transformation that yields a a linear relationship. Once this is found, we can fit a SLR model to the transformed variables. The basic transformations to consider (for \\(y\\), or \\(x\\), or both) are log10 (log base-10), sqrt, and inverse. (2c) Fit the “best” model from (2b) (2d) Once you find a model that “fits”, interpret your model parameters. What is the fitted model? How does cost change if house size is doubled? Get a CI for this effect. How does cost change if house size decreases by 10%? Get a CI for the effect. 2.12 \\(R^2\\) and ANOVA for SLR After we check the fit of a model and determine that there are no concerns with the SLR assumptions, we can summarize it with inference, interpretations and measures of “strength” of the association. One common measure of strength is called “R-squared”: \\[ R^2 = 1- \\dfrac{(n-2)\\hat{\\sigma}^2}{(n-1)s^2_y} \\] \\(R^2\\) measures the proportion of total variation in the response that is explained by the model. Total variation in \\(y\\) is measured, in part, by the sample SD of the response, which is a piece of the denominator above. The numerator measures the varation in \\(y\\) around the regression line (\\(\\hat{\\sigma}\\)), which is a measure of unexplained, or residual, variation. One minus the unexplained variation proportion gives us the value of \\(R^2\\). 2.12.1 Example: \\(R^2\\) Let’s revisit the woodpecker nest depth model: &gt; wpdata&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; library(ggplot2) &gt; ggplot(wpdata, aes(x=temp, y=depth)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + labs(x=&quot;air temperature (C)&quot;, y=&quot;nest depth (cm)&quot;, title=&quot;woodpeckers scatterplot&quot;) &gt; wood.lm&lt;- lm(depth~temp, data=wpdata) We get \\(R^2\\) in the Multiple R-squared entry in the lm summary output: &gt; summary(wood.lm) ## ## Call: ## lm(formula = depth ~ temp, data = wpdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8066 -1.3321 -0.6529 0.6811 4.8512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.12228 0.94024 21.401 1.11e-09 *** ## temp -0.34218 0.05961 -5.741 0.000188 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.335 on 10 degrees of freedom ## Multiple R-squared: 0.7672, Adjusted R-squared: 0.7439 ## F-statistic: 32.96 on 1 and 10 DF, p-value: 0.0001875 Here we have \\(R^2 = 0.7672\\), meaning the regression of depth on temp helps explain about 76.7% of the observed variation in depth. We can see how R computes this value by looking at the sample SD of \\(y\\), \\(s^2_y = 4.6133124\\) and the estimated model SD \\(\\hat{\\sigma} =2.3345298\\) &gt; sd(wpdata$depth) # sample SD of y ## [1] 4.613312 &gt; summary(wood.lm)$sigma # mode SD estimate ## [1] 2.33453 &gt; n &lt;- 12 &gt; (n-2)*summary(wood.lm)$sigma^2/((n-1)*sd(wpdata$depth)^2) # unexplained ## [1] 0.2327986 &gt; 1 - (n-2)*summary(wood.lm)$sigma^2/((n-1)*sd(wpdata$depth)^2) # explained ## [1] 0.7672014 2.12.2 ANOVA for SLR Analysis of Variance (ANOVA) decomposes the total variation in \\(y_i\\), \\(y_i - \\bar{y}\\), into a portion that is explained by a model, \\(\\hat{y}_i - \\bar{y}\\), and a portion that is unexplained, \\(y_i - \\hat{y}_i\\): \\[ y_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) \\] For row 2 in the wpdata, the total variation is show in red in the plot below while the explained variation is in blue and unexplained in green. Notice that the “explained” portion is what gets us closer to the actual response \\(y=26\\) (compared to just the mean response), which is what you would expect to see if temp is useful in explaining why we see variation in nest depths. The next step in ANOVA, is to total the squared variation distances across all \\(n\\) data points (squared because we don’t care if the differences are positive or negative). Some very useful mathematics is used to prove that the total squared variation of all \\(n\\) cases is equal to \\[ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 \\] The three components are called: total variation: \\(SST = \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1)s^2_y\\) regression (explained) variation: \\(SSreg = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\) residual (unexplained) variation: \\(SSR = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = (n-2)\\hat{\\sigma}^2\\) These sum of squares (SS) can also be used to compute \\(R^2\\) since \\[ R^2 = 1- \\dfrac{(n-2)\\hat{\\sigma}^2}{(n-1)s^2_y} = 1- \\dfrac{SSR}{SST} = \\dfrac{SSreg}{SST} \\] 2.12.3 Example: ANOVA Back to the nest depth model. The anova function extracts the sum of square values from our lm: &gt; anova(wood.lm) # ANOVA Table ## Analysis of Variance Table ## ## Response: depth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## temp 1 179.61 179.61 32.956 0.0001875 *** ## Residuals 10 54.50 5.45 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For this model we have - regression (explained) variation: from the temp row, \\(SSreg = 179.61\\) - residual (unexplained) variation: from the Residuals row, \\(SSR = 54.60\\) - total variation: adding the two SS values gives total variation \\(SST = SSreg + SSR = 179.61 + 54.60 = 234.21\\) The ANOVA SS can also give us the value of \\(R^2\\) for the model: \\[ R^2 = \\dfrac{179.61}{179.61 + 54.60} = 0.7668759 \\] "],
["mlr.html", "Chapter 3 Multiple Regression 3.1 The variables 3.2 The model form 3.3 Example: MLR fit and visuals 3.4 Categorical Predictors 3.5 Inference for MLR 3.6 ANOVA for MLR 3.7 Model Checking 3.8 MLR: Visualizing effects 3.9 Collinearity", " Chapter 3 Multiple Regression This chapter covers material from chapters 9-12 of Sleuth. 3.1 The variables Suppose we have a quantitative response variable \\(y\\) that we want to relate to \\(p\\) explantory variable (aka predictors, covariates) \\(x_1, \\dotsc, x_p\\). There is no restriction on the type of covariates, they can be both quantitative and categorical variables. 3.2 The model form This section describes the multiple linear regression (MLR) model for a particular population of interest. Another way to frame the model is that it describes a hypothetical data generating process (DGP) that was used to generate the sample of data that we have on hand. The major change in the MLR model compared to the SLR model is that now the mean function \\(\\mu_{y\\mid x_1, \\dotsc, x_p}\\) is a function of all covariates. The expression for \\(\\mu\\) must be linear with respect to the \\(\\beta\\) parameters even if we used a function of a predictor like \\(\\log(x)\\). The expression for \\(\\mu\\) is can even involve polynomial functions of \\(x\\) like \\(x^2\\) or interactions of predictors like \\(x_1 \\times x_p\\). In this section, we will describe the basic MLR with the simplest form it can take: \\[ \\mu_{y\\mid x_1, \\dotsc, x_p} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dotsm \\beta_p x_{p,i} \\] More complicated forms will be described later in this chapter. Let \\(Y_i\\) be the response from unit \\(i\\) that has explanatory (aka predictor) values \\(x_{1,i}, x_{2,i}, \\dotsc, x_{p,i}\\). There are two equivalent ways to express the SLR model for \\(Y\\): Conditional normal model: \\[ Y_i \\mid x_{1,i}, x_{2,i}, \\dotsc, x_{p,i}\\sim N(\\mu_{y\\mid x} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dotsm \\beta_p x_{p,i}, \\sigma) \\] Mean + error: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dotsm \\beta_p x_{p,i} + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\end{equation}\\] Both expressions of the MLR model above say the same thing: Linear Mean: \\(\\mu_{y\\mid x}\\) describes the population mean value of \\(Y\\) given all predictor values and it is linear with respect to the \\(\\beta\\) parametrs. (We still have a “linear” model even if we used \\(\\log(x)\\) or \\(x^2\\)!) Constant SD: \\(SD(Y\\mid x)=\\sigma\\) describes the SD of \\(Y\\)’s in the population around a given mean value \\(\\mu_{y\\mid x}\\). The fact that this SD does not depend on the value of \\(x\\) is called the contant variance, or homoscedastic, assumption. Normality: The shape of population response values around \\(\\mu_{y\\mid x}\\) is described by a normal distribution model. Indepedence: Given a predictor value of \\(x\\), all responses \\(Y\\) occur independently of each other. There are a total of \\(\\pmb{p+1}\\) parameters in this MLR model: the \\(p\\) mean parameters \\(\\beta_0, \\beta_1, \\dotsc, \\beta_p\\) the SD parameter \\(\\sigma\\) 3.2.1 Interpretation How a predictor influences the mean response is determined by the form of the \\(\\mu\\) function. Some common forms are discussed here. 3.2.1.1 Planar model The mean model described above models the relationship between \\(y\\) and all corvariates as a \\((p+1)-\\)dimensional plane: \\[ \\mu_{y\\mid x_1, \\dotsc, x_p} = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\dotsm \\beta_p x_{p} \\] When \\(p=1\\), we have a SLR and this “plane” simplifies to a 2-d line. When \\(p=2\\), the mean “surface” is a 3-D plane. The plot below displays an example of a sample of point triples \\((x_{1,i}, x_{2,i}, y_i)\\) that form a point “cloud” that is floating in the x-y-z coordinate system. The mean function is a plane that floats through the “middle” of the point cloud, hitting the mean value of \\(y\\) for each combination of \\(x_1\\) and \\(x_2\\). Notice that when we “fix” one of the predictor values, the trend between \\(y\\) and the other predictor is linear. E.g. Pick any place on the mean surface, then any place you “trace” along the surface will result in a line. How should we interpret the \\(\\beta\\)’s in our planar mean function? \\(\\beta_0\\) is the mean response when all predictor values are 0 since \\(\\mu_{y \\mid 0} = \\beta_0 + \\beta_1(0) + \\dotsm + \\beta_p (0)= \\beta_0\\). \\(\\beta_j\\) tells us how the mean response changes for a one unit increase in \\(x_j\\) holding all other predictors fixed. We can illustrate this for the the predictor \\(x_1\\). The mean function at \\(x_1 + 1\\), holding \\(x_2, \\dotsc, x_p\\) fixed, is \\[ \\begin{split} \\mu(y \\mid x_1+1, x_2, \\dotsc, x_p) &amp;= \\beta_0 + \\beta_1 (x_{1}+1) + \\beta_2 x_{2} + \\dotsm + \\beta_p x_p \\\\ &amp; = \\beta_0 + \\beta_1 x_{1} + \\beta_1 + \\beta_2 x_{2} + \\dotsm + \\beta_p x_p \\\\ &amp; = \\mu(y \\mid x_1, x_2, \\dotsc, x_p) + \\beta_1 \\end{split} \\] This shows that a 1 unit increase in \\(x_1\\) is associated with a \\(\\beta_1\\) change in the mean response holding all other predictors fixed. This holds in general too: a 1 unit increase in \\(x_j\\) is associated with a \\(\\beta_j\\) change in the mean response holding all other predictors fixed. 3.2.1.2 Quadratic model A model that incorporates polynomial functions of predictors, like \\(x^2, x^3\\), etc, is also a MLR model. Here is an example that says the mean of \\(y\\) is a quadratic function of \\(x_1\\) but a linear function of \\(x_2\\): \\[ \\mu_{y\\mid x_1, x_2} = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{1}^2 + \\beta_3 x_{2} \\] Notice that this model has two covariates \\(x_1\\) and \\(x_2\\) but four mean function parameters \\(\\beta_0 - \\beta_4\\) due to the extra quadratic term. An example of this model is visualized below (with all \\(\\beta\\)’s equal to 1). You see the quadratic relationship with \\(x_1\\) when you orient the \\(x_1\\) axis to be the left-to-right axes with the \\(x_2\\) axis coming out of the page. When these axes are flipped, you see a linear relationship. How should we interpret the \\(\\beta\\)’s in this quadratic mean function? \\(\\beta_0\\) is the mean response when all predictor values are 0. \\(\\beta_3\\) tells us how the mean response changes for a one unit increase in \\(x_2\\) holding \\(\\pmb{x_1}\\) fixed. \\(\\beta_1\\) and \\(\\beta_2\\) tell us how the mean response changes as a function of \\(x_1\\), but since it is quadratic the exact change in the mean response depends on the value of \\(x_1\\). E.g. the closer the \\(x_1\\) value is to the “top” or “bottom” of the quadratic curve, the smaller the changes in the mean response. We can illustrate this for the the predictor \\(x_1\\). The mean function at \\(x_1 + 1\\), holding \\(x_2\\) fixed, is \\[ \\begin{split} \\mu(y \\mid x_1+1, x_2) &amp;= \\beta_0 + \\beta_1 (x_{1}+1)+ \\beta_2 (x_{1}+1)^2 + \\beta_3 x_{2} \\\\ &amp; = \\beta_0 + \\beta_1 x_{1} + \\beta_1 + \\beta_2 x_{1}^2 + \\beta_2 2x_1 + \\beta_2 + \\beta_3 x_2 \\\\ &amp; = \\mu(y \\mid x_1, x_2) + \\beta_1 + \\beta_2(2x_1+1) \\end{split} \\] This shows that a 1 unit increase in \\(x_1\\) is associated with a \\(\\beta_1 + \\beta_2(2x_1+1)\\) change in the mean response holding all other predictors fixed. For example, if \\(x_1 = 1\\) the mean change is \\(\\beta_1 + 3\\beta_2\\). 3.2.1.3 Interactions A model that incorporates predictor interactions says that the effect of one predictor is dependent on the value of the other predictor, and vica versa. Here is an example that has the interaction of \\(x_1\\) and \\(x_2\\): \\[ \\mu_{y\\mid x_1, x_2} = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\beta_3 x_1x_{2} \\] An example of this model is visualized below (with all \\(\\beta\\)’s equal to 1). We obviously don’t see a planar surface. It’s a bit hard to see, but any “slice” you take from the surface along the \\(x_1\\) axis will create a linear function along the \\(x_2\\) axis. The trend, or steepness, of this line depends on the value you chose for \\(x_1\\). Switching the \\(x_1\\) and \\(x_2\\) variables results in the same observation. The effect of each predictor is linear, but its slope, or effect size, is a function of the other predictor. How should we interpret the \\(\\beta\\)’s in this interaction mean function? \\(\\beta_0\\) is the mean response when all predictor values are 0. \\(\\beta_1\\) tells us how the mean response changes for a one unit increase in \\(x_1\\) when \\(\\pmb{x_2=0}\\). \\(\\beta_2\\) tells us how the mean response changes for a one unit increase in \\(x_2\\) when \\(\\pmb{x_1=0}\\). \\(\\beta_3\\) is the interaction effect that tells us how the effect of \\(x_1\\) varies as a function of \\(x_2\\), and vice versa. We can illustrate this for the the predictor \\(x_1\\). The mean function at \\(x_1 + 1\\), holding \\(x_2\\) fixed, is \\[ \\begin{split} \\mu(y \\mid x_1+1, x_2) &amp;= \\beta_0 + \\beta_1 (x_{1}+1)+ \\beta_2 x_2+ \\beta_3 (x_1+1)x_{2} \\\\ &amp; = \\beta_0 + \\beta_1 x_{1} + \\beta_1 + \\beta_2 x_{2} + \\beta_3x_1x_2 + \\beta_3x_2 \\\\ &amp; = \\mu(y \\mid x_1, x_2) + \\beta_1 + \\beta_3x_2 \\end{split} \\] This shows that a 1 unit increase in \\(x_1\\) is associated with a \\(\\beta_1 + \\beta_3x_2\\) change in the mean response holding all other predictors fixed. For example, if \\(x_2 = 5\\) the mean change is \\(\\beta_1 + 5\\beta_3\\). Similary, a 1 unit increase in \\(x_2\\) is associated with a \\(\\beta_2 + \\beta_3x_1\\) change in the mean response holding all other predictors fixed. 3.3 Example: MLR fit and visuals 3.3.1 lm fit We fit a MLR model in R using the same command as a SLR model, but we add model predictors on the right-hand side of the formula. Examples include: planar model: lm(y ~ x1 + x2 + x3, data=, subset=) interaction model: lm(y ~ x1 + x2 + x1:x2 + x3, data=, subset=) or lm(y ~ x1*x2 + x3, data=, subset=) quadratic model: lm(y ~ x1 + I(x1^2) + x2, data=, subset=). This uses the “as is” operator I() that tells R that ^ is interpreted as a power rather than as its symbolic use in a formula (see ?formula for more details) Here is the multiple linear regression of brain weight (g) on gestation (days), body size (kg) and litter size from Case Study 9.2: &gt; library(Sleuth3) &gt; brain &lt;- case0902 &gt; brain.lm &lt;- lm(Brain ~ Gestation + Body + Litter, data=brain ) &gt; brain.lm ## ## Call: ## lm(formula = Brain ~ Gestation + Body + Litter, data = brain) ## ## Coefficients: ## (Intercept) Gestation Body Litter ## -225.2921 1.8087 0.9859 27.6486 The estimated mean function is \\[ \\hat{\\mu}(brain \\mid gest,body,litter) = -225.2921 + 1.8087 Gestation + 0.9859 Body + 27.6486 Litter \\] Holding gestation length and body weight fixed, increasing litter size by one baby increases estimated mean brain weight by 27.6g. But is this an appropriate model to use? This interpretation is meaningless if the model doesn’t fit the data! We need to check this with scatterplots and residual plots. 3.3.2 Graphics for MLR If we have \\(p\\) predictors in our model, then the MLR model can be viewed in a (at least) \\(p\\)-dimensional picture! Viewing this is difficult, if not impossible. The best we can do is look at 2-d scatterplots of \\(y\\) vs. all the predictor variables. (But, unfortunately, what we see in these 2-d graphs doesn’t always explain to us what we will “see” in the MLR model.) 3.3.2.1 Scatterplot matrix A scatterplot matrix plots all pairs of variables used in a model. The primary plots of interest will have the response \\(y\\) on the y-axis and the predictors on the x-axis. But the predictor plots (e.g. \\(x_1\\) vs. \\(x_2\\)) are useful to see if any predictors are related, which is a topic dicussed in more detail later in these notes. The basic scatterplot matrix command pairs takes in a data frame, minus any columns you don’t want plotted: &gt; names(brain) ## [1] &quot;Species&quot; &quot;Brain&quot; &quot;Body&quot; &quot;Gestation&quot; &quot;Litter&quot; &gt; # we want to omit column 1 (Species) from our graph: &gt; pairs(brain[,-1]) The top row shows scatterplots of the response Brain vs. the three predictors. All three indicate that transformations of all variables should be explored. A slightly nicer version that includes univariate density curves and correlation coefficients is made using ggpairs in the GGally package. This option fits a smoother curve to the scatterplots: &gt; library(GGally) &gt; ggpairs(brain, columns = c(&quot;Body&quot;,&quot;Gestation&quot;, &quot;Litter&quot;,&quot;Brain&quot;), + lower = list(continuous = &quot;smooth&quot;)) In this plot command, we select variables from brain in the columns argument with the response Brain listed last to make the lower row of the plot show the response Brain on the y-axis vs. all three predictors on the x-axes. Conclusion: transformations are likely needed. Since all variables have positive values, we can try logs first. We can’t use a scale argument in a scatterplot matrix to explore transformations. Instead, we can use the dplyr package to transform all variables (except Species) using the mutate_all function, then we plot using ggpairs again. Here we also added the ggpairs argument columnLabels to remind us what transformations were made: &gt; library(dplyr) &gt; brain %&gt;% select(-Species) %&gt;% # omit species + mutate_all(.funs = log) %&gt;% # apply the log function to all variables + ggpairs(columns = c(&quot;Body&quot;,&quot;Gestation&quot;, &quot;Litter&quot;,&quot;Brain&quot;), + lower = list(continuous = &quot;smooth&quot;), + columnLabels = c(&quot;log(Body)&quot;,&quot;log(Gestation)&quot;, &quot;log(Litter)&quot;,&quot;log(Brain)&quot;)) Does Litter need to be logged? If we don’t want to log all variables, we can use mutate_at instead of mutate_all: &gt; brain %&gt;% select(-Species) %&gt;% # omit species + mutate_at(.vars = c(&quot;Brain&quot;,&quot;Body&quot;,&quot;Gestation&quot;), .funs = log) %&gt;% # pick variables to log + ggpairs(columns = c(&quot;Body&quot;,&quot;Gestation&quot;, &quot;Litter&quot;,&quot;Brain&quot;), + lower = list(continuous = &quot;smooth&quot;), + columnLabels = c(&quot;log(Body)&quot;,&quot;log(Gestation)&quot;, &quot;Litter&quot;,&quot;log(Brain)&quot;)) 3.3.2.2 Jittered scatterplot: Jittered plots are useful when data points overlap (discrete variables like litter) and your sample size isn’t huge. Change the alpha transparency value with large data sets. Here we compare geom_point() against geom_jitter, using grid.arrange from the gridExtra package to put these plots side-by-side: &gt; base &lt;- ggplot(brain, aes(x=Litter, y=Brain)) + scale_y_log10() &gt; plotA &lt;- base + geom_point() + labs(title=&quot;unjittered&quot;) &gt; plotB &lt;- base + geom_jitter(width=.1) + labs(title=&quot;jittered&quot;) &gt; library(gridExtra) &gt; grid.arrange(plotA,plotB, nrow=1) The width=.1 amount specifies how much to jitter the points (we changed it here because the default amount didn’t display enough of a change). We can see the jittering most in the points clustered around Litter=0. 3.3.3 Residual plots for MLR If we have \\(p\\) predictors, then there are \\(p+1\\) residual plots to check: plot \\(r_i\\) against all predictors \\(x_1, \\dotsc, _p\\). The motivation for these plots is the same as SLR (residuals should not be related to the \\(x\\)’s) plot \\(r_i\\) against the fitted values \\(\\hat{y}_i\\). The motivation for this may be less clear, but the fitted values are simply a linear function of the predictor values: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1,i} + \\hat{\\beta}_2 x_{2,i} + \\dotsm + \\hat{\\beta}_p x_{p,i} \\] so the residuals should not be related to the fitted values if the model fits. All \\(p+1\\) plots should be checked. A quick starting point is the fitted value plot which is the first plot when plot-ing the lm. Here is the residuals vs. fitted plot for the untransformed variable model: &gt; # check residuals vs. fitted (y-hat) values: &gt; plot(brain.lm, which=1) This plot indicates non-linearity and non-constant variance. Here is the same residual plot for the model will all variables logged except Litter: &gt; brain.lm2 &lt;- lm(log(Brain) ~ log(Gestation) + log(Body) + Litter, data=brain) &gt; plot(brain.lm2, which=1) We can use the ggnostic function in GGally to get plots of residuals vs. all predictors: &gt; ggnostic(brain.lm2, columnsY = &quot;.resid&quot;) # from GGally The residual plots reveal some outliers that should be explored but the overall fit, while not perfect, is much better than the untransformed version. &gt; library(broom) &gt; library(knitr) &gt; kable(tidy(brain.lm2), digits=4) term estimate std.error statistic p.value (Intercept) 0.8234 0.6621 1.2437 0.2168 log(Gestation) 0.4396 0.1370 3.2095 0.0018 log(Body) 0.5745 0.0326 17.6009 0.0000 Litter -0.1104 0.0423 -2.6115 0.0105 The estimated median function for this model is \\[ \\widehat{median}(y \\mid x) = e^{0.8234}(Gest)^{0.4396}(Body)^{0.5745}e^{-0.1104(Litter)} \\] The effect of Gestation is interpreted as you would in a power model (both variables logged). For example, doubling gestational days is associated with an estimated $(2^{0.4396}-1)100%=$35.6% increase in median brain weight, holding body weight and litter size constant. The effect of Litter is interpreted as you would in an exponential model (with just the response logged). For example, after controlling for body size and gestation time, each additional offspring decreases estimated median brain weight by 10.5% (work is \\((e^{-0.1104}-1)100%=\\)). 3.3.4 EDA for interactions We visualize interactions by using a graphic that looks at the relationship between \\(y\\) and \\(x_1\\) while holding the value of \\(x_2\\) fixed (or almost fixed), and vice versa for flipping the role of \\(x\\) variables. An interaction may be needed if the relationship between \\(y\\) and \\(x_1\\) depends on the value of \\(x_2\\). 3.3.4.1 Predictors: one quantitative and one categorical For example, if \\(x_1\\) is quantitative and \\(x_2\\) is categorical, then we use facet_wrap (or facet_grid) to split the scatterplot of \\(y\\) vs. \\(x_1\\) by group of \\(x_2\\): &gt; ggplot(mydata, aes(x=x1, y=y)) + + geom_point() + + facet_wrap(~x2) 3.3.4.2 Predictors: both quantitative We use the same facet_wrap function, but we need to group the data into similar cases with respect to their \\(x_2\\) value, which is now assumed to be quantitative. One way to do this is to use the ntile(n=) function from dplyr package where n determines how many groups the data will be divided into. Is there an interaction between body weight and gestation?? Does the relationship between brain weight and gestation length change as we vary body weight? If yes, then we should include an interaction term between gestation and body weight. Here we hold body weight “fixed” by using the ntile command from the dplyr package to divide the data into chunks of animals with similar body weights. Here we pick n=4 for this function which divides the cases into 4 equal sized chunks based on the quartiles (4) of Body. In the plot below the cases in “1” are the lower 25% of body weights, “2” are the 25-50th percentile values of Body weights, etc. &gt; ggplot(brain, aes(x=Gestation, y=Brain)) + + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + scale_x_log10() + scale_y_log10() + + facet_wrap(~ ntile(Body, n=4)) Conclusion: the trend within each level of body weight is about the same. No obvious interactive effect of body weight and gestation length on brain weight. Of course we can always check significance by adding interaction term to model: &gt; brain.lm3 &lt;- lm(log(Brain) ~ log(Gestation) + log(Body) + Litter + log(Gestation):log(Body), data=brain) &gt; summary(brain.lm3) ## ## Call: ## lm(formula = log(Brain) ~ log(Gestation) + log(Body) + Litter + ## log(Gestation):log(Body), data = brain) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95837 -0.29354 0.01594 0.27960 1.62013 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.64124 0.67137 0.955 0.342050 ## log(Gestation) 0.48758 0.14051 3.470 0.000797 *** ## log(Body) 0.69602 0.09268 7.510 3.91e-11 *** ## Litter -0.10050 0.04264 -2.357 0.020566 * ## log(Gestation):log(Body) -0.02678 0.01914 -1.399 0.165080 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4731 on 91 degrees of freedom ## Multiple R-squared: 0.9545, Adjusted R-squared: 0.9525 ## F-statistic: 477.4 on 4 and 91 DF, p-value: &lt; 2.2e-16 3.3.5 Quadratic models: Corn yields (exercise 9.15) This final example illustrates a quadratic model fit and scatterplot. Consider the corn yield data in textbook exercise 9.15. How is corn yield (measured bushels/acre) in a year related to the amount of rainfall (inches) in that summer? A linear model for yield against rainfall is not appropriate: &gt; corn &lt;- ex0915 &gt; summary(corn) ## Year Yield Rainfall ## Min. :1890 Min. :19.40 Min. : 6.800 ## 1st Qu.:1899 1st Qu.:29.95 1st Qu.: 9.425 ## Median :1908 Median :32.15 Median :10.500 ## Mean :1908 Mean :31.92 Mean :10.784 ## 3rd Qu.:1918 3rd Qu.:35.20 3rd Qu.:12.075 ## Max. :1927 Max. :38.30 Max. :16.500 &gt; ggplot(corn, aes(x=Rainfall, y=Yield)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + labs(title=&quot;SLR model&quot;) &gt; corn.lm1&lt;-lm(Yield ~ Rainfall, data =corn) &gt; plot(corn.lm1, which=1) We can add a quadratic term (using the I() operator): &gt; corn.lm2 &lt;- lm(Yield ~ Rainfall + I(Rainfall^2), data =corn) Alternatively, we can update model 1 (SLR) using the update command: on right side of formula the ~ . + newstuff says to add the newstuff to the old model formula which is denoted with the period . . &gt; corn.lm2 &lt;- update(corn.lm1, ~ . + I(Rainfall^2)) &gt; summary(corn.lm2) ## ## Call: ## lm(formula = Yield ~ Rainfall + I(Rainfall^2), data = corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.4642 -2.3236 -0.1265 3.5151 7.1597 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.01467 11.44158 -0.438 0.66387 ## Rainfall 6.00428 2.03895 2.945 0.00571 ** ## I(Rainfall^2) -0.22936 0.08864 -2.588 0.01397 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.763 on 35 degrees of freedom ## Multiple R-squared: 0.2967, Adjusted R-squared: 0.2565 ## F-statistic: 7.382 on 2 and 35 DF, p-value: 0.002115 &gt; plot(corn.lm2, which=1) This residual plot looks much better for this quadratic model compared to the linear model. We can visualize this quadratic model using the geom_smooth(method=\"lm\") function but we have to specify this model form since the default is a SLR. This is done in the formula argument, using y and x to denote the x and y that you specify in the aes argument. &gt; ggplot(corn, aes(x=Rainfall, y=Yield)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, formula= y ~ x + I(x^2), se=FALSE) + + labs(title=&quot;quadratic fit!&quot;) The model fit is &gt; kable(tidy(corn.lm2), digits = 4) term estimate std.error statistic p.value (Intercept) -5.0147 11.4416 -0.4383 0.6639 Rainfall 6.0043 2.0389 2.9448 0.0057 I(Rainfall^2) -0.2294 0.0886 -2.5877 0.0140 so \\[ \\hat{\\mu}(yield \\mid rain) = -5.0147 + 6.0043(rain)- 0.2294(rain)^2 \\] An increase from 9 to 10 inches of rainfall is associated with a mean yield increase of 1.646 bushels per acre. \\[ 6.0043- 0.2294(2\\times 9 + 1) = 1.646 \\] An increase from 14 to 15 inches of rainfall is associated with a mean yield decrease of 0.648 bushels per acre. \\[ 6.0043- 0.2294(2\\times 14 + 1) = -0.648 \\] 3.4 Categorical Predictors Categorical predictors can be included in a regression model the same way that a quantitative predictor can. But since a categorical variable doesn’t have a numerical scaling, we don’t have a “linearity” assumption that needs to be met (though the three other assumptions still hold). A categorical variable is included by using one or more indicator variables (aka dummy variables) which indicate different levels of the variable. An indicator variable equals 1 to indicate the level of interest, and is 0 otherwise. The baseline level of an indicator variable is the level of the factor variable that doesn’t have an indicator variable made for it. R will create indicator variables for us in an lm, so there is no need to do this “by hand”. If a variable, which is stored as a factor in R, has \\(k\\) levels then we need \\(k-1\\) indicator variables. Consider the following examples to illustrate this idea. Suppose we are looking at how gender and education level are associated with income. If there are two genders in our data, here recorded as Female and Male, then we need one indicator to indicate one of the two levels. R will create an indicator for the second level, ordered alphabetically. If a case has Indicator_Male=1 then the case is Male but if the case has Indicator_Male=0 then the case is Female. So this one indicator variable can classify two levels and the baseline level is Female. Gender Indicator_Male Female 0 Male 1 Suppose we are looking at how number of farms per county in 1992 is related to number of farms in 1987 and region of the country. If there are four regions in our data, here recorded as NC, NE, S and W, then we need three indicators. R will create an indicator for all but the first level of NC. If a case has Indicator_NE=1 then the case is NE, if the case has Indicator_S=1 then the case is S, if the case has Indicator_W=1 then the case is W. If a case has all indicator values equal to 0, then the case is the baseline level of NC. So we need three indicators to classify the four region levels. Region Indicator_NE Indicator_S Indicator_W NC 0 0 0 NE 1 0 0 S 0 1 0 W 0 0 1 3.4.1 Interpretation: adding a categorical You add a categorical predictor x2 to the lm function just as you would any quantitative predictor: lm(y ~ x1 + x2, data). R automatically creates indicator variables for x2. When adding a categorical x2 which has, say levels A, B and C, the basic mean function form looks like: \\[ \\mu(Y \\mid x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_3 LevelB + \\beta_4 LevelC \\] The mean function for cases where \\(x_2=A\\) sets the indicators for levels B and C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=A) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (0) = \\beta_0 + \\beta_1 x_1 \\] The mean function for cases where \\(x_2=B\\) sets the indicator for level B equalt to 1 and the indicator for level C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=B) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (1) + \\beta_4 (0) = \\beta_0 + \\beta_1 x_1 + \\beta_3 \\] The mean function for cases where \\(x_2=C\\) sets the indicator for level C equalt to 1 and the indicator for level B equal to 0: \\[ \\mu(Y \\mid x_1, x_2=C) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (1) = \\beta_0 + \\beta_1 x_1 + \\beta_4 \\] Interpretation of indicator effects: \\(\\beta_3\\) is the difference between \\(\\mu(Y \\mid x_1, x_2=B)\\) and \\(\\mu(Y \\mid x_1, x_2=A)\\), so it measures the mean change between levels B and A, holding \\(x_1\\) fixed. \\(\\beta_4\\) is the difference between \\(\\mu(Y \\mid x_1, x_2=C)\\) and \\(\\mu(Y \\mid x_1, x_2=A)\\), so it measures the mean change between levels C and A, holding \\(x_1\\) fixed. \\(\\beta_3-\\beta_4\\) is the difference between \\(\\mu(Y \\mid x_1, x_2=B)\\) and \\(\\mu(Y \\mid x_1, x_2=C)\\), so it measures the mean change between levels B and C, holding \\(x_1\\) fixed. 3.4.1.1 Example: Agstrat Consider the agstrat data again, fitting the regression of square root of farms92 (number of farms/county in 1992) against square root of farms87 and region. &gt; agstrat&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/agstrat.csv&quot;) &gt; farms.lm &lt;- lm(sqrt(farms92) ~ sqrt(farms87) + region, data=agstrat) &gt; farms.lm ## ## Call: ## lm(formula = sqrt(farms92) ~ sqrt(farms87) + region, data = agstrat) ## ## Coefficients: ## (Intercept) sqrt(farms87) regionNE regionS regionW ## -0.9369 0.9798 -0.0826 0.7214 1.0100 The fitted model mean function is estimated as \\[ \\begin{split} \\hat{\\mu}(\\sqrt{farms92} \\mid \\sqrt{farms92}, region) &amp; = -0.9369 + 0.9798 \\sqrt{farms92} -0.0826(regionNE) + \\\\ &amp; 0.7214(regionS) + 1.0100(regionW) \\end{split} \\] For the baseline NC region, the estimated mean function is \\[\\begin{split} \\hat{\\mu}(\\sqrt{farms92} \\mid \\sqrt{farms92}, region=NC) &amp; = -0.9369 + 0.9798 \\sqrt{farms92} -0.0826(0) + \\\\ &amp; 0.7214(0)+ 1.0100(0) \\\\ &amp; = -0.9369 + 0.9798 \\sqrt{farms92} \\end{split} \\] For the S region, the estimated mean function is \\[\\begin{split} \\hat{\\mu}(\\sqrt{farms92} \\mid \\sqrt{farms92}, region=S) &amp; = -0.9369 + 0.9798 \\sqrt{farms92} -0.0826(0) + \\\\ &amp; 0.7214(1)+ 1.0100(0)\\\\ &amp; = -0.9369 + 0.9798 \\sqrt{farms92}+ 0.7214 \\\\ &amp; = -0.2155+ 0.9798 \\sqrt{farms92} \\end{split} \\] Interpretation \\(\\hat{\\beta}_3 = 0.7214\\) tells us that the mean square root farms92 is estimated to be 0.7214 units higher in the S compared to the NC region, holding farms87 constant. But the effect of farms87 is the same, regardless of region: holding region constant, a one unit increase in the square root of farms87 is associated with a 0.9798 unit increase in the mean square root of farms92. With one quatitative preditor and one categorical predictor, we can visual this data by coloring plot symbols by region. We can also add parallel lines that show how farms87 is related to farms92 in each region. We do this by adding the model’s fitted values (\\(\\hat{y}\\)’s) to the augmented data frame and plotting lines, by region, with these fitted values as the y-axis measurement. &gt; library(ggplot2) &gt; library(broom) &gt; agstrat.aug &lt;- augment(farms.lm) &gt; head(agstrat.aug) ## # A tibble: 6 x 10 ## sqrt.farms92. sqrt.farms87. region .fitted .se.fit .resid .hat .sigma ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26.9 29.3 NC 27.7 0.0895 -0.821 0.00977 0.906 ## 2 25.7 25.9 NC 24.4 0.0902 1.21 0.00992 0.905 ## 3 39.8 41.6 NC 39.9 0.121 -0.0895 0.0178 0.907 ## 4 34.1 35.7 NC 34.1 0.100 0.0269 0.0123 0.907 ## 5 21.2 22.0 NC 20.6 0.0965 0.569 0.0113 0.907 ## 6 24.1 26.4 NC 25.0 0.0898 -0.823 0.00983 0.906 ## # ... with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; &gt; ggplot(agstrat.aug, aes(x=sqrt.farms87., y=sqrt.farms92., color=region)) + + geom_point() + + geom_line(data=agstrat.aug, aes(y=.fitted), size=1) + + labs(title=&quot;Parallel line model&quot;, + x=&quot;square root of number of farms in 1987&quot;, + y=&quot;square root of number of farms in 1992&quot;) The difference of \\(\\hat{\\beta}_3 = 0.7214\\) is shown by the fact that the blue South line is always 0.7214 units above the red North Central line for any value of farms87. The small value of \\(\\hat{\\beta}_2 = -0.0826\\) for the NE indicator show that there is basically no difference between the NE and NC lines, which can be seen by the almost overlapping red and green lines. 3.4.2 Interpretation: adding a categorical interaction What if we thought that the effect of \\(x_1\\) depends on the level of the categorical variable \\(x_2\\). E.g. What if the slopes for each region in the above scatterplot were allowed different? To create a separate lines model we would add the interaction of \\(x_1\\) and \\(x_2\\): lm(y ~ x1 * x2, data). When adding a categorical x2 which has, say levels A, B and C, the basic mean function form looks like: \\[ \\mu(Y \\mid x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_3 LevelB + \\beta_4 LevelC + \\beta_5 x_1LevelB + \\beta_6 x_1LevelC \\] The mean function for cases where \\(x_2=A\\) sets the indicators for levels B and C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=A) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (0) = \\beta_0 + \\beta_1 x_1 + \\beta_5 x_1(0) + \\beta_6 x_1(0) = \\beta_0 + \\beta_1 x_1 \\] The mean function for cases where \\(x_2=B\\) sets the indicator for level B equalt to 1 and the indicator for level C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=B) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (1) + \\beta_4 (0) + \\beta_5 x_1(1) + \\beta_6 x_1(0) = (\\beta_0+ \\beta_3) + (\\beta_1+\\beta_5) x_1 \\] The mean function for cases where \\(x_2=C\\) sets the indicator for level C equalt to 1 and the indicator for level B equal to 0: \\[ \\mu(Y \\mid x_1, x_2=C) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (1) + \\beta_5 x_1(0) + \\beta_6 x_1(1)= (\\beta_0+ \\beta_4) + (\\beta_1+\\beta_6) x_1 \\] Interpretation of indicator effects: \\(\\beta_3\\) is the difference between \\(\\mu(Y \\mid x_1=0, x_2=B)\\) and \\(\\mu(Y \\mid x_1=0, x_2=A)\\) at the intercept where \\(x_1=0\\). \\(\\beta_4\\) is the difference between \\(\\mu(Y \\mid x_1=0, x_2=C)\\) and \\(\\mu(Y \\mid x_1=0, x_2=A)\\) at the intercept where \\(x_1=0\\). \\(\\beta_1\\) is the effect of \\(x_1\\) when \\(x_2 = A\\) (level A) \\(\\beta_1+\\beta_5\\) is the effect of \\(x_1\\) when \\(x_2 = B\\) (level B) \\(\\beta_5\\) measures how much more or less the effect of \\(x_1\\) is in level B compared to level A (difference in slopes of the two lines) \\(\\beta_1+\\beta_6\\) is the effect of \\(x_1\\) when \\(x_2 = C\\) (level C) \\(\\beta_6\\) measures how much more or less the effect of \\(x_1\\) is in level C compared to level A (difference in slopes of the two lines) 3.4.2.1 Example: Sleep This data sleep contains information on the sleep and physical characteristics of 62 species of mammals. Our goal is to see how the total amount of sleep (hours) that an animal gets in 24 hours is related to its body weight (kg) and the amount of danger (low danger to high danger) to which it is exposed. &gt; sleep &lt;- read.csv(&quot;http://math.carleton.edu/kstclair/data/sleep.csv&quot;) &gt; sleep %&gt;% select(TS, BodyWt, D) %&gt;% summary() ## TS BodyWt D ## Min. : 2.60 Min. : 0.005 Min. :1.000 ## 1st Qu.: 8.05 1st Qu.: 0.600 1st Qu.:1.000 ## Median :10.45 Median : 3.342 Median :2.000 ## Mean :10.53 Mean : 198.790 Mean :2.613 ## 3rd Qu.:13.20 3rd Qu.: 48.202 3rd Qu.:4.000 ## Max. :19.90 Max. :6654.000 Max. :5.000 ## NA&#39;s :4 Our EDA above shows that danger levelD is numerically coded, with a “1” indicating very low danger level up to “5” indicating very high danger. If we used this version in our model that forces a linear relationship, which means a constant change in TS for each bump up in danger level, which is a strong assumption to make given the categorical nature of this variable. Plus, this constant rate of change does not look to be the case based on EDA: &gt; boxplot(TS ~ D, data=sleep) We must make a factor version of this variable to correctly use it in our model. We will use the fct_recode command from the forcats package: &gt; table(sleep$D) # EDA counts for each level ## ## 1 2 3 4 5 ## 19 14 10 10 9 &gt; library(forcats) &gt; sleep$danger &lt;- fct_recode(factor(sleep$D), + &quot;very low&quot; = &quot;1&quot;, + &quot;low&quot; = &quot;2&quot;, + &quot;moderate&quot; = &quot;3&quot;, + &quot;high&quot; = &quot;4&quot;, + &quot;very high&quot; = &quot;5&quot;) &gt; table(sleep$danger) # check that we recoded correctly ## ## very low low moderate high very high ## 19 14 10 10 9 &gt; levels(sleep$danger) # very low is the first=baseline level ## [1] &quot;very low&quot; &quot;low&quot; &quot;moderate&quot; &quot;high&quot; &quot;very high&quot; Always do an EDA check (here, checking counts) when manipulating or recoding a variable. Our factor variable danger’s first level is very low which will be the baseline level for any model fit in R. When we add lm lines to a scatterplot with color coded points, then ggplot will fit separate lines for each color. Here we plot TS (total sleep) against the logged version of BodyWt since we don’t have a linear relationship on the untransformed scale. &gt; library(ggplot2) &gt; ggplot(sleep, aes(x=BodyWt, y=TS, color=danger)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + scale_x_log10() + + labs(title=&quot;daily sleep vs. body weight by danger level&quot;) This plot suggests that there may be an interaction between danger and BodyWt since the lm lines have some variation in slope across danger levels. The model we will fit has ten parameters ($_0-_9) due to the four indicator variables that are also interacted with body weight: \\[\\begin{split} \\mu(TS \\mid body, danger) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 low + \\beta_3 moderate + \\beta_4 high + \\\\ &amp; \\beta_5 veryhigh + \\beta_6 \\log(body)low + \\beta_7 \\log(body)moderate + \\\\ &amp; \\beta_8 \\log(body)high + \\beta_9 \\log(body)veryhigh \\end{split} \\] &gt; sleep.lm &lt;- lm(TS ~ log(BodyWt)*danger, data=sleep) &gt; sleep.lm ## ## Call: ## lm(formula = TS ~ log(BodyWt) * danger, data = sleep) ## ## Coefficients: ## (Intercept) log(BodyWt) ## 13.86818 -0.58104 ## dangerlow dangermoderate ## -2.35924 -3.50087 ## dangerhigh dangervery high ## -4.22738 -7.03108 ## log(BodyWt):dangerlow log(BodyWt):dangermoderate ## -0.03446 -0.34437 ## log(BodyWt):dangerhigh log(BodyWt):dangervery high ## 0.16966 -0.09638 The estimated mean function is \\[\\begin{split} \\mu(TS \\mid body, danger) &amp;= 13.868 -0.581 \\log(body)-2.359 low -3.501 moderate -4.227 high - \\\\ &amp; 7.031 veryhigh -0.034 \\log(body)low -0.344\\log(body)moderate + \\\\ &amp; 0.170 \\log(body)high -0.096 \\log(body)veryhigh \\end{split} \\] There are many parameters to interpret, so here are a few examples: \\(\\beta_1\\) is the effect of body weight for species in the “very low” danger level. To see this, write out the mean functions for the two levels. For very low level: \\[\\begin{split} \\mu(TS \\mid body, danger=verylow) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (0) + \\beta_4 (0) + \\beta_5 (0) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(0) + \\beta_8 \\log(body)(0) + \\beta_9 \\log(body)(0) \\\\ &amp; = \\beta_0 + \\beta_1 \\log(body) \\end{split}\\] \\(\\beta_9\\) tells us how the effect of body weight differs for the very low and very high danger levels. Compare the mean response for very low to the mean response for the very high level: \\[\\begin{split} \\mu(TS \\mid body, danger=veryhigh) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (0) + \\beta_4 (0) + \\beta_5 (1) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(0) + \\beta_8 \\log(body)(0) + \\beta_9 \\log(body)(1) \\\\ &amp; = (\\beta_0 + \\beta_5) + (\\beta_1 + \\beta_9) \\log(body) \\end{split}\\] In the “very low” model, \\(\\beta_1\\) measures the effect of body weight on mean sleep while in the “very high” model, \\(\\beta_1 + \\beta_9\\) measures this effect. So the parameter \\(\\beta_9\\) tells us how this effect differs for the two danger levels. The small estimated value \\(\\hat{\\beta}_9 = -0.096\\), along with the separate lines scatterplot, suggest little difference in the effect of body weight in these two danger groups. The parameter function \\(\\beta_5 + \\beta_9 \\log(body)\\) tells us the difference in total sleep between the very high and very low danger levels at a given value of body weight. Recall that in this interaction model, the effect of danger should depend on the value of body weight. For example, for animals that are 1kg in weight, the parameter \\(\\beta_5 + \\beta_9\\log(1) = \\beta_5\\) measures the difference in mean total sleep for the very high and very low levels. The estimated parameter value of \\(\\hat{\\beta}_5 = -7.031\\) and the large difference shown in the scatterplot suggest that for 1 kg animals, that the average total sleep for animals with very high danger is about 7 hours less than for animals in very low danger. To determine if the effect of body weight on mean total sleep differs for “high” and “moderate” levels of danger, we would test the hypotheses \\[ H_0: \\beta_8 = \\beta_7 \\ \\ \\ \\ vs. \\ \\ \\ \\ H_A: \\beta_8 \\neq \\beta_7 \\] Again, write down the mean equations for these two levels. For “high” all dummy variables equal 0 expect high equals 1: \\[\\begin{split} \\mu(TS \\mid body, danger=high) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (0) + \\beta_4 (1) + \\beta_5 (0) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(0) + \\beta_8 \\log(body)(1) + \\beta_9 \\log(body)(0) \\\\ &amp; = (\\beta_0 + \\beta_4) + (\\beta_1 + \\beta_8) \\log(body) \\end{split} \\] for “moderate” all dummy variables equal 0 expect moderate equals 1. \\[\\begin{split} \\mu(TS \\mid body, danger=moderate) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (1) + \\beta_4 (0) + \\beta_5 (0) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(1) + \\beta_8 \\log(body)(0) + \\beta_9 \\log(body)(0) \\\\ &amp; = (\\beta_0 + \\beta_3) + (\\beta_1 + \\beta_7) \\log(body) \\end{split}\\] In the “high” model, \\(\\beta_1 + \\beta_8\\) measures the effect of body weight on mean sleep while in the “moderate” model, \\(\\beta_1 + \\beta_7\\) measures this effect. Our null hypothesis would state that these two effects are equal: \\[ H_0: \\beta_1 + \\beta_8 = \\beta_1 + \\beta_7 \\] which simplifies to the hypothesis given above. 3.5 Inference for MLR Inference methods for individual \\(\\beta\\)-parameters, the mean function \\(\\mu\\) and predictions are all very similar to the inference methods outlined for SLR models in Sections 2.5 and 2.6. The biggest difference is that we now use a t-distribution with degrees of freedom equal to \\(\\pmb{n-(p+1)}\\) where \\(p+1\\) is equal to the number of \\(\\beta\\)’s in the mean function. To review: Confidence Intervals for \\(\\pmb{\\beta}_i\\): A \\(C\\)% confidence for \\(\\beta_i\\) has the form: \\[ \\hat{\\beta}_i \\pm t^* SE(\\hat{\\beta}_i) \\] where \\(t^*\\) is the \\((100-C)/2\\) percentile from the t-distribution with \\(df=n-(p+1)\\) degrees of freedom. Hypothesis tests We can test the hypothesis \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ vs. \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with the following t-test statistic: \\[ t =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] The t-distribution with \\(n-(p+1)\\) degrees of freedom is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. Interpretation of results: Recall the general interpretation for the planar model: \\(\\beta_i\\) is the effect of a one unit increase in \\(x_i\\), holding all other predictors constant. Testing \\(\\pmb{\\beta}_i =0\\) is the same as asking: Is the observed effect of \\(x_i\\) on \\(\\mu\\) statistically significant after accounting for all other terms in the model? For this reason, an individual t-test is only good for determing if \\(x_i\\) is needed in a model that contains all other terms. If you want to test the significance of multiple predictors at once, you need to conduct an F-test using ANOVA (Section 3.6. Inference for \\(\\pmb{\\mu(Y \\mid X_0)}\\) and \\(\\pmb{pred(Y \\mid X_0)}\\): Confidence and prediction intervals calculations are very similar to SLR in Section 3.5.1, except for the change in the degrees of freedom and a slightly more complex form for the SE of the estimated mean response \\(SE(\\hat{\\mu}(Y \\mid X_0))\\) which now involves more than one predictor. 3.5.1 Inference for a linear combination of \\(\\beta\\)’s One new inference method which is sometimes needed in MLR models, is to make inferences about a linear combination of \\(\\beta_i\\)’s. A linear combination is of the form: \\[ \\gamma = c_i \\beta_i + c_j \\beta_j \\] where \\(c_i\\) and \\(c_j\\) are known numbers. Some examples of a linear combination include: Suppose we fit the quadratic model in Section 3.2.1.2 and want to estimate the change in the mean response for a change in \\(x_1\\) from 9 to 10. This change is measured by the parameter \\[ \\gamma = \\beta_1 + \\beta_2(2\\times 9+1) = \\beta_1 + 19\\beta_2 \\] so \\(c_1 = 1\\) and \\(c_2 = 19\\). Suppose we fit the interaction model in Section 3.2.1.3 and want to estimate the change in the mean response for a 1 unit increase in \\(x_1\\), holding \\(x_2\\) constant at the value of -5. This change is measured by the parameter \\[ \\gamma = \\beta_1 + \\beta_3 (-5) = \\beta_1 -5 \\beta_3 \\] so \\(c_1 = 1\\) and \\(c_3 = -5\\). Any inference for this new \\(\\gamma\\) parameter will rely on an estimate, \\(\\hat{\\gamma}\\) and standard error \\(SE(\\hat{\\gamma})\\). Estimation: Just plug in our \\(\\beta\\) estimates! \\[ \\hat{\\gamma} = c_i \\hat{\\beta}_i + c_j \\hat{\\beta}_j \\] SE: This is more complex of a calculation because the estimates \\(\\hat{\\beta}_i\\) and \\(\\hat{\\beta}_i\\) are correlated. We can see this for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) in the SLR simulation in Section 2.4.4. We use probabilities rules for the linear combination of two correlated random variables to derive our SE as \\[ SE(\\hat{\\gamma}) = \\sqrt{c^2_i Var(\\hat{\\beta}_i) + c^2_j Var(\\hat{\\beta}_j) + 2c_i c_j Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) } \\] The variances (\\(Var\\)) values are the squared SE’s for each estimate, e.g. \\(Var(\\hat{\\beta}_i) = SE(\\hat{\\beta}_i)^2\\). The covariance (\\(Cov\\)) value measures how the two estiamtes co-vary together over many, many samples from the populations (just like SE tells us how each estimate varies by itself). Positive values of covariance means the two estimates are positively correlated, negative means negatively correlated. The magnitude of the covariance is a function of each estimate’s SE. In R, we will get these variance and covariance values from a MLR model’s estimated covariance matrix, which for a MLR planar model with two predictors will look like a 3x3 matrix with variance values on the diagonal and covariance values on the off-diagonals: \\[ \\begin{pmatrix} Var(\\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_2) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) &amp; Var(\\hat{\\beta}_1)&amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_2) \\\\ Cov(\\hat{\\beta}_2, \\hat{\\beta}_0)&amp; Cov(\\hat{\\beta}_2, \\hat{\\beta}_1)&amp; Var(\\hat{\\beta}_2) \\end{pmatrix} \\] Note that covariance is symmetric so that \\(Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = Cov(\\hat{\\beta}_j, \\hat{\\beta}_i)\\). Inference for \\(\\gamma\\) then looks like: Confidence interval for \\(\\pmb{\\gamma}\\): \\(\\hat{\\gamma} \\pm t^*_{df} SE(\\hat{\\gamma})\\) Hypothesis test for \\(\\pmb{H_0: \\gamma = \\gamma^*}\\): uses the test statistic \\[ t = \\dfrac{\\hat{\\gamma} - \\gamma^*}{SE(\\hat{\\gamma})} \\] and a p-value from a t-distribution with model degrees of freedom. 3.5.2 Example: Agstrat Let’s continue with inference for the parallel lines model for modeling the number of farms in 1992 as a function of region and number of farms in 1987 (Section 3.4.1.1). The mean model form is \\[ \\begin{split} \\mu(\\sqrt{farms92} \\mid \\sqrt{farms92}, region) &amp; = \\beta_0 + \\beta_1 \\sqrt{farms92} + \\beta_2(regionNE) + \\\\ &amp; \\beta_3(regionS) + \\beta_4(regionW) \\end{split} \\] &gt; agstrat&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/agstrat.csv&quot;) &gt; farms.lm &lt;- lm(sqrt(farms92) ~ sqrt(farms87) + region, data=agstrat) &gt; kable(tidy(farms.lm, conf.int=TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) -0.937 0.191 -4.905 0.000 -1.313 -0.561 sqrt(farms87) 0.980 0.006 162.869 0.000 0.968 0.992 regionNE -0.083 0.219 -0.377 0.706 -0.514 0.349 regionS 0.721 0.123 5.884 0.000 0.480 0.963 regionW 1.010 0.170 5.933 0.000 0.675 1.345 Compare S to NC: \\(\\hat{\\beta}_3 = 0.721\\) tells us that the mean square root farms92 is estimated to be 0.721 units higher in the S compared to the NC region, holding farms87 constant. The t-test results in the summary output test \\(H_0: \\beta_3 = 0\\), which has a t-test statistic of \\(t = 0.721/0.123 = 5.884\\) which gives a p-value that is less than 0.001. Interpretation: Holding all other model terms constant (including other indicators for W and NE), there is a statistically significant difference between the mean number of farms in 1992 between South and North Central regions for any given value of farms in 1987. I am 95% confident that the average square root of farms in 1992 is anywhere from 0.48 to 0.963 units higher in the south than in the north central regions, holding the number of farms in 1987 fixed. Compare NE to NC: \\(\\hat{\\beta}_2 = -0.083\\) tells us that the mean square root farms92 is estimated to be 0.083 units lower in the NE compared to the NC region, holding farms87 constant. The t-test results in the summary output test \\(H_0: \\beta_2 = 0\\), which has a t-test statistic of \\(t = -0.377\\) which gives a p-value that is 0.706. Interpretation: Holding all other model terms constant (including other indicators for S and W), there is no statistically significant difference between the mean number of farms in 1992 between Northeast and North Central regions for any given value of farms in 1987. Compare S to W: In the parallel line model, we would like to know if there is a difference in the mean number of farms in 1992 in the S and W regions, after controlling for the number of farms in 1987. The question can be answered by comparing \\[ \\mu(farms92 \\mid farms87, region=S) - \\mu(farms92 \\mid farms87, region=W) = \\beta_3 - \\beta_4 \\] The hypotheses are then \\[ H_0: \\beta_3 - \\beta_4 = 0 \\ \\ vs. \\ \\ H_A: \\beta_3 - \\beta_4 \\neq 0 \\] Note that this is not the same as asking if each parameter equals 0. We just want to know if they are the same value, so we cannot use a t-test given in the summary output. This is a question about a linear combination of parameters: \\[ \\gamma = \\beta_3 - \\beta_4 \\] where \\(c_3 = 1\\) and \\(c_4 = -1\\). Our estimated parameter difference is \\[ \\hat{\\gamma} = \\hat{\\beta}_3 - \\hat{\\beta}_4 = 0.7214 - 1.0100 = -0.2886 \\] &gt; farms.lm ## ## Call: ## lm(formula = sqrt(farms92) ~ sqrt(farms87) + region, data = agstrat) ## ## Coefficients: ## (Intercept) sqrt(farms87) regionNE regionS regionW ## -0.9369 0.9798 -0.0826 0.7214 1.0100 &gt; # estimate of beta3 - beta4 (do S and W lines have same intercepts?) &gt; est &lt;- 0.7214 - 1.0100 &gt; est ## [1] -0.2886 We use the vcov(my.lm) command to get the estimated model’s covariance matrix: &gt; vcov(farms.lm) ## (Intercept) sqrt(farms87) regionNE regionS ## (Intercept) 0.036481631 -1.015899e-03 -0.0132540419 -0.013273838 ## sqrt(farms87) -0.001015899 3.619176e-05 0.0001884077 0.000189113 ## regionNE -0.013254042 1.884077e-04 0.0480149402 0.008949944 ## regionS -0.013273838 1.891130e-04 0.0089499443 0.015030978 ## regionW -0.013332531 1.912040e-04 0.0089608296 0.008964555 ## regionW ## (Intercept) -0.013332531 ## sqrt(farms87) 0.000191204 ## regionNE 0.008960830 ## regionS 0.008964555 ## regionW 0.028986383 The parts we need are \\(Var(\\hat{\\beta}_3) = 0.015030978\\), \\(Var(\\hat{\\beta}_4) = 0.028986383\\), and \\(Cov(\\hat{\\beta}_3, \\hat{\\beta}_4) = 0.008964555\\). The SE of our estimated difference is then \\[ SE(\\hat{\\gamma}) = \\sqrt{(1)^20.015030978 + (-1)^20.028986383 + 2(1)(-1)0.008964555} = 0.1615186 \\] &gt; # SE of this estimate &gt; se.est &lt;- sqrt(0.015030978 + (-1)^2*0.028986383 + 2*(-1)*0.008964555) &gt; se.est ## [1] 0.1615186 The test statistic is then \\(t = (-0.2886 - 0)/0.1615186 = -1.786791\\) which yields a 2-sided p-value of 0.075. This suggests that there is moderately weak evidence that there is a difference in means between the south and west, holding farms in 1987 fixed, but it is not statistically significant at the 5% level. &gt; # test stat &gt; t &lt;- (est - 0)/se.est &gt; t ## [1] -1.786791 &gt; 2*pt(t,df=295) ## [1] 0.07499793 3.5.3 Example: Sleep Let’s continue with inference for the separate lines model for modeling the amount of daily sleep for variety of animals as a function of their body size and danger level (Section 3.4.2.1). The interaction model looks liked \\[\\begin{split} \\mu(TS \\mid body, danger) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 low + \\beta_3 moderate + \\beta_4 high + \\\\ &amp; \\beta_5 veryhigh + \\beta_6 \\log(body)low + \\beta_7 \\log(body)moderate + \\\\ &amp; \\beta_8 \\log(body)high + \\beta_9 \\log(body)veryhigh \\end{split} \\] The estimated model parameters from the model fit in Section 3.4.2.1 is &gt; summary(sleep.lm) ## ## Call: ## lm(formula = TS ~ log(BodyWt) * danger, data = sleep) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.5685 -2.0181 -0.2475 1.7083 6.6108 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.86818 0.89109 15.563 &lt;2e-16 *** ## log(BodyWt) -0.58104 0.27659 -2.101 0.0409 * ## dangerlow -2.35924 1.30538 -1.807 0.0770 . ## dangermoderate -3.50087 1.40445 -2.493 0.0162 * ## dangerhigh -4.22738 1.58186 -2.672 0.0103 * ## dangervery high -7.03108 3.32910 -2.112 0.0399 * ## log(BodyWt):dangerlow -0.03446 0.72333 -0.048 0.9622 ## log(BodyWt):dangermoderate -0.34437 0.42848 -0.804 0.4255 ## log(BodyWt):dangerhigh 0.16966 0.41793 0.406 0.6866 ## log(BodyWt):dangervery high -0.09638 0.76994 -0.125 0.9009 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.432 on 48 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.5326, Adjusted R-squared: 0.4449 ## F-statistic: 6.076 on 9 and 48 DF, p-value: 1.177e-05 &gt; kable(tidy(sleep.lm, conf.int = TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 13.868 0.891 15.563 0.000 12.077 15.660 log(BodyWt) -0.581 0.277 -2.101 0.041 -1.137 -0.025 dangerlow -2.359 1.305 -1.807 0.077 -4.984 0.265 dangermoderate -3.501 1.404 -2.493 0.016 -6.325 -0.677 dangerhigh -4.227 1.582 -2.672 0.010 -7.408 -1.047 dangervery high -7.031 3.329 -2.112 0.040 -13.725 -0.337 log(BodyWt):dangerlow -0.034 0.723 -0.048 0.962 -1.489 1.420 log(BodyWt):dangermoderate -0.344 0.428 -0.804 0.426 -1.206 0.517 log(BodyWt):dangerhigh 0.170 0.418 0.406 0.687 -0.671 1.010 log(BodyWt):dangervery high -0.096 0.770 -0.125 0.901 -1.644 1.452 Here are some inferences we can make: Body weight effect for very low danger: Recall from above that \\(\\beta_1\\) is the effect of body weight for species in the “very low” danger level. The coefficients table above provide results for testing \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\neq 0\\). The results of this test show that the effect of body weight is moderately significant for the very low danger subpopulation (t=-2.101, df=48, p=0.0409). A doubling of body weight for the low danger population is associated with a decrease in mean total sleep of anywhere from 0.02 to 0.80 hours. &gt; # logarithmic model : additive change in mean(y) = beta*log(m) &gt; log(2)*-0.581 ## [1] -0.4027185 &gt; log(2)*-1.137 ## [1] -0.7881083 &gt; log(2)*-0.025 ## [1] -0.01732868 Difference between body weight effect in very low and very high danger levels: \\(\\beta_9\\) tells us how the effect of body weight differs for the very low and very high danger levels. The small estimated value \\(\\hat{\\beta}_9 = -0.096\\), along with the separate lines scatterplot, suggest little difference in the effect of body weight in these two danger groups. The summary output gives test results for \\(H_0: \\beta_9 = 0\\) vs. \\(H_A: \\beta_9 \\neq 0\\). The test results suggest that the difference effects of body weight on mean total sleep in the two groups is not statistically significant (t=-0.125, df=48, p=0.9). Difference between body weight effect in high and moderate danger levels: As explained in Section 3.4.2.1, to determine if the effect of body weight on mean total sleep differs for “high” and “moderate” levels of danger, we would test the hypotheses \\[ H_0: \\beta_8 = \\beta_7 \\ \\ \\ \\ vs. \\ \\ \\ \\ H_A: \\beta_8 \\neq \\beta_7 \\] We can rearrange to look at the difference: \\[ H_0: \\beta_8 - \\beta_7 = 0 \\ \\ vs. \\ \\ H_A: \\beta_8 - \\beta_7 \\neq 0 \\] This difference is a linear combination of parameters: \\[ \\gamma = \\beta_8 - \\beta_7 \\] where we know constants \\(c_8 = 1\\) and \\(c_7 = -1\\). The estimated mean difference is \\[ \\hat{\\gamma} = \\hat{\\beta}_8 - \\hat{\\beta}_7 = 0.16966 - (-0.34437) \\approx 0.51403 \\] The SE of this estimate uses the SE’s of \\(Var(\\hat{\\beta}_8) = 0.17466548\\) and $Var(_7) = 0.18359650 $, along with their covariance term \\(Cov(\\hat{\\beta}_8,\\hat{\\beta}_7) = 0.07650199\\). \\[ SE(\\hat{\\gamma}) = \\sqrt{ 0.17466548 + 0.18359650(-1)^2 + 2(1)(-1)(0.07650199)} \\approx 0.45305 \\] The test stat is \\[ t = \\dfrac{0.51403 - 0}{0.45305} = 1.1346 \\] The p-value is the area above 1.1346 and below -1.1346 (or double the lower tail area). The p-value is about 26%, which means we cannot conclude that the effect of body weight on mean total sleep differs between animals in moderate danger and high danger. &gt; vcov(sleep.lm) # really big! ## (Intercept) log(BodyWt) dangerlow dangermoderate ## (Intercept) 0.7940483 -0.10333697 -0.79404832 -0.7940483 ## log(BodyWt) -0.1033370 0.07650199 0.10333697 0.1033370 ## dangerlow -0.7940483 0.10333697 1.70402151 0.7940483 ## dangermoderate -0.7940483 0.10333697 0.79404832 1.9724931 ## dangerhigh -0.7940483 0.10333697 0.79404832 0.7940483 ## dangervery high -0.7940483 0.10333697 0.79404832 0.7940483 ## log(BodyWt):dangerlow 0.1033370 -0.07650199 0.07161464 -0.1033370 ## log(BodyWt):dangermoderate 0.1033370 -0.07650199 -0.10333697 -0.1099700 ## log(BodyWt):dangerhigh 0.1033370 -0.07650199 -0.10333697 -0.1033370 ## log(BodyWt):dangervery high 0.1033370 -0.07650199 -0.10333697 -0.1033370 ## dangerhigh dangervery high log(BodyWt):dangerlow ## (Intercept) -0.7940483 -0.7940483 0.10333697 ## log(BodyWt) 0.1033370 0.1033370 -0.07650199 ## dangerlow 0.7940483 0.7940483 0.07161464 ## dangermoderate 0.7940483 0.7940483 -0.10333697 ## dangerhigh 2.5022880 0.7940483 -0.10333697 ## dangervery high 0.7940483 11.0829102 -0.10333697 ## log(BodyWt):dangerlow -0.1033370 -0.1033370 0.52320246 ## log(BodyWt):dangermoderate -0.1033370 -0.1033370 0.07650199 ## log(BodyWt):dangerhigh -0.3013217 -0.1033370 0.07650199 ## log(BodyWt):dangervery high -0.1033370 -2.2112565 0.07650199 ## log(BodyWt):dangermoderate log(BodyWt):dangerhigh ## (Intercept) 0.10333697 0.10333697 ## log(BodyWt) -0.07650199 -0.07650199 ## dangerlow -0.10333697 -0.10333697 ## dangermoderate -0.10997000 -0.10333697 ## dangerhigh -0.10333697 -0.30132171 ## dangervery high -0.10333697 -0.10333697 ## log(BodyWt):dangerlow 0.07650199 0.07650199 ## log(BodyWt):dangermoderate 0.18359650 0.07650199 ## log(BodyWt):dangerhigh 0.07650199 0.17466548 ## log(BodyWt):dangervery high 0.07650199 0.07650199 ## log(BodyWt):dangervery high ## (Intercept) 0.10333697 ## log(BodyWt) -0.07650199 ## dangerlow -0.10333697 ## dangermoderate -0.10333697 ## dangerhigh -0.10333697 ## dangervery high -2.21125653 ## log(BodyWt):dangerlow 0.07650199 ## log(BodyWt):dangermoderate 0.07650199 ## log(BodyWt):dangerhigh 0.07650199 ## log(BodyWt):dangervery high 0.59281009 &gt; vcov(sleep.lm)[c(7,8)+1, c(7,8)+1] # get the 8/9th rows/columns for beta7,beta8 ## log(BodyWt):dangermoderate log(BodyWt):dangerhigh ## log(BodyWt):dangermoderate 0.18359650 0.07650199 ## log(BodyWt):dangerhigh 0.07650199 0.17466548 &gt; (est &lt;- 0.16966 - (-0.34437)) ## [1] 0.51403 &gt; (se.est &lt;- sqrt(0.17466548 + 0.18359650*(-1)^2 + 2*(-1)*0.07650199)) ## [1] 0.4530541 &gt; (tstat &lt;- est/se.est ) ## [1] 1.134589 &gt; 2*pt(-tstat, df=48) ## [1] 0.2621821 3.6 ANOVA for MLR Recall the discussion of Analysis of Variance (ANOVA) for SLR models in Section 2.12.2. The same idea holds in MLR models, we decompose the total squared variation in a response (\\(SST\\)) into parts explained by a certain model (\\(SSreg\\)) and unexplained by the model (\\(SSR\\)): \\[ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 \\] The three components are called: total variation: \\(SST = \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1)s^2_y\\) regression (explained) variation: \\(SSreg = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\) residual (unexplained) variation: \\(SSR = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = (n-(p+1))\\hat{\\sigma}^2\\) where \\(n-(p+1)\\) is the model degrees of freedom (sample size minus number of \\(\\beta\\)’s) Both the regression and residual SS depends on which MLR model you fit to make predictions \\(\\hat{y}_i\\), with different models (for the same response) producing different \\(SSR\\) and \\(SSref\\) values. The total variation in the response, \\(SST\\), always remains the same, regardless of model fit. We often compare regression and residual SS for different models. When doing this we make reference to the model terms. For example, \\(SSreg(x_1, x_2, x_2^2)\\) would be the regression SS for fitting the regression of \\(y\\) on \\(x_1\\), \\(x_2\\) and \\(x_2^2\\). Similar notation is used for \\(SSR\\). We often are interested in comparing a large (“full”) model to a smaller nested (“reduced”) model. A nested model contains a smaller subset of terms compared to a larger model. We can form a nested model by setting some \\(\\beta\\)’s in a larger model equal to zero. For example, if our larger model is \\[ \\mu(y \\mid X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_3^2 \\] then the following is a nested model found by setting \\(\\beta_1 = \\beta_2 = 0\\): \\[ \\mu(y \\mid X) = \\beta_0 + \\beta_3 x_3 + \\beta_4 x_3^2 \\] We often are interested in comparing models, is a full model “better” than a smaller, nested model? Or can the smaller model “fit” the data as well as a larger model? We can assess the significance of terms in a model on a one-by-one basis using t-test (i.e. is \\(x_i\\) needed assuming all other terms are in the model). To test more than one term at a time, we look at the extra sum of squares that the terms constribute to the regression SS. Extra sum of squares: the amount by which \\(SSreg\\) is increased by adding one or more terms to a model, or, equivalently, the amount by which \\(SSR\\) is decreased by adding these terms. For the models above, the extra SS for adding \\(x_1\\) and \\(x_2\\) to the reduce (nested) model would be equal to \\[ extraSS = SSreg(x_1,x_2,x_3,x_3^2) - SSreg(x_3,x_3^2) = SSR(x_3,x_3^2) - SSR(x_1,x_2,x_3,x_3^2) \\] Extra sum of squares is always a positive value, due to the fact that you never “lose” regression SS by adding terms. So \\(SSreg\\) increases as you add model terms while \\(SSR\\) will decrease. 3.6.1 Mean Squares Mean square measures of variation are equal to a SS measure divided by the degrees of freedom associated with that measure. The degrees of freedom capture, roughly, now many terms are being added up in the SS measure. Here are the mean square values for a MLR model: Mean square of total: \\(MST = \\dfrac{SST}{n-1} = s^2_y\\) which equals the sample variance of the response Mean square for regression: \\(MSreg = \\dfrac{SSreg}{p}\\) where \\(p\\) is the number of terms in the model Mean square for residuals (error): \\(MSR = \\dfrac{SSR}{n-(p+1)} = \\hat{\\sigma}^2\\), which is the estimated model SD 3.6.2 \\(R^2\\) and adjusted \\(R^2\\) \\(R^2\\) has the same interpretation and calculation as it did in SLR, it measures the proportion of variation in the response that is explained by a MLR model: \\[ R^2 = 1- \\dfrac{SSR}{SST} = \\dfrac{SSreg}{SST} \\] As we add terms to a model, \\(R^2\\) always increases, even if the terms don’t add much information about the response. Adjusted \\(R^2\\) is similar to \\(R^2\\), but it uses mean SS which can help us assess whether the increased gain in \\(SSreg\\) is worth the addition (or complexity) of another term. \\[ R^2_{adjust} = 1- \\dfrac{MSR}{MST} \\] Unlike \\(R^2\\), it is possible for \\(R^2_a\\) to decrease when adding a new term if it adds little extra SS to the model. 3.6.3 ANOVA F-tests We can use ANOVA to compare nested models. The hypotheses compare a reduced (nested) to full (larger) model: \\[ H_0: \\textrm{nested model} \\ \\ \\ \\ \\ H_A: \\textrm{full model} \\] Consider the nested model example above, the nested model is the null claim \\[ H_0: \\mu(y \\mid X) = \\beta_0 + \\beta_3 x_3 + \\beta_4 x_3^2 \\ \\ \\ \\ \\ \\ \\ (\\beta_1 = \\beta_2 = 0) \\] and the larger model is the alternative claim: \\[ H_A: \\mu(y \\mid X) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\beta_4 x_3^2 \\] Notice that the null model is equivalent to saying that neither \\(x_1\\) nor \\(x_2\\) have an effect on the mean response after accounting for \\(x_3\\) and its quadratic term. The F-test statistic compares the average extra SS add by the terms being tested to the best estimate of model SD (given by the full model): \\[ F = \\dfrac{extraSS/(\\textrm{# terms tested})}{MSR_{full}} \\] For the example, the F test stat would look like \\[ F = \\dfrac{(SSR(x_3,x_3^2)-SSR(x_1,x_2,x_3,x_3^2))/2}{MSR_{full}} \\] The p-value is found in the right-tail of the \\(F-\\)distribution with degrees of freedom equal to the # of terms tested (numerator) and \\(n-(p+1)\\) from the full model (denominator). For the example, the first df is 2 (two terms being tested) and the second would be \\(n-(4+1)\\) (4+1=5 \\(\\beta\\)’s in the full model). If you have a small p-value, this gives evidence that at least one of the tested terms is useful in the model (e.g. reject the smaller model). If you have a larger p-value, then we don’t have much evidence that the terms being tested are useful in the model (e.g. do not reject the smaller model). Special cases of ANOVA F-tests: Testing one term: If our reduced and full models only differ by one term, then the F-test will be exactly the same as the t-test for that term from the full model. Testing all terms: This is the “overall F-test” that is part of the summary output for most regression software. It tests a full model against a “null” model of no predictors: \\(H_0: \\mu(y \\mid x) = \\beta_0\\). 3.6.4 Example: Sleep We will revisit the animal sleep data from Sections 3.4.2.1 and 3.5.3. &gt; summary(sleep.lm) ## ## Call: ## lm(formula = TS ~ log(BodyWt) * danger, data = sleep) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.5685 -2.0181 -0.2475 1.7083 6.6108 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.86818 0.89109 15.563 &lt;2e-16 *** ## log(BodyWt) -0.58104 0.27659 -2.101 0.0409 * ## dangerlow -2.35924 1.30538 -1.807 0.0770 . ## dangermoderate -3.50087 1.40445 -2.493 0.0162 * ## dangerhigh -4.22738 1.58186 -2.672 0.0103 * ## dangervery high -7.03108 3.32910 -2.112 0.0399 * ## log(BodyWt):dangerlow -0.03446 0.72333 -0.048 0.9622 ## log(BodyWt):dangermoderate -0.34437 0.42848 -0.804 0.4255 ## log(BodyWt):dangerhigh 0.16966 0.41793 0.406 0.6866 ## log(BodyWt):dangervery high -0.09638 0.76994 -0.125 0.9009 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.432 on 48 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.5326, Adjusted R-squared: 0.4449 ## F-statistic: 6.076 on 9 and 48 DF, p-value: 1.177e-05 &gt; kable(tidy(sleep.lm, conf.int = TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 13.868 0.891 15.563 0.000 12.077 15.660 log(BodyWt) -0.581 0.277 -2.101 0.041 -1.137 -0.025 dangerlow -2.359 1.305 -1.807 0.077 -4.984 0.265 dangermoderate -3.501 1.404 -2.493 0.016 -6.325 -0.677 dangerhigh -4.227 1.582 -2.672 0.010 -7.408 -1.047 dangervery high -7.031 3.329 -2.112 0.040 -13.725 -0.337 log(BodyWt):dangerlow -0.034 0.723 -0.048 0.962 -1.489 1.420 log(BodyWt):dangermoderate -0.344 0.428 -0.804 0.426 -1.206 0.517 log(BodyWt):dangerhigh 0.170 0.418 0.406 0.687 -0.671 1.010 log(BodyWt):dangervery high -0.096 0.770 -0.125 0.901 -1.644 1.452 The anova command for this model gives the \\(SSR\\) and \\(MSR\\) for a model, along with the extra SS for adding a term to the model above it in the table: &gt; anova(sleep.lm) ## Analysis of Variance Table ## ## Response: TS ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## log(BodyWt) 1 343.44 343.44 29.1537 2.043e-06 *** ## danger 4 285.01 71.25 6.0485 0.0005036 *** ## log(BodyWt):danger 4 15.76 3.94 0.3345 0.8534077 ## Residuals 48 565.46 11.78 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we have: \\(SSreg(log(BodyWt)) = 343.44\\) gives the regression SS for the regression of total sleep on log-body weight. \\(SSreg(log(BodyWt), danger) - SSreg(log(BodyWt)) = 285.01\\) gives the extra SS for adding the factor variable danger (with 5 levels) to the model that already includes log-body weight. \\(SSreg(log(BodyWt), danger, log(BodyWt):danger) - SSreg(log(BodyWt), danger) = 15.76\\) gives the extra SS for adding the interaction term to the model that already includes danger level and log-body weight. \\(SSR(log(BodyWt), danger, log(BodyWt):danger) = 565.46\\) is the residual SS for the interaction model and \\(MSR=11.78 = \\hat{\\sigma}\\) is the estimated model SD for this model. \\(SSreg(log(BodyWt), danger, log(BodyWt):danger) = 343.44 + 285.01 + 15.76 = 644.21\\) is the regression SS for the interaction model. 3.6.4.1 Are any terms significant? The summary output above shows results for the overall \\(F-\\)test: \\[\\begin{split} H_0: \\mu(TS \\mid body, danger) &amp;= \\beta_0 \\end{split} \\] vs. \\[\\begin{split} H_A: \\mu(TS \\mid body, danger) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 low + \\beta_3 moderate + \\beta_4 high + \\\\ &amp; \\beta_5 veryhigh + \\beta_6 \\log(body)low + \\beta_7 \\log(body)moderate + \\\\ &amp; \\beta_8 \\log(body)high + \\beta_9 \\log(body)veryhigh \\end{split} \\] The F-test statistic that is given has a value of 6.076 and the p-value has a value less than 0.0001 using an F-distribution with 9 and 48 degrees of freedom. The test stat is the ratio of regression MS to residual MS: \\[ F = \\dfrac{MSreg(log(BodyWt), danger, log(BodyWt):danger)}{MSR(log(BodyWt), danger, log(BodyWt):danger)} = \\dfrac{644.21/9}{3.432^2} = 6.077 \\] &gt; 1- pf(6.077, 9, 48) # p-value ## [1] 1.174811e-05 3.6.4.2 Do we need the interaction of sleep and body weight in the model for total sleep? I.e. are any of the slopes for the different danger levels different? &gt; ggplot(sleep, aes(x=BodyWt, y=TS, color=danger)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + scale_x_log10() + + labs(title=&quot;daily sleep vs. body weight by danger level&quot;) We can’t answer this with individual t-tests which only compare the slopes (effects of body weight) of danger levels two at a time. An F-test can address this question. We will test a model with no interaction to the full model with an interaction: \\[\\begin{split} H_0: \\mu(TS \\mid body, danger) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 low + \\beta_3 moderate + \\beta_4 high + \\beta_5 veryhigh \\end{split} \\] vs. \\[\\begin{split} H_A: \\mu(TS \\mid body, danger) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 low + \\beta_3 moderate + \\beta_4 high + \\\\ &amp; \\beta_5 veryhigh + \\beta_6 \\log(body)low + \\beta_7 \\log(body)moderate + \\\\ &amp; \\beta_8 \\log(body)high + \\beta_9 \\log(body)veryhigh \\end{split} \\] The ANOVA command above gives the results for this hypothesis test because the interaction, with 4 terms in it, was added last in the lm command. The results are in the row for the interaction term log(BodyWt):danger. It compares the extra SS for adding the interaction (which has 4 terms in it!) to the full model: \\[ F = \\dfrac{15.76/4}{11.78} = 0.3345 \\] and the p-value is \\[ P(F_{4,48} \\geq 0.3345) = 1-pf(0.3345, 4, 48) = 0.863 \\] &gt; 1-pf(.3345, 4, 48) ## [1] 0.8533796 The command anova(my.lm) will only give valid F-test results for the last term added in the lm. The other F-tests shows aren’t valid because they aren’t using the correct full model SD estimate (e.g. the denomiator is wrong). Another use of the anova command is to compare a reduced and full lm fit: anova(reduced.lm, full.lm). Here we fit the reduced model without an interaction term then compare it to the full interaction model: &gt; sleep.lm2 &lt;- lm(TS ~ log(BodyWt) + danger, data=sleep) &gt; anova(sleep.lm2, sleep.lm) ## Analysis of Variance Table ## ## Model 1: TS ~ log(BodyWt) + danger ## Model 2: TS ~ log(BodyWt) * danger ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 52 581.22 ## 2 48 565.46 4 15.76 0.3345 0.8534 Here we get the \\(SSR\\) values for both models, the extra SS for the added terms (15.76), the F-test statistic (0.3345) and the p-value (0.8534). Conclusion: Even though we see some different between the effect of body weight in the high danger level, compared to other levels, these overall differences is slopes aren’t statistically significant after accounting for danger level and body weight. 3.6.4.3 \\(R^2\\) and \\(R^2_a\\) As you can see at the top of this example section, the summary function gives both coefficient estimates and ANOVA info about the overall F-test and \\(R^2\\) values. The broom package’s glance function isolates just the model fit info like \\(R^2\\) and the overall \\(F-\\) test: &gt; library(broom) &gt; glance(sleep.lm) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.533 0.445 3.43 6.08 1.18e-5 10 -148. 319. 341. ## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; &gt; glance(sleep.lm2) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.520 0.473 3.34 11.2 2.23e-7 6 -149. 312. 327. ## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; The larger model, sleep.lm, will have the larger \\(R^2\\) value. The model with the interaction will explain about 53.3% of the variation in total sleep while the model without the interaction explains about 52% of the variation in total sleep. But are these 4 extra interaction terms worth it to gain only 1.3% more explanatory power? The adjusted \\(R^2\\) for the smaller model is 0.473 which is a larger value than adjusted \\(R^2\\) for the larger model (0.445). This, along with our F-test above, indicates that the interaction terms don’t add sufficient information about the response when the other two terms (danger level and body weight) are already included in the model. 3.6.4.4 Comparing models when values are missing The summary output let’s us know when there are cases with at least one value missing for the predictors and/or response: &gt; summary(sleep.lm2) ## ## Call: ## lm(formula = TS ~ log(BodyWt) + danger, data = sleep) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.6165 -1.8447 -0.0214 1.9043 6.7414 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.9325 0.8173 17.046 &lt; 2e-16 *** ## log(BodyWt) -0.6287 0.1606 -3.914 0.000266 *** ## dangerlow -2.4287 1.2238 -1.985 0.052479 . ## dangermoderate -3.5836 1.3347 -2.685 0.009714 ** ## dangerhigh -3.8535 1.3691 -2.815 0.006879 ** ## dangervery high -7.2945 1.5525 -4.699 1.96e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.343 on 52 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.5195, Adjusted R-squared: 0.4733 ## F-statistic: 11.25 on 5 and 52 DF, p-value: 2.23e-07 There are 4 cases that have at least one value missing for TS, danger and BodyWt. Here we select these three variables (only), then use the filter command to find the cases that are “not” (!) complete cases. The period in the complete.cases function tells R to map the data frame into the function. &gt; sleep %&gt;% + select(TS, BodyWt, danger) %&gt;% + filter(!complete.cases(.)) ## TS BodyWt danger ## 1 NA 529.00 very high ## 2 NA 35.00 high ## 3 NA 250.00 very high ## 4 NA 4.05 very low The offending variable here is TS, with missing values for all 4 cases. Suppose we wanted to include Life in the model, which measures an animal’s typical life span (in years). EDA suggests that we should log the life span variable: &gt; sleep %&gt;% + mutate_at(.vars=c(&quot;BodyWt&quot;, &quot;Life&quot;), .funs = log) %&gt;% + ggpairs( aes(color=danger), + columns=c(&quot;BodyWt&quot;,&quot;Life&quot;,&quot;TS&quot;), + lower=list(continuous = wrap(&quot;smooth&quot;, se=FALSE)), + columnLabels = c(&quot;log(BodyWt)&quot;, &quot;log(Life)&quot;, &quot;TS&quot;)) We fit the model, then use ANOVA (cause why not) instead of a t-test to compare the model with Life to the model without: &gt; sleep.lm3 &lt;- lm(TS ~ log(BodyWt)+danger + log(Life), data=sleep) &gt; anova(sleep.lm2, sleep.lm3) # reduced, full ## Error in anova.lmlist(object, ...): models were not all fitted to the same size of dataset The error occurs because there are 4 additional cases that are missing values for Life. We can see this in the summary output and in the filtering command: &gt; summary(sleep.lm3) ## ## Call: ## lm(formula = TS ~ log(BodyWt) + danger + log(Life), data = sleep) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.3841 -1.5295 0.2586 1.6906 5.2957 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.9876 1.8716 9.077 6.63e-12 *** ## log(BodyWt) -0.4509 0.2066 -2.183 0.034082 * ## dangerlow -2.8939 1.2283 -2.356 0.022700 * ## dangermoderate -5.5467 1.4679 -3.779 0.000444 *** ## dangerhigh -4.1830 1.3249 -3.157 0.002781 ** ## dangervery high -7.5354 1.4943 -5.043 7.28e-06 *** ## log(Life) -1.0948 0.6784 -1.614 0.113251 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.207 on 47 degrees of freedom ## (8 observations deleted due to missingness) ## Multiple R-squared: 0.5878, Adjusted R-squared: 0.5351 ## F-statistic: 11.17 on 6 and 47 DF, p-value: 1.033e-07 &gt; sleep %&gt;% + select(TS, BodyWt, danger, Life) %&gt;% + filter(!complete.cases(.)) ## TS BodyWt danger Life ## 1 16.5 0.920 moderate NA ## 2 10.3 0.550 low NA ## 3 NA 529.000 very high 28.0 ## 4 NA 35.000 high 16.3 ## 5 10.6 0.122 very low NA ## 6 11.2 1.350 moderate NA ## 7 NA 250.000 very high 23.6 ## 8 NA 4.050 very low 13.0 We need to make sure that any ANOVA model comparisons are done using the same cases in the data set. To do this, create a version of the data that only contains complete cases for all four variables using the tidyr function drop_na: &gt; library(tidyr) &gt; sleep.noNA &lt;- drop_na(sleep, TS, BodyWt, Life, danger) &gt; nrow(sleep) ## [1] 62 &gt; nrow(sleep.noNA) ## [1] 54 Using drop_na ensures us that only cases with NAs in the listed variables are removed from the data set. Now you can compare the two models: &gt; sleep.lm2.noNA &lt;- lm(TS ~ log(BodyWt)+danger, data=sleep.noNA) &gt; sleep.lm3.noNA &lt;- lm(TS ~ log(BodyWt)+danger + log(Life), data=sleep.noNA) &gt; anova(sleep.lm2.noNA, sleep.lm3.noNA) ## Analysis of Variance Table ## ## Model 1: TS ~ log(BodyWt) + danger ## Model 2: TS ~ log(BodyWt) + danger + log(Life) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 48 510.27 ## 2 47 483.48 1 26.793 2.6046 0.1133 More on missing data: R’s default na.action for the lm command is to omit NA cases from the data before fitting the model. Any action done on the lm, like getting residuals, returns values only for the complete cases. For example, the regression of total sleep on body weight and danger using the complete data set will give 58 residual and fitted values because there are only 58 complete cases. &gt; nrow(sleep) # 62 cases in the data ## [1] 62 &gt; length(resid(sleep.lm2)) ## [1] 58 &gt; library(broom) &gt; sleep.lm2.aug &lt;- augment(sleep.lm2) # danger + bodywt &gt; nrow(sleep.lm2.aug) # only 58 residuals and fitted values (62-4) ## [1] 58 If we change the na.action argument in the lm to exclude the NA cases during the model fit, then they won’t be use in the model estimation but they will be preserved in the model attributes like the residuals. Here those cases are given NA values in the residual vector: &gt; sleep.lm2.exclude &lt;- lm(TS ~ log(BodyWt) + danger, data=sleep, na.action=na.exclude) &gt; length(resid(sleep.lm2.exclude)) ## [1] 62 &gt; resid(sleep.lm2.exclude) ## 1 2 3 4 5 6 7 ## -1.5149212 -2.0489364 -0.6659516 6.0986456 -1.2487150 1.2021519 3.3960550 ## 8 9 10 11 12 13 14 ## -0.6885106 1.3180609 -1.7466097 1.8830495 1.1231940 -1.5796181 -0.2491280 ## 15 16 17 18 19 20 21 ## -7.1608811 -2.2131403 -0.9559651 -1.8155640 -7.6165033 6.7414197 NA ## 22 23 24 25 26 27 28 ## -0.7508961 1.5633040 1.4199273 -4.9396162 1.3260241 2.0097921 -1.8543773 ## 29 30 31 32 33 34 35 ## 0.1946797 -0.2374482 NA -4.3098363 3.0724442 -3.3379668 -4.6550207 ## 36 37 38 39 40 41 42 ## 1.0397253 0.4796206 0.5421249 5.8010790 4.2550511 NA 5.0348020 ## 43 44 45 46 47 48 49 ## 2.2684934 2.4994925 1.6261064 2.3380130 1.9114196 2.0508100 -3.2251148 ## 50 51 52 53 54 55 56 ## -0.6987071 -5.0846383 -4.1436735 -2.3427576 -0.3131049 -0.2922619 -2.9724451 ## 57 58 59 60 61 62 ## 1.7299787 -4.5131873 2.8733432 1.1216380 6.2550511 NA The augment function gives you a warning that it doesn’t include these missing value cases when it creates the augmented data set with residuals and fitted values. Here we still get a 58 cases augmented data frame: &gt; sleep.lm2.exclude.aug &lt;- augment(sleep.lm2.exclude) # danger + bodywt &gt; nrow(sleep.lm2.exclude.aug) # only 58 residuals and fitted values (62-4) ## [1] 58 Add the data=sleep argument to preserve the length of the original sleep data frame (62 cases) but to put NA values in for .fitted and .resid values since these cases were not used to fit the model. &gt; sleep.lm2.exclude.aug &lt;- augment(sleep.lm2.exclude, data=sleep) # danger + bodywt &gt; nrow(sleep.lm2.exclude.aug) # all 62 cases ## [1] 62 &gt; sleep.lm2.exclude.aug %&gt;% # but no resid/fitted provided for missing cases + filter(is.na(.resid)) ## # A tibble: 4 x 19 ## SWS PS TS BodyWt BrainWt Life GP P SE D Species danger ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; ## 1 NA 0.3 NA 529 680 28 400 5 5 5 Giraffe very ~ ## 2 NA NA NA 35 56 16.3 33 3 5 4 Kangar~ high ## 3 NA 1 NA 250 490 23.6 440 5 5 5 Okapi very ~ ## 4 NA NA NA 4.05 17 13 38 3 1 1 Yellow~ very ~ ## # ... with 7 more variables: .fitted &lt;dbl&gt;, .se.fit &lt;dbl&gt;, .resid &lt;dbl&gt;, ## # .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; 3.7 Model Checking After fitting a potential MLR model to a data set, we use residuals to help diagnosis any issues in the “fit” of our model. I.e. are any of our MLR model assumptions violated. We can also use residuals, along with a other measures, to determine if there are any cases that could be influencing our model fit. Timing: You should always check model fit and outliers affects before any model testing or interpretation! Even though this section is after all our inference sections, you need to make sure your choice of model doesn’t violate our model assumptions. Using an ill-fitting model to determine which predictors are “statistically significant” or how they change the mean function is irrelevant if the model is not correctly specificied, or if an outlier or two is seriously influencing the model fit. We will use the same methods as SLR to check the model assumptions of independence (Section 2.8.4) and normality of errors (Section 2.8.3) and when the model is robust (or not) against violations (Section 2.8.5 3.7.1 Residual plots Residual plots for MLR were first discussed in Section 3.3.3 and they are used to check the linearity and constant variance MLR model assumptions. We typically look at \\(p+1\\) plots: residuals vs. \\(\\hat{y}_i\\) and residuals vs. all \\(p\\) predictors. (Here, \\(p\\) tells us how many predictors are in the model, terms like interactions are not used to produce residual plots.) Recall that plot(my.lm, which=1) gives the residuals vs. fitted values. For the regression of total sleep on (log) body weight, life span and danger level (Section 3.6.4), the residual vs. fitted plot is: &gt; plot(sleep.lm3, which=1) If you don’t have a ton of data points, it is sometime nice to turn the smoother off since it is sensitive to areas in the plot with only a few data points: &gt; plot(sleep.lm3, which=1, add.smooth = FALSE) One option for plotting residuals vs. all predictors is the function ggnostic from the GGally package: &gt; library(GGally) &gt; ggnostic(sleep.lm3, columnsY = &quot;.resid&quot;) Again, if we want to turn both the smoother and error ribbon off, you need to add the continuous argument shown below (just setting se=FALSE will remove the ribbon but keep the smoother): &gt; ggnostic(sleep.lm3, columnsY = &quot;.resid&quot;, + continuous = list(.resid=wrap(&quot;nostic_resid&quot;, se=FALSE, method=NULL))) Conclusion: The mean function linearity assumptions seems to be adequately met, since the “middle” of each slice of points is roughly centered at 0. For the boxplot of residuals with danger level, we are looking for each boxplot to be roughly centered around 0 (holding a danger level constant for each boxplot). But there are some concerns about the constant variance assumption: for low fitted values (low predicted total sleep), we see less variation in residuals than for high fitted values. we see less variation in residuals for high levels of danger and high values of body weight and life span. Note that from EDA (above), we see that the association between these predictors and total sleep is negative, meaning these high values of predictors correspond to the low fitted values for predicted sleep. At this stage, you should consider an alternative model version. To help “spread out” the lower sleep values we could try a log-transformation of TS: &gt; sleep.lm3a &lt;- lm(log(TS) ~ log(BodyWt)+danger + log(Life), data=sleep) &gt; plot(sleep.lm3, which=1, add.smooth = FALSE) &gt; ggnostic(sleep.lm3a, columnsY = &quot;.resid&quot;, + continuous = list(.resid=wrap(&quot;nostic_resid&quot;, se=FALSE, method=NULL))) Overall, these residual plots look “better” than before. It looks like the model that best meets our MLR linearity and constant variance assumptions. The normality assumption is violated (see the QQ plot below), but with a little over 50 cases we should be able to trust inferences that involve the mean function (but not prediction intervals). Independence would only be violated if there were predator-prey relationships that would cause a correlation in total sleep among certain groups of species. &gt; plot(sleep.lm3a, which = 2) 3.7.2 Outliers When analyzing one variable, a case that is “far enough” from the mean or median of that variable’s distribution could be deemed “unusual” and called an outlier. In regression modeling, we need to broaden this notion of “unusual” or outlying cases because we have more than one variable involved. There are three common statistics used to measure the “unusualness” of a case: leverage, standardized residual, and Cook’s distance. These are all examples of case influence statistics. 3.7.2.1 Leverage Leverage is a statistic that assesses how unusual a case’s predictor values are compared to average predictor values for all cases. This statistic doesn’t consider a case’s response at all. For SLR, the leverage of case \\(i\\) equals \\[ h_i = \\dfrac{1}{n-1}\\left(\\dfrac{x_i - \\bar{x}}{s_x}\\right)^2 + \\dfrac{1}{n} \\] This basically measures how far \\(x_i\\) is from the mean predictor value, a larger deviation means higher leverage. In the plots below for a SLR model, we see an “outlier” in red in all three scatterplots of \\(y\\) vs. \\(x\\). But only when the red case has an extreme predictor value (along the \\(x\\)-axis) will it have high leverage (shown in the bottom plots). The first case will have a large residual but not a large leverage value. In a MLR model, with more than one predictor, leverage \\(h_i\\) is computed in the same spirit as the SLR formula but using matrix algebra to measure how far a case’s predictor values are from the main “point cloud” of predictor values. Visualizing points with large leverage is harder because we are usually only using 2-dimensional graphs. The plots below show scatterplots for a regression with two predictors, \\(x_1\\) and \\(x_2\\). In all three cases the outlier point in red is a similar distance to the “middle” of the point cloud (shown by the green triangle). Because of this, all three cases have roughly the same leverage value (bottom graphs). Mathematically: \\(\\dfrac{1}{n} \\leq h_i &lt; 1\\) average leverage across all cases is \\(\\bar{h} = (p+1)/n\\). Guidelines for “large” leverage are based on the average leverage value: \\(h_i &gt; 2(p+1)/n\\): potential for some influence \\(h_i &gt; 3(p+1)/n\\): potential for large influence Why care about leverage? A case with high leverage has the potential to be influential in the regression the fit of the regression line or surface. A regression line/surface could be pulled towards cases with high leverage. This is due to the fact that the SE of each observed residual \\(r_i = y_i - \\hat{y}_i\\) is inversely related to leverage: \\[ SE(r_i) = \\hat{\\sigma}\\sqrt{1-h_i} \\] This SE tells us how much variability we will see in case \\(i\\)’s residual when computing the residual for case \\(i\\) for many, many regression fits from many, many sample responses taken from a population with predictor values fixed. (Think back to the SLR simulation in Chapter 2.) A case with \\(h_i \\approx 1\\), means that it’s residual has little variability and all possible regression lines/surfaces will result in \\(\\hat{y_i}\\) being very close to \\(Y_i\\) (since the expected value of each residual is 0). If \\(Y_i\\) for case \\(i\\) is not following the “trend” of the other \\(n-1\\) cases, then this case will be very influential on where the estimated regression line/surface goes. A case with \\(h_i \\approx 1/n\\), means that if \\(n\\) is large, it’s residual has variability of about \\(\\sigma\\) and all possible regression lines/surfaces will result in \\(\\hat{y_i}\\) that can vary widely from \\(Y_i\\). 3.7.2.2 Standardized (internally studentized) residuals Standardized residuals, also known as internally studentized residuals, are the ratio of a residual to its SE: \\[ studr_i = \\dfrac{r_i}{SE(r_i)} = \\dfrac{y_i - \\hat{y}_i}{\\hat{\\sigma}\\sqrt{1-h_i}} \\] The reasons for a case with a large standardized residual are a large \\(r_i\\) value (just poorly predicted) but “regular” leverage value. These cases will have typical predictor values but an unusual response. a “regular” \\(r_i\\) value but a small SE due to a large leverage value. If our normality model assumption holds, then these standardized residuals should behave, roughly, like standard normal values (N(0,1)). Guidelines to flag unusual cases are \\(| studr_i| &gt; 2\\) for smaller data sets \\(| studr_i| &gt; 4\\) for large data sets In the plots below we see Y outlier: This red case has a very large residual value \\(r_i\\) but it doesn’t have high leverage. The extremely high residual value leads to a large standardized residual value. X outlier: This red case has a large negative standardized residual due to its large residual value and small SE (due to large leverage, shown above). X and Y outlier: The left case doesn’t have an unusual standardized residual value because though it has large leverage (shown above), its residual is still small (because it follows the trend of the other cases). The right case has the same leverage as the left case, but this time it has a larger residual because it deviates a bit from the overall trend. For this reason, the right case has a larger standardized residual than the left case. 3.7.2.3 Cook’s distance Cook’s distance, \\(D_i\\), is a measure of a case’s influence on the fitted regression line/surface. It is computed for case \\(i\\) by removing case \\(i\\) from the data, then fitting a regression model and predicting responses for all cases \\(\\hat{Y}_{j(-i)}\\). These “remove \\(i\\)” predictions are compared to the predictions we get when including case \\(i\\), \\(\\hat{Y}_i\\). If these two predictions deviate a lot, overall all cases, then we would say that case \\(i\\) is influential in the regression fit. \\[ D_i = \\sum_{j=1}^n \\dfrac{(\\hat{Y}_{j(-i)} - \\hat{Y}_{j})^2}{(p+1)\\hat{\\sigma}^2} = \\dfrac{studr_i}{p+1} \\dfrac{h_i}{1-h_i} \\] Note in the equation above shows that the measure of the “remove \\(i\\)” predictions from overall predictions can be expressed in terms of a cases’s leverage and standardized residual. A case can have high Cook’s distance if it has high standardized residual, high leverage, or a combination of both. A case should be flagged as likely influential if is has a Cook’s distance for 1 or larger. In the plots below, the red regression line was fit including the red point and the black line was fit without the red point. A case where the red and black lines deviate a lot means that the red point in influential. Y outlier: This red case has a very large residual value \\(r_i\\) but it doesn’t have high leverage. It is not very influential. X outlier: This red case has a large negative standardized residual and large leverage, so it is very influential (\\(D_i \\approx 6\\)). X and Y outlier: The left case doesn’t have an unusual standardized residual value though it has large leverage, so it isn’t very influential because it follows the trend of the other cases. The right case has the same leverage as the left case, but this time it has a larger standardized residual than the left case and it is deemed to be influential (\\(D_i &gt; 3\\)). 3.7.2.4 Example: Sleep Consider the regression of log of total sleep on log-body weight, log-life span and danger level. &gt; library(tidyr) &gt; sleep.noNA &lt;- drop_na(sleep, TS, BodyWt, Life, danger) &gt; sleep.lm3a &lt;- lm(log(TS) ~ log(BodyWt)+danger + log(Life), data=sleep.noNA) &gt; sleep.lm3a ## ## Call: ## lm(formula = log(TS) ~ log(BodyWt) + danger + log(Life), data = sleep.noNA) ## ## Coefficients: ## (Intercept) log(BodyWt) dangerlow dangermoderate ## 2.87435 -0.05598 -0.23465 -0.56386 ## dangerhigh dangervery high log(Life) ## -0.35383 -0.99515 -0.09913 We can use the plot(my.lm, which = 4) command to plot Cook’s distance against row number. Adding id.n=5 will tell R to identify the row number of the 5 largest Cook’s distance cases. Note that this is just a tool for identifying cases, it doesn’t mean that they are influential cases if their values are all well below 1. &gt; plot(sleep.lm3a, which=4, id.n = 5) We can use the plot(my.lm, which = 5) command to plot standardized residuals (y-axis) against leverage (x-axis) with contours given by Cook’s distance. For this model fit, we have two cases (13, 17) with standardized residuals below -2 but these are mid-range leverage cases and all Cook’s distances are below 0.5. THe largest Cook’s distance case, 32, has the third largest leverage value. &gt; plot(sleep.lm3a, which=5, id.n=5) The four cases that have the highest Cook’s distance are rows 13, 17, 28 and 39 in sleep.noNA. These are the (hidden) row names that are being used by the plot command to ID cases. So using the original data frame, sleep, we can ID these cases: &gt; sleep.noNA %&gt;% select(Species, TS, BodyWt, Life) %&gt;% + slice(13, 17, 28, 39) %&gt;% kable() Species TS BodyWt Life Eastern_American_mole 8.4 0.075 3.5 Genet 6.1 1.410 34.0 Lesser_short-tailed_shrew 9.1 0.005 2.6 Rabbit 8.4 2.500 18.0 We can make the augmented data frame to see the case influence stat values for cases. When creating the augmented data frame, I enter the data set sleep.noNA that was used to fit the model sleep.lm3a. This command adds case influence stats and residuals to the the data set. We can see that these Cook’s distance values align with those shown in the Cook’s plot above. &gt; library(broom) &gt; sleep.noNA.aug &lt;- augment(data=sleep.noNA, sleep.lm3a) &gt; sleep.noNA.aug %&gt;% + select(Species, TS, BodyWt, Life, danger, .cooksd, .std.resid, .hat) %&gt;% + slice(13, 17, 28, 39) %&gt;% + arrange(desc(.cooksd)) # report in descending order according to cook&#39;s D ## # A tibble: 4 x 8 ## Species TS BodyWt Life danger .cooksd .std.resid .hat ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Lesser_short-tailed_shrew 9.1 0.005 2.6 high 0.162 -1.87 0.246 ## 2 Eastern_American_mole 8.4 0.075 3.5 very low 0.133 -2.58 0.123 ## 3 Rabbit 8.4 2.5 18 very hi~ 0.121 2.03 0.170 ## 4 Genet 6.1 1.41 34 very low 0.0902 -2.32 0.105 The R package GGally also has plotting options for case influence stats, plotted against predictor variables. Reference lines for leverage (.hat) are average leverage and twice average leverage. &gt; ggnostic(sleep.lm3a, columnsY = c(&quot;.std.resid&quot;, &quot;.hat&quot;, &quot;.cooksd&quot;)) Here we remove the SE ribbon and smoother from the standardized residual plot. &gt; ggnostic(sleep.lm3a, columnsY = c(&quot;.std.resid&quot;, &quot;.hat&quot;, &quot;.cooksd&quot;), + continuous = list(.std.resid=wrap(&quot;nostic_resid&quot;, se=FALSE, method=NULL))) We can use the GGally plots of case influence stats against predictors to help us understand the characteristics of cases that have concerning case influence stats. For example, the smallest body weight case, 0.005kg, corresponds to the high Cook’s distance value (Lesser short-tailed shrew). &gt; summary(sleep.noNA.aug %&gt;% select(BodyWt, Life)) ## BodyWt Life ## Min. : 0.005 Min. : 2.000 ## 1st Qu.: 0.547 1st Qu.: 6.125 ## Median : 3.342 Median : 13.850 ## Mean : 213.037 Mean : 19.852 ## 3rd Qu.: 48.202 3rd Qu.: 27.750 ## Max. :6654.000 Max. :100.000 &gt; sleep.noNA %&gt;% slice(28) ## SWS PS TS BodyWt BrainWt Life GP P SE D Species danger ## 1 7.7 1.4 9.1 0.005 0.14 2.6 21.5 5 2 4 Lesser_short-tailed_shrew high This shrew has a higher leverage value, in part because it also has one of the smallest life spans of 2.6 (min life span is 2). The smallest body weight species also has high leverage, but it doesn’t not have a high Cook’s distance. Why? The standardized residual plot shows that this second smallest body weight case has a standardized residual below 1. Meaning that while it has higher leverage, it’s response is not as far from the regression surface as the smallest body weight shrew. For the shrew, the combo of higher leverage and larger residual combine to give it the highest Cook’s distance value. We can remove a case(s) from a lm fit by either creating a data frame without the case(s) or by using the subset command in the lm fit: lm(y ~ x, data = , subset = -c(row numbers)). This second option is quick and easy for comparing model fits with and without a case. Here we remove row 28 (Lesser short-tailed shrew) by updating our original model with a subset command to omit row 28: &gt; summary(sleep.lm3a) ## ## Call: ## lm(formula = log(TS) ~ log(BodyWt) + danger + log(Life), data = sleep.noNA) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.76693 -0.12239 0.05347 0.21708 0.58685 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.87435 0.18518 15.522 &lt; 2e-16 *** ## log(BodyWt) -0.05598 0.02044 -2.739 0.008683 ** ## dangerlow -0.23465 0.12154 -1.931 0.059565 . ## dangermoderate -0.56386 0.14524 -3.882 0.000322 *** ## dangerhigh -0.35383 0.13109 -2.699 0.009631 ** ## dangervery high -0.99515 0.14785 -6.731 2.09e-08 *** ## log(Life) -0.09913 0.06712 -1.477 0.146359 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3173 on 47 degrees of freedom ## Multiple R-squared: 0.6872, Adjusted R-squared: 0.6473 ## F-statistic: 17.21 on 6 and 47 DF, p-value: 2.105e-10 &gt; # we must use original sleep data to remove the correct &gt; summary(update(sleep.lm3a, subset = -28)) ## ## Call: ## lm(formula = log(TS) ~ log(BodyWt) + danger + log(Life), data = sleep.noNA, ## subset = -28) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.82496 -0.11298 0.04157 0.18939 0.55285 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.92357 0.18194 16.069 &lt; 2e-16 *** ## log(BodyWt) -0.06537 0.02048 -3.193 0.002542 ** ## dangerlow -0.26021 0.11896 -2.187 0.033842 * ## dangermoderate -0.59004 0.14193 -4.157 0.000139 *** ## dangerhigh -0.27344 0.13422 -2.037 0.047408 * ## dangervery high -0.96593 0.14461 -6.679 2.76e-08 *** ## log(Life) -0.11153 0.06560 -1.700 0.095880 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3087 on 46 degrees of freedom ## Multiple R-squared: 0.7104, Adjusted R-squared: 0.6726 ## F-statistic: 18.8 on 6 and 46 DF, p-value: 6.564e-11 If you want to create a “no NA” version of the data set that also excludes the lesser short-tailed shrew, go back and delete row 28 from sleep.noNA: &gt; sleep.noNA.noLSTShrew &lt;- sleep.noNA %&gt;% slice(-28) &gt; # refit with subsetted data &gt; sleep.lm3a.noLSTshrew &lt;- lm(log(TS) ~ log(BodyWt) + danger + log(Life), data = sleep.noNA.noLSTShrew) Comparing the model inferences and fit with and without the lesser short-tailed shrew shows no major changes in our conclusions. For example, the danger variable is still statistically significant when body weight and life span are included. Life is marginally significant when body weight and danger level are included. We have a slight reduction in SSR when short-tailed shrew is excluded, which reduces SE’s a bit and increases \\(R^2\\), but this fact along should not justify the removal of this case. We can’t just cherry-pick cases to improve \\(R^2\\)! So when can we consider excluding a case? Cases can be considered for removal if they change our inference conclusions. If this occurs, then consider: Is the case part of a different population compared to all other cases? If yes, then removal of the case is justified. If no, does the case have high leverage? If yes, describe how it is different in predictor value(s), remove the case and explain the new restricted predictor range for your model If no, then there is no clear path! Report results with and without the case and dig into the data and sampling process to try to understand what makes this case different. 3.8 MLR: Visualizing effects In SLR, we use a scatterplot to display the relationship between \\(y\\) and one predictor \\(x\\) and can see easily see the “effect” of \\(x\\) on \\(y\\) via the slope of the fitted regression line. As shown in Section 3.2.1, the relationship between \\(y\\) and two predictors \\(x_1\\) and \\(x_2\\) can be viewed in a 3-D graphic with the “effect” of each predictor shown with a fitted regression surface. But what if \\(p&gt;2\\)? And why not just use the scatterplot matrix to see how predictors affect the response in an MLR model? In our regression model, the “effect” of \\(x_1\\) tells us how changing \\(x_1\\) changes the mean response, holding all other terms fixed. The scatterplot of \\(y\\) vs. \\(x_1\\) does not hold all other terms fixed. In fact, when model terms are related, this scatterplot can give misleading information about how \\(x_1\\) is related to \\(y\\) in the MLR model. 3.8.1 Partial residual plots One tool use can use to see the how \\(x_i\\) is related to \\(y\\), holding other model terms fixed, is a partial residual plot. Suppose we have two predictors \\(x_1\\) and \\(x_2\\) and we want to understand the form of the mean function: \\[ \\mu_{y \\mid X=x_1, x_2} = ?? \\] Let’s assume that the effect of \\(x_2\\) in this mean function is linear while the form of the relationship with \\(x_1\\) is described by \\(f(x_1)\\), then \\[ \\mu_{y \\mid X} = \\beta_0 + f(x_1) + \\beta_2x_2 \\] We can express our unknown function \\(f\\) in terms of \\[ f(x_1) = \\mu_{y \\mid X} - (\\beta_0 + \\beta_2x_2) \\] To visualize this function with data, we could plot \\[ f(x_{1,i}) = Y_i - (\\beta_0 + \\beta_2x_{2,i}) \\] against the predictors \\(x_{1,i}\\). If \\(f(x_1)\\) is a linear function, then this plot will be linear. If \\(f(x_1)\\) is not a linear function, then this plot will not be linear. In practice, we need to estimate the mean function to compute these “partial” residuals: approximate \\(f(x_1)\\) with a linear function and fit the regression of \\(Y\\) on \\(x_1\\) and \\(x_2\\) to get \\[ \\hat{\\mu}_{y \\mid X} = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2 \\] compute the partial residuals for \\(x_1\\): \\[ pres_{1,i} = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_2x_{2,i}) \\] plot \\(pres_{1,i}\\) against \\(x_1\\) to see what the relationship between \\(y\\) and \\(x_1\\) looks like after “accounting for \\(x_2\\)”. The slope of the least squares line in the partial residuals in this plot for \\(x_1\\) will equal its estimated (linear) effect \\(\\hat{\\beta}_1\\) in the MLR of \\(y\\) on \\(x_1\\) and \\(x_2\\). We get partial residuals for \\(x_2\\) by adjusting for effects of \\(x_1\\): \\[ pres_{2,i} = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1x_{1,i}) \\] If we have a larger MLR model, with more terms, then we end up computing partial residuals for term \\(x_i\\) by adjusting for (substracting off) the effects of all other terms except \\(x_i\\). Using partial residual plots: We can see the “effect” of \\(x_i\\) after adjusting for other model terms. We can also see the variation in \\(y\\) that remains after adjusting for other model terms. We can look for outliers that could be affecting the estimated effect of \\(x_i\\) We can see if the effect of \\(x_i\\) is correctly modeled, signs on non-linearity and/or non-constant variance suggest we need to correct our model form. 3.8.2 Example: Sleep Back to the total sleep example and the regression of total sleep on body weight, brain weight, life span, gestation period (in days) and danger level with all quantitative predictors logged: &gt; sleep.lm4 &lt;- lm(log(TS) ~ log(BodyWt) + log(BrainWt) + log(Life) + log(GP) + danger, data=sleep) &gt; tidy(sleep.lm4, conf.int = TRUE) ## # A tibble: 9 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.37 0.243 13.9 2.85e-17 2.88 3.86 ## 2 log(BodyWt) -0.0277 0.0457 -0.606 5.48e- 1 -0.120 0.0645 ## 3 log(BrainWt) -0.0229 0.0688 -0.332 7.41e- 1 -0.162 0.116 ## 4 log(Life) 0.0486 0.0728 0.668 5.08e- 1 -0.0983 0.196 ## 5 log(GP) -0.201 0.0625 -3.21 2.53e- 3 -0.327 -0.0746 ## 6 dangerlow -0.155 0.108 -1.43 1.59e- 1 -0.374 0.0634 ## 7 dangermoderate -0.442 0.128 -3.45 1.30e- 3 -0.700 -0.183 ## 8 dangerhigh -0.237 0.117 -2.03 4.92e- 2 -0.474 -0.000918 ## 9 dangervery high -0.943 0.128 -7.38 4.19e- 9 -1.20 -0.685 The R package car has a function called crp that will plot partial residual plots for any model that doesn’t have an interaction term: &gt; library(car) &gt; crp(sleep.lm4) In this package, the values plotted on the y-axis are “Component + Residual” which is computed as the partial residual for the predictor variable of interest, minus the average partial residual value. In this way, they are centered around 0 which makes them “feel” like a usual residual, but the trend in points we observe is the effect of the variable on the response after accounting for all other model variables. We can change the layout: &gt; crp(sleep.lm4, layout = c(2,3)) Only plot a subset of terms: &gt; crp(sleep.lm4, terms = ~ log(Life) + log(BrainWt)) The blue trend like is the least squares fit for each partial residual plot, and the slope of this line is the predictor’s estimated effect. For example, the slope for the log(Life) partial residual plot is 0.0486 and the slope for the log(BodyWt) partial residual plot is -0.0277 (both shown in the output table above). And identify unusual cases by their row number (in rownames of original data set): &gt; crp(sleep.lm4, term = ~ log(BodyWt), id = list(n=4)) &gt; sleep %&gt;% slice(1,5,15,32) %&gt;% select(Species, BodyWt, TS) ## Species BodyWt TS ## 1 African_elephant 6654.000 3.3 ## 2 Asian_elephant 2547.000 3.9 ## 3 Eastern_American_mole 0.075 8.4 ## 4 Lesser_short-tailed_shrew 0.005 9.1 3.8.2.1 Comparing TS against Life vs. partial plot for Life TS vs Life: In our initial EDA for Life, we can observe that the scatterplot of TS against Life (both logged) shows a negative relationship between total sleep and life span. As life span gets bigger, total sleep gets smaller, not accounting for any other predictors. &gt; sleep %&gt;% + mutate_at(.vars=c(&quot;BodyWt&quot;, &quot;BrainWt&quot;, &quot;Life&quot;, &quot;GP&quot;, &quot;TS&quot;), .funs = log) %&gt;% + ggpairs(columns=c(&quot;BodyWt&quot;, &quot;BrainWt&quot;, &quot;Life&quot;, &quot;GP&quot;,&quot;TS&quot;), + lower=list(continuous = wrap(&quot;smooth&quot;, se=FALSE)), + columnLabels = c(&quot;log(BodyWt)&quot;,&quot;log(BrainWt)&quot;, &quot;log(Life)&quot;, &quot;log(GP)&quot;, &quot;TS&quot;)) Partial residual plot of Life:But notice that the effect of log(Life) in the MLR model that adjusts for brain size, body size, gestational period and danger level shows a positive association with total sleep (holding other terms fixed)! &gt; crp(sleep.lm4, terms = ~ log(Life)) We can attempt to visualize why we have a negative trend in the scatterplot but a positive trend in the partial residual plot (and MLR estimated effect) with a coded scatterplot that “controls for” one of the model terms. Here we grouped cases together into 5 groups based on their value of BrainWt. So BrainWtQuintile number 1 contains all cases with brain weights in the lower 20% of all brain weights. When we group our cases into similar brain weight cases, we (roughly) are controlling for brain weight. Our least square trends between total sleep and life space are mostly positive within each group of similar brain weights. This is the positive effect of life span on total sleep in our model that controls for brain weight (and other predictors). &gt; sleep %&gt;% + mutate(BrainWtQuintiles = factor(ntile(BrainWt, n=5))) %&gt;% + ggplot(aes(x=Life, y=TS, color= BrainWtQuintiles) )+ + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + scale_x_log10() + scale_y_log10() + + labs(title = &quot;TS against Life, holding brain weight &#39;fixed&#39;&quot;) So why is the scatterplot of total sleep vs. life span negative? This is due to the correlation between the predictors and it really an example of Simpson’s paradox! The association between Life and BrainWt is positive The negative effect of BrainWt on TS is stronger than the effect of Life. As Life increases, so does BrainWt and this increase in BrainWt causes a decrease in TS. Since this effect is stronger than the positive effect of Life, we see a decrease in TS as Life increases. (Notice that the BrainWtQuintile groups trend down as Life gets bigger.) 3.9 Collinearity As shown above with the total sleep example, predictors effects in a MLR model depend on which other predictors are contolled for in a model, especially when predictors are associated with one another. Associations between predictors can make it hard to “see” a predictors effect using EDA, make it difficult to interpret predictor effects since it is hard to “hold fixed” all other predictors if they are correlated, inflate our measures of SE which can make it difficult to identify statistically significant predictors. Two predictor terms \\(x_1\\) and \\(x_2\\) are exactly collinear if for all cases \\[ c_1 x_{1,i} + c_2 x_{2,i} = c \\ \\ \\ \\ \\Rightarrow \\ \\ \\ \\ x_{1,i} = (c - c_2 x_{2,i})/c_1 \\] where the \\(c\\)’s are fixed constants. For example, if \\(x_1\\) measures height in inches and \\(x_2\\) measures height in feet then these terms are exactly collinear and we can write \\(x_1 = 12x_2\\). We can this idea to more than two predictors: \\[ c_1 x_{1,i} + c_2 x_{2,i} + \\dotsm + c_p x_{p,i} = c \\] We often don’t have exactly collinear term, but we can have approximately collinear terms if the linear combinations are approximately constant. We can measure how linearly related predictors are in a model by measuring the \\(R^2_i\\) for the regression of \\(x_i\\) on all other predictors (ignoring the response right now). It turns out that the SE of our model parameters depends on this measure of predictor association. The estimated effect of predictor \\(x_i\\) has a SE of \\[ SE(\\hat{\\beta}_i) = \\dfrac{\\hat{\\sigma}}{\\sqrt{(n-1)s^2_i}} \\sqrt{\\dfrac{1}{1-R^2_i}} \\] Conclusion: This SE is smallest if \\(x_i\\) is not associated with other predictors (\\(R^2_i \\approx 0\\)) and it grows larger the more correlated \\(x_i\\) is with other predictors. A common measure of the collinearity of term \\(x_i\\) with all other terms is the variance inflation factor (VIF): \\[ VIF_i = \\dfrac{1}{1-R^2_i} \\] If \\(R^2_i = 0.5\\), then \\(VIF_i = 2\\) meaning the SE is about \\(\\sqrt{2}=1.4\\) times larger than it would be if \\(x_i\\) was not correlated with other terms. If \\(R^2_i = 0.75\\), then \\(VIF_i = 4\\) meaning the SE is about \\(2\\) times larger than it would be if \\(x_i\\) was not correlated with other terms. If \\(R^2_i = 0.9\\), then \\(VIF_i = 10\\) meaning the SE is about \\(3.2\\) times larger than it would be if \\(x_i\\) was not correlated with other terms. 3.9.1 Example: Sleep Back to the total sleep example and the regression of total sleep on body weight, brain weight, life span, gestation period (in days) and danger level with all quantitative predictors logged: &gt; sleep.lm4 &lt;- lm(log(TS) ~ log(BodyWt) + log(BrainWt) + log(Life) + log(GP) + danger, data=sleep) The car package contains the function vif that gives us the VIF values for this model: &gt; library(car) &gt; vif(sleep.lm4) ## GVIF Df GVIF^(1/(2*Df)) ## log(BodyWt) 15.033055 1 3.877248 ## log(BrainWt) 21.604988 1 4.648117 ## log(Life) 3.541828 1 1.881974 ## log(GP) 3.037467 1 1.742833 ## danger 1.676492 4 1.066719 Note that when we have a factor term (danger), the VIF’s are calculated as usual for quantitative terms but a generalized version (GVIF) is computed for factor terms by basically adding up the cumulative VIF across indicator variables for that factor. Here we see that brain and body weight are highly correlated, mostly with one another when we revisit the scatterplot matrix shown in the previous example. This is a main reason that neither term is statistically significant when the other term is in the model, as shown by the t-test results: &gt; tidy(sleep.lm4, conf.int = TRUE) ## # A tibble: 9 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.37 0.243 13.9 2.85e-17 2.88 3.86 ## 2 log(BodyWt) -0.0277 0.0457 -0.606 5.48e- 1 -0.120 0.0645 ## 3 log(BrainWt) -0.0229 0.0688 -0.332 7.41e- 1 -0.162 0.116 ## 4 log(Life) 0.0486 0.0728 0.668 5.08e- 1 -0.0983 0.196 ## 5 log(GP) -0.201 0.0625 -3.21 2.53e- 3 -0.327 -0.0746 ## 6 dangerlow -0.155 0.108 -1.43 1.59e- 1 -0.374 0.0634 ## 7 dangermoderate -0.442 0.128 -3.45 1.30e- 3 -0.700 -0.183 ## 8 dangerhigh -0.237 0.117 -2.03 4.92e- 2 -0.474 -0.000918 ## 9 dangervery high -0.943 0.128 -7.38 4.19e- 9 -1.20 -0.685 A good strategy with two highly collinear terms is to remove one of the two terms. Here we remove brain weight since it has the highest VIF and a higher p-value that body weight. &gt; sleep.lm5 &lt;- lm(log(TS) ~ log(BodyWt) +log(Life) + log(GP) + danger, data=sleep) &gt; tidy(sleep.lm5, conf.int = TRUE) ## # A tibble: 8 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.38 0.239 14.1 9.00e-18 2.90 3.86 ## 2 log(BodyWt) -0.0413 0.0202 -2.04 4.77e- 2 -0.0821 -0.000442 ## 3 log(Life) 0.0371 0.0633 0.586 5.61e- 1 -0.0907 0.165 ## 4 log(GP) -0.208 0.0578 -3.60 8.13e- 4 -0.325 -0.0916 ## 5 dangerlow -0.162 0.106 -1.53 1.33e- 1 -0.375 0.0514 ## 6 dangermoderate -0.444 0.127 -3.51 1.07e- 3 -0.699 -0.189 ## 7 dangerhigh -0.237 0.116 -2.04 4.70e- 2 -0.471 -0.00325 ## 8 dangervery high -0.938 0.125 -7.47 2.68e- 9 -1.19 -0.684 &gt; anova(sleep.lm5) ## Analysis of Variance Table ## ## Response: log(TS) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## log(BodyWt) 1 5.8426 5.8426 85.3002 9.105e-12 *** ## log(Life) 1 0.0234 0.0234 0.3418 0.5618 ## log(GP) 1 1.5091 1.5091 22.0333 2.739e-05 *** ## danger 4 4.1709 1.0427 15.2234 7.879e-08 *** ## Residuals 43 2.9452 0.0685 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 &gt; vif(sleep.lm5) ## GVIF Df GVIF^(1/(2*Df)) ## log(BodyWt) 3.012225 1 1.735576 ## log(Life) 2.738637 1 1.654883 ## log(GP) 2.651420 1 1.628318 ## danger 1.567496 4 1.057793 Now all but life span is statistically significant and VIF is greatly reduce. "],
["logistic.html", "Chapter 4 Logistic Regression 4.1 The variables 4.2 The Bernoulli distribution 4.3 The logistic model form 4.4 Inference and estimation 4.5 Deviance 4.6 Checking Assumptions 4.7 Residuals and Case influence 4.8 Binomial responses 4.9 Inference for Binomial response models 4.10 Deviance for Binomial responses 4.11 Checking Assumptions 4.12 Residuals and case influence for binomial responses 4.13 Quasi-binomial logistic model", " Chapter 4 Logistic Regression This chapter covers material from chapters 20-22 of Sleuth. 4.1 The variables Suppose we have a categorical response variable \\(Y\\) that can take one of two values, which we will generically call a success or failure. We want to relate the probability of success to \\(p\\) explantory variables (aka predictors, covariates) \\(x_1, \\dotsc, x_p\\). There is no restriction on the type of covariates, they can be both quantitative and categorical variables. 4.1.1 Example: Donner party EDA Sleuth case study 20.1 looks at data from the infamous Donner party. This wagon train was migrating to the west coast of the US in the mid-1800s and became snow-bound in the Sierra Nevada mountains during the winter of 1846-7. We are interested in modeling the categorical variable “survival” (or not) as a function of individual covariates like age or sex. &gt; library(Sleuth3) &gt; donner &lt;- case2001 &gt; summary(donner) ## Age Sex Status ## Min. :15.0 Female:15 Died :25 ## 1st Qu.:24.0 Male :30 Survived:20 ## Median :28.0 ## Mean :31.8 ## 3rd Qu.:40.0 ## Max. :65.0 A stacked bar graph shows that females had a higher surival rate than males: &gt; library(ggplot2) &gt; ggplot(donner, aes(fill = Status, x = Sex)) + + geom_bar(position=&quot;fill&quot;) + + labs(y=&quot;proportion&quot;, title=&quot;Surival rates by Sex&quot;) We can use the dplyr package’s group_by function to divide the data into the two Sex groups and compute the proportion who Survived within each group. Here we see that 2/3 of females survived while only 1/3 of males survived. &gt; library(dplyr) &gt; donner %&gt;% + group_by(Sex) %&gt;% # for each Sex group + summarize(mean(Status == &quot;Survived&quot;)) # proportion who survived ## # A tibble: 2 x 2 ## Sex `mean(Status == &quot;Survived&quot;)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Female 0.667 ## 2 Male 0.333 A side-by-side boxplot shows that people who survived tended to be younger: &gt; library(ggplot2) &gt; ggplot(donner, aes(x = Status, y = Age)) + + geom_boxplot() + + coord_flip() We can get stats by status group: &gt; donner %&gt;% + group_by(Status) %&gt;% # for each status group + summarize(mean(Age), sd(Age), median(Age)) # get summary stats ## # A tibble: 2 x 4 ## Status `mean(Age)` `sd(Age)` `median(Age)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Died 35.5 14.3 30 ## 2 Survived 27.2 8.00 25 But these stats are not quite what we want when trying to model survival as a function of age. E.g. of people who survived, we know that the mean age was 27.2 while of people who died, the mean age was 35.5. But this is the wrong direction of conditioning, we would like to say how survival rates change as we increase age by a year, for example. We could employ a scatterplot for this purpose, but we need to recode (in dplyr) Status into a binary variable that recodes survival as a 1 and death as a 0: &gt; donner$Ind_surv &lt;- dplyr::recode(donner$Status, Survived = 1, Died = 0) Then use a jitter plot to avoid overplotting: &gt; ggplot(donner, aes(x = Age, y = Ind_surv)) + + geom_jitter(aes(color=Status), height = .01) As we increase age, try vertically “slicing” these points. The proportion of survivals in these similar age groups will just be the mean of the binary 0’s and 1’s in each slice. We have more blue points than red in the low age slices than in high age, so we see that the probability of survival tends to decrease as age increases. We will next consider how to construct a model that gives us a “best fit” curve for this probability of surival. 4.2 The Bernoulli distribution The Bernoulli distribution is a probability model for a random trial that has two possible outcomes: success or failure. A Bernoulli random variable \\(Y\\) “counts” the number of successes in a Bernoulli random trial. If a “success” occurred then \\(Y=1\\) and if a “failure” occurred then \\(Y=0\\). We will let \\(\\pi\\) be the probability of success: \\[ \\pi = P(Y=1) = P(success), \\ \\ \\ \\ \\ 1-\\pi = P(Y=0) = P(failure) \\] If \\(Y\\) is a Bernoulli random variable, then we can use the shorthand notation \\(Y \\sim Bern(\\pi)\\) to denote this. The expected value, or mean, of \\(Y\\) is equal to \\[ E(Y) = \\mu = \\pi \\] and the standard deviation of \\(Y\\) is equal to \\[ SD(Y) = \\sigma = \\sqrt{\\pi(1-\\pi)} \\] The expected value (or mean) of a random variable measures the “long run” average value that we would see from \\(Y\\) if we were to repeat the random trial many, many times. The standard deviation tells us how these values of \\(Y\\) will vary over these repeated trials. 4.3 The logistic model form The population, or data generating, model for a logistic regression model for \\(Y\\) assumes that each \\(Y_i\\) is a Bernoulli random variable whose probability of success depends on covariates \\(\\pmb{x_{1,i}, \\dotsc, x_{p,i}}\\). Specifically, \\(Y_i \\mid X_i \\overset{indep.}{\\sim} Bern(\\pi(X_i))\\) binary response: \\(Y_i\\)’s are categorical with only two options independence: Given \\(X_i\\) values, \\(Y_i\\)’s are independent We connect the linear combination of predictors \\[ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i} \\] to the probability of success using the logistic function form: \\[ \\pi(X_i) = \\dfrac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\dfrac{e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}} \\] This function form is used because its inverse is equal to \\[ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}= \\ln \\left( \\dfrac{\\pi(X_i)}{1-\\pi(X_i)}\\right) \\] This function is called the logit function: \\(logit(\\pi) = \\ln \\left( \\dfrac{\\pi}{1-\\pi}\\right)\\). This means that a one unit increase in \\(x_1\\) can be interpreted as an additive \\(\\beta_1\\) change in in the logit function, holding other terms fixed. But what does this mean? The odds of success is defined as the ratio of the probability of success to the probability of failure: \\[ odds = \\dfrac{\\pi(X)}{1-\\pi(X)} \\] For example, if the probability of success is 0.6 then the odds of success is \\(0.6/0.4 = 1.5\\). Meaning for every 6 successes, we see 4 failures. If the probability of success is 0.1, then the odds of success is \\(0.1/0.9 \\approx 0.111\\), meaning for every 1 success we see 9 failures. Odds greater than 1 indicate the probability of success is above 50% while odds less than 1 indicate the probability of success is less than 50%. So, we now can see that the logit function equals the log-odds of success: \\[ \\ln \\left( \\dfrac{\\pi(X_i)}{1-\\pi(X_i)}\\right) = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i} \\] This model form is an example of a generalized linear model which relates the response \\(Y\\) to predictors through a linear combination \\(\\eta\\) of predictors. Is does this by defining the following functions: The kernel mean function defines the expected value (mean) of \\(Y\\) as a function of \\(\\eta\\). in a logistic model, the kernel mean function is the logistic function \\(E(Y \\mid X) = \\pi(X) = \\dfrac{e^{\\eta}}{1+e^{\\eta}}\\) The link function defines the linear combination \\(\\eta\\) as a function of the mean of \\(Y\\). in a logistic model, the link function is the logit function \\(\\eta = \\ln(\\pi/(1-\\pi))\\) These two functions are inverses of one another. 4.3.1 Interpretation Changes in predictors can be interpreted as changes in the odds of success (we can’t make general statement about changes in the probability of success). Specifically, we “unlog” the logit equation to get an expression for the odds of success for predictors \\(x_1, \\dotsc, x_p\\): \\[ odds(x_1, \\dotsc, x_p) = \\dfrac{\\pi(X)}{1-\\pi(X)} = e^{\\beta_0 + \\beta_1 x_{1} + \\dotsm + \\beta_p x_{p}} \\] What happens if we increase \\(x_1\\) by one unit, holding other predictors fixed? \\[ odds(x_1+1, \\dotsc, x_p) = e^{\\beta_0 + \\beta_1 (x_{1}+1) + \\dotsm + \\beta_p x_{p}} = e^{\\beta_0 + \\beta_1 x_{1} + \\dotsm + \\beta_p x_{p}} \\times e^{\\beta_1} \\] Increasing \\(x_1\\) by one unit has a multiplicative change of \\(e^{\\beta_1}\\) in the odds of success. Note that this is a similar interpretation to the exponential model in SLR or MLR. The multiplicative change of \\(e^{\\beta_1}\\) is also called the odds ratio for a one unit increase in \\(x_1\\). An odds ratio is just the ratio of the odds for two different groups, here groups with \\(x_1+1\\) vs. \\(x_1\\): \\[ \\dfrac{\\textrm{odds of succes at } x_1+1}{\\textrm{odds of succes at } x_1} = \\dfrac{odds(x_1+1, \\dotsc, x_p) }{odds(x_1, \\dotsc, x_p) } = e^{\\beta_1} \\] What is we have a predictor that is logged? \\[ odds(x_1, \\dotsc, x_p) = e^{\\beta_0 + \\beta_1 \\ln(x_{1}) + \\dotsm + \\beta_p x_{p}} = e^{\\beta_0}x_1^{\\beta_1} e^{\\beta_2 x_2 + \\dotsm + \\beta_p x_{p}} \\] Then our interpretation is similar to a power model. Changing \\(x_1\\) by a factor of \\(m\\): \\[ odds(mx_1, \\dotsc, x_p) = e^{\\beta_0}(mx_1)^{\\beta_1} e^{\\beta_2 x_2 + \\dotsm + \\beta_p x_{p}} = e^{\\beta_0}x_1^{\\beta_1} e^{\\beta_2 x_2 + \\dotsm + \\beta_p x_{p}} \\times m^{\\beta_1} \\] results in a multiplicative change of \\(m^{\\beta_1}\\) in the odds of success. 4.4 Inference and estimation Estimation of logistic model parameters \\(\\beta_0, \\dotsc, \\beta_p\\) is done using maximum likelihood estimation (MLE). The likelihood function is the probability of the observed data, writen as a function of our unknown \\(\\beta\\)’s *(which are used to compute \\(\\pi(X_i)\\)’s) \\[ L(\\beta) = \\prod_{i=1}^n \\pi(X_i)^{y_i} (1-\\pi(X_i))^{1-y_i} \\] Notice that for each case \\(i\\), the term in this product is equal to just \\(\\pi(X_i)\\) when case \\(i\\) is a success (\\(y_i=1\\)) and equal to \\(1-\\pi(X_i)\\) when case \\(i\\) is a failure (\\(y_i=0\\)). Our MLE method says to find the \\(\\beta\\)’s that maximize the likelihood \\(L(\\beta)\\) of the observed data. Unlike SLR or MLR, there is no “closed form” for these MLE \\(\\hat{\\beta}_i\\) estimates (meaning we can’t write down a formula for the estimates). Rather, software uses a numerical optimization method to compute the MLEs \\(\\hat{\\beta}_i\\) and the standard errors \\(SE(\\hat{\\beta}_i)\\). (The R function glm uses Iterative reweighted least squares.) These MLE estimates of \\(\\beta\\) parameters are approximately normally distributed and unbiased when n is “large enough.” Much like with “intro stats” inference, when your response variable is categorical (or, equivalently, binary 0/1), we usually use the normal distribution for inference (CI and tests). When your response variable is quantitative (like in SLR/MLR models), we usually use the t-distribution for inference. 4.4.1 Confidence intervals for \\(\\pmb{\\beta_i}\\) A \\(C\\)% confidence interval for \\(\\beta_i\\) equals \\[ \\hat{\\beta}_i \\pm z^*SE(\\hat{\\beta}_i) \\] where \\(z^*\\) is the \\((100-C)/2\\) percentile from the \\(N(0,1)\\) distribution. 4.4.2 Hypothesis tests for \\(\\pmb{\\beta_i}\\) We can test the hypothesis \\[ H_0: \\beta_i = \\beta^*_i \\] with the following z-test statistic: \\[ z =\\dfrac{\\hat{\\beta}_i - \\beta^*_i}{SE(\\hat{\\beta}_i)} \\] where \\(\\beta^*_i\\) is our hypothesized value of \\(\\beta_i\\) . The \\(N(0,1)\\) is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. The usual test results given by standard regression output tests whether a parameter value (intercept or slope) is equal to 0 vs. not equal to 0: \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with a test stat of \\[ z =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] 4.4.3 R glm We fit a logistic regression model in R with the glm function. The basic syntax is glm(y ~ x1 + x2, family = binomial, data= ) Careful not to forget the family=binomial argument! If you omit this, you will just be trying to fit a regular MLR model which is not appropriate for a categorical response. The variable y can be either form: y can be binary 0/1 coded response where 1 is a “success” y can be a factor variable with two levels. The second level is what R will call a “success” Once you fit a glm model, you can extract attributes of the model fitted(my.glm) gives the estimated probabilities of success for each case in your data predict(my.glm) gives estimated log-odds of success for each case in your data. Add newdata= to get predicted log-odds for new data. predict(my.glm, type = \"response\") gives estimated probabilities of success for each case in your data. Add newdata= to get predicted probabilities for new data. The broom package also allows us to get fitted probabilities or log odds for all cases in the data, or for new data: augment(my.glm) gets estimated log-odds of success added to the variables used in the glm fit. add data=my.data to get estimated log-odds added to the full data set my.data used in the glm fit add newdata= new.data to get predicted log-odds added to the new data set new.data augment(my.glm, type.predict= \"response\") gets estimated probabilities of success added to the variables used in the glm fit. add data=my.data to get estimated probabilities added to the full data set my.data used in the glm fit add newdata= new.data to get predicted probabilities added to the new data set new.data 4.4.4 Example: Donner party model Let’s revist the Donner party data and start with considering the logistic regression of survival status on age (only). We can add the fitted logistic model probability curve the scatterplot we created in Section 4.1.1. We use the glm smoothing method with an args that specifies the binomial family: &gt; ggplot(donner, aes(x = Age, y = Ind_surv)) + + geom_jitter(aes(color=Status), height = .01) + + geom_smooth(method=&quot;glm&quot;, method.args = list(family=binomial), se=FALSE) + + labs(y = &quot;probability of survival&quot;) We can see that an age of about 30 yields an estimated survival probability of 50% while an age of about 45 yields an estimated survival probability of 25%. We can better quantify these values by fitting the model using the glm function. Here is our simple model, the logistic regression of survival on age: \\[ logit(\\pi) = \\log(\\dfrac{\\pi}{1-\\pi}) = \\beta_0 + \\beta_1 Age \\] &gt; levels(donner$Status) # second level = Surived ## [1] &quot;Died&quot; &quot;Survived&quot; &gt; donner.glm1 &lt;- glm( Status ~ Age , family=binomial, data=donner) &gt; summary(donner.glm1) ## ## Call: ## glm(formula = Status ~ Age, family = binomial, data = donner) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5401 -1.1594 -0.4651 1.0842 1.7283 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.81852 0.99937 1.820 0.0688 . ## Age -0.06647 0.03222 -2.063 0.0391 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 61.827 on 44 degrees of freedom ## Residual deviance: 56.291 on 43 degrees of freedom ## AIC: 60.291 ## ## Number of Fisher Scoring iterations: 4 &gt; confint(donner.glm1) ## 2.5 % 97.5 % ## (Intercept) -0.005987258 3.99016010 ## Age -0.139737905 -0.01016096 The estimated log odds of survival is \\[ logit(\\hat{\\pi}) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = 1.81852 -0.06647 Age \\] and the estimated odds of survival is \\[ \\hat{odds}(age) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = e^{1.81852}e^{-0.06647 Age} \\] and the estimated probability of survival is \\[ \\hat{\\pi}(age) = \\dfrac{e^{1.81852 -0.06647 Age}}{1+e^{1.81852 -0.06647 Age}} \\] A one year increase in age will have a \\(e^{-0.06647} = 0.936\\) multiplicative change on the odds of survival. A one year increase in age decreases the odds of survival by 6.4% (95% CI 0.3% to 12.2%). &gt; exp(-0.06647) # factor change ## [1] 0.935691 &gt; 100*(exp(-0.06647) - 1) # percent change ## [1] -6.430901 &gt; exp(-0.06647 + c(-1,1)*qnorm(0.975)*0.03222) # factor change CI ## [1] 0.8784291 0.9966855 &gt; 100*(exp(-0.06647 + c(-1,1)*qnorm(0.975)*0.03222) - 1) # % change CI ## [1] -12.1570864 -0.3314455 The broom package’s tidy function can also be used to get estimates, SEs and confidence intervals. If we add exponentiate=TRUE, then we we get exponentiated estiamtes and confidence intervals (but SE, test stat and p-values are untouched). &gt; library(broom) &gt; tidy(donner.glm1, conf.int=TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.82 0.999 1.82 0.0688 -0.00599 3.99 ## 2 Age -0.0665 0.0322 -2.06 0.0391 -0.140 -0.0102 &gt; tidy(donner.glm1, conf.int=TRUE, exponentiate = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.16 0.999 1.82 0.0688 0.994 54.1 ## 2 Age 0.936 0.0322 -2.06 0.0391 0.870 0.990 We can use the predict command with response type values to get predicted survival rates for 30 and 45 year olds: &gt; new.ages &lt;- data.frame(Age = c(30, 45)) &gt; predict(donner.glm1, newdata = new.ages, type=&quot;response&quot;) ## 1 2 ## 0.4562149 0.2363774 &gt; exp(1.81852 - 0.06647*30)/(1+exp(1.81852 - 0.06647*30)) # prob age=30 ## [1] 0.4562174 &gt; exp(1.81852 - 0.06647*45)/(1+exp(1.81852 - 0.06647*45)) # prob age=45 ## [1] 0.2363799 So we have \\[ \\hat{\\pi}(age = 30) = \\dfrac{e^{1.81852 -0.06647(30)}}{1+e^{1.81852 -0.06647(30)}} \\approx 0.456 \\] and \\[ \\hat{\\pi}(age = 45) = \\dfrac{e^{1.81852 -0.06647(45)}}{1+e^{1.81852 -0.06647(45)}} \\approx 0.236 \\] Finally, what if we want to understand how the odds of death change as a function of age? Well, odds of death is equal to the ratio death to survival probabilities: \\[ \\hat{odds.death}(age) = \\dfrac{1-\\hat{\\pi}}{\\hat{\\pi}} = \\dfrac{1}{e^{1.81852}e^{-0.06647 Age}} = e^{-1.81852}e^{0.06647 Age} \\] A one year increase in age will have a \\(e^{0.06647} = 1.069\\) multiplicative change on the odds of survival. A one year increase in age increases the odds of death by 6.9% (95% CI 0.3% to 13.8%). &gt; exp(0.06647) # factor change in odds of death ## [1] 1.068729 &gt; 100*(exp(0.06647) - 1) # percent change ## [1] 6.87289 &gt; exp(0.06647 + c(-1,1)*qnorm(0.975)*0.03222) # factor change CI ## [1] 1.003325 1.138396 &gt; 100*(exp(0.06647 + c(-1,1)*qnorm(0.975)*0.03222) - 1) # % change CI ## [1] 0.3325478 13.8395756 We can verify our mathematical work by refitting a glm with an indicator of death: &gt; donner$Ind_death &lt;- dplyr::recode(donner$Status, Survived = 0, Died = 1) &gt; tidy(glm(Ind_death ~ Age, family = binomial, data=donner)) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.82 0.999 -1.82 0.0688 ## 2 Age 0.0665 0.0322 2.06 0.0391 4.4.5 Example: Donner party, adding sex We will fit the logistic regression of survival on age and sex: \\[ logit(\\pi) = \\log(\\dfrac{\\pi}{1-\\pi}) = \\beta_0 + \\beta_1 Age + \\beta_2 Male \\] The estimated mode fit is: &gt; donner.glm2 &lt;- glm( Status ~ Age + Sex, family=binomial, data=donner) &gt; donner.glm2 ## ## Call: glm(formula = Status ~ Age + Sex, family = binomial, data = donner) ## ## Coefficients: ## (Intercept) Age SexMale ## 3.2304 -0.0782 -1.5973 ## ## Degrees of Freedom: 44 Total (i.e. Null); 42 Residual ## Null Deviance: 61.83 ## Residual Deviance: 51.26 AIC: 57.26 &gt; tidy(donner.glm2, conf.int=TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.23 1.39 2.33 0.0198 0.851 6.43 ## 2 Age -0.0782 0.0373 -2.10 0.0359 -0.162 -0.0141 ## 3 SexMale -1.60 0.755 -2.11 0.0345 -3.23 -0.195 &gt; tidy(donner.glm2, conf.int=TRUE, exponentiate = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.3 1.39 2.33 0.0198 2.34 618. ## 2 Age 0.925 0.0373 -2.10 0.0359 0.850 0.986 ## 3 SexMale 0.202 0.755 -2.11 0.0345 0.0396 0.823 The estimated log odds of survival is \\[ logit(\\hat{\\pi}) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = 3.23041 -0.07820 Age -1.59729 Male \\] and the estimated odds of survival is \\[ odds(Sex, Age) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = e^{3.23041}e^{-0.07820 Age}e^{-1.59729 Male} = (25.3)(0.925)^{Age}(0.202)^{Male} \\] and the estimated probability of survival is \\[ \\hat{\\pi}(Sex, Age) = \\dfrac{e^{3.23041 -0.07820 Age -1.59729 Male}}{1+e^{3.23041 -0.07820 Age -1.59729 Male}} \\] The exponentiated coefficient estimates give the odds ratios for a one unit increase in age or for males (compated to females). Holding gender constant, a one year increase in age decreases the odds of survival by 7.5% (95% CI 0.5% to 14.0%). &gt; exp(-0.07820) # age effect on odds ## [1] 0.9247795 &gt; 100*(exp(-0.07820) - 1) # % change ## [1] -7.522055 &gt; exp(-0.07820 + c(-1,1)*qnorm(0.975)*0.03728) # CI for factor ## [1] 0.8596178 0.9948806 &gt; 100*(exp(-0.07820 + c(-1,1)*qnorm(0.975)*0.03728) - 1) # CI for % change ## [1] -14.0382243 -0.5119394 Holding age constant, males had a 79.8% lower odds of survival compared to females (95% CI 11.0% to 95.4%). \\[ \\textrm{estimated odds ratio of survival for males vs females} = \\dfrac{\\hat{odds}(Sex=male,Age)}{\\hat{odds}(Sex=female,Age)} = e^{-1.5973} \\] &gt; exp(-1.59729) ## [1] 0.2024444 &gt; 100*(exp(-1.59729) -1 ) ## [1] -79.75556 &gt; exp(-1.59729 + c(-1,1)*qnorm(0.975)*0.75547) ## [1] 0.0460520 0.8899447 &gt; 100*(exp(-1.59729 + c(-1,1)*qnorm(0.975)*0.75547) - 1) ## [1] -95.39480 -11.00553 What if we wanted the odds ratio for comparing females to males? \\[ \\textrm{estimated odds ratio of survival for females vs males} = \\dfrac{\\hat{odds}(Sex=female,Age)}{\\hat{odds}(Sex=male,Age)} = \\dfrac{1}{e^{-1.5973}} = e^{1.5973} \\] Holding age constant, females had a 4.9-fold increased odds of survival (95% CI 1.1 to 21.7). &gt; # odds ratio Female/Male &gt; 1/exp(-1.59729) ## [1] 4.939628 &gt; 1/exp(-1.59729 + c(-1,1)*qnorm(0.975)*0.75547) ## [1] 21.714581 1.123665 There are two separate log-odds, odds and probability functions for the two levels of Sex in the model. Since we do not have an interaction between Sex and Age, we have a parallel line log-odds model. To plot the probabilities for each Sex as Age varies, we need to get the fitted probabities for each case in the data set: &gt; library(broom) &gt; donner.aug2 &lt;- augment(data=donner, donner.glm2, type.predict = &quot;response&quot;) &gt; head(donner.aug2) ## # A tibble: 6 x 12 ## Age Sex Status Ind_surv Ind_death .fitted .se.fit .resid .hat .sigma ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23 Male Died 0 1 0.459 0.111 -1.11 0.0492 1.10 ## 2 40 Fema~ Survi~ 1 0 0.526 0.160 1.13 0.103 1.10 ## 3 40 Male Survi~ 1 0 0.183 0.0921 1.84 0.0567 1.08 ## 4 30 Male Died 0 1 0.329 0.0924 -0.893 0.0387 1.11 ## 5 28 Male Died 0 1 0.364 0.0949 -0.952 0.0389 1.11 ## 6 40 Male Died 0 1 0.183 0.0921 -0.636 0.0567 1.11 ## # ... with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; Then add geom_line with the .fitted probabilities for each Sex (by linetype) added to the y-axis of the jittered scatterplot: &gt; ggplot(donner.aug2, aes(x = Age, y = Ind_surv)) + + geom_jitter(aes(color=Status), height = .01) + + geom_line(aes(y=.fitted, linetype=Sex)) + + labs(y = &quot;probability of survival&quot;) Note that these lines are not parallel on the probability scale while they are on the log-odds scale. This is because we apply the logistic function to the log-odds to produce our probabilities. The logistic function is not a linear function, so it does not produce parallel lines (or even lines at all). 4.5 Deviance In a MLR, ANOVA is used to determine how much of the overall variability in a quantatitive response is explained by the model. With large \\(n\\) and/or normally distributed errors, F tests can be used to compare models based on how much the residual sum of squares is reduced by adding terms. In a GLM, deviance is the term used to measure “unexplained” variation in the response. When the GLM is a MLR model, deviance equals the residual sum of squares. In a logistic GLM deviance, denoted as \\(G^2\\), is the difference of two likelihoods (defined in @(logistic-est)): \\[ G^2 = 2[\\ln L(\\bar{\\pi}) - \\ln L(\\hat{\\pi}(X))] = 2\\sum_{i=1}^n \\left[ y_i \\ln \\left( \\dfrac{y_i}{\\hat{\\pi}(X_i)} \\right) + (1- y_i) \\ln \\left( \\dfrac{1-y_i}{1-\\hat{\\pi}(X_i)} \\right) \\right] \\] \\(L(\\hat{\\pi}(X)):\\) likelihood of the data that plugs in estimates \\(\\hat{\\pi}(X_i)\\) from the logistic model. \\(L(\\bar{\\pi}):\\) likelihood of the data that plugs in estimates \\(\\bar{\\pi} = y_i\\), basing a case’s “predicted” value soley on the response observed for that case. This is called a saturated model and it will always have a higher likelihood than the logistic model: \\(L(\\bar{\\pi}) \\geq L(\\hat{\\pi}(X))\\) Deviance, sometimes called residual deviance, is close to 0 when the logistic model is a good predictor of the responses. Meaning \\(\\hat{\\pi}(X_i)\\) are close to 1 when \\(y_i = 1\\) and close to 0 when \\(y_i = 0\\). Residual deviance will decrease as model terms are added to the logistic model. We can compare nested logistic models by comparing deviance. 4.5.1 Drop in Deviance test Hypotheses: \\(H_0:\\) reduced model vs. \\(H_A:\\) full model Test Statistic: The likelihood ratio test (LRT) stat compares the drop in deviance from the reduced to the full models \\[ LRT = G^2_{reduced} - G^2_{full} \\] When \\(n\\) is “large enough”, the LRT will have a chi-square (\\(\\chi^2\\)) distribution with \\(df = df_{reduced} - df_{full}\\).The p-value is a right tailed area \\[ p-value = P(\\chi^2 &gt; LRT) = 1- pchisq(LRT, df) \\] Special cases of drop in deviance tests: The overall drop in deviance test compares a null “intercept only” model to a logistic model: \\[ H_0: \\ln(odds) = \\beta_0 \\ \\ \\ \\ H_A: \\ln(odds) = \\beta_0 + \\beta_1 x_1 + \\dotsm + \\beta_p x_p \\] The deviance for the null model is computed by using the overall rate of success as the estimated \\(\\hat{\\pi}\\) for all cases. This null deviance is similar in spirit to the total sum of squares in ANOVA. If our reduced and full models differ by one term, then the drop in deviance test will test the same hypotheses as the z-test (a.k.a. Wald test) for the term, but the two methods of testing are not identical. The two types of tests will usually give results that agree, but if they do not agree you should use the drop in deviance LRT test results. We can compare two logistic models in R via a drop in deviance test using the command anova(reduced.glm, full.glm, test = &quot;Chisq&quot;) The anova command with just one model gives the residual deviance drops for adding each term listed to the model that already contains the terms above it. (Much like anova for a MLR.) 4.5.2 Example: NES The National Election Studies project recorded party identification for two random samples of people during 1980 and 2000. &gt; nes &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/NES.csv&quot;) &gt; head(nes) ## year age gender race region income union dem educ ## 1 year1980 70 male black S lower 1/3 no 1 HS or less ## 2 year1980 67 male white NC middle 1/3 yes 1 HS or less ## 3 year1980 47 female black S lower 1/3 no 1 HS or less ## 4 year1980 52 female white W upper 1/3 yes 0 College ## 5 year1980 30 female white NC upper 1/3 no 1 HS or less ## 6 year1980 37 male black NC upper 1/3 no 1 College Here we recode the binary Democrat voter party affiliation dem variable to record two levels: Democrat and Other. Here we see the proportion of Democrats (in red) by region for the survey years of 1980 and 2000. In the south, we see a decrease in the proportion of Democrats between 1980 and 2000. In the north east, we see an increase in the proportion of Democrats between 1980 and 2000. This gives us evidence that the effect of year (1980 vs 2000) on the odds of being a Democrat depend on the region of the country. &gt; # recode to make a factor version: &gt; nes$party &lt;- recode_factor(nes$dem, `1`=&quot;Democrat&quot;, `0`=&quot;Other&quot;) &gt; ggplot(nes, aes(x=year, fill = party)) + + geom_bar(position=&quot;fill&quot;) + + facet_wrap(~region) This EDA suggests that an interaction between year and region is needed, so our first model for the log odds of being a Democrat (dem = 1) looks like \\[ \\ln(odds) = \\beta_0 + \\beta_1NE + \\beta_2S + \\beta_3W +\\beta_4Year2000 + \\beta_5NE:2000 + \\beta_6S:2000 + \\beta_7 W:2000 \\] &gt; nes.glm1 &lt;- glm(dem ~ region*year , data=nes, family = binomial) &gt; summary(nes.glm1) ## ## Call: ## glm(formula = dem ~ region * year, family = binomial, data = nes) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.3666 -1.2049 0.9993 1.1131 1.1969 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.04581 0.12359 -0.371 0.71090 ## regionNE 0.02811 0.18159 0.155 0.87698 ## regionS 0.45127 0.16199 2.786 0.00534 ** ## regionW 0.11035 0.19184 0.575 0.56515 ## yearyear2000 0.19893 0.16924 1.175 0.23982 ## regionNE:yearyear2000 0.25334 0.25923 0.977 0.32842 ## regionS:yearyear2000 -0.63257 0.22136 -2.858 0.00427 ** ## regionW:yearyear2000 -0.08701 0.25748 -0.338 0.73540 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3083.3 on 2231 degrees of freedom ## Residual deviance: 3065.5 on 2224 degrees of freedom ## AIC: 3081.5 ## ## Number of Fisher Scoring iterations: 4 The residual deviance for this model is \\(G^2 = 3065.5\\) and the model has 2224 degrees of freedom. The null deviance of 3083.3 is for the “intercept only” model. The Wald z-tests suggest that terms involving the south are statistically significant. The model without the interaction terms also has a null deviance of 3083.3 (since the response and models are the same). But the reduced, no interaction, model’s residual deviance is higher than the interaction model at 3081.9. &gt; nes.glm2 &lt;- glm(dem ~ region+year , data=nes, family = binomial) &gt; summary(nes.glm2) ## ## Call: ## glm(formula = dem ~ region + year, family = binomial, data = nes) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.261 -1.252 1.096 1.105 1.152 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.059324 0.095847 0.619 0.536 ## regionNE 0.132352 0.128824 1.027 0.304 ## regionS 0.113739 0.110055 1.033 0.301 ## regionW 0.068130 0.127806 0.533 0.594 ## yearyear2000 0.002029 0.085211 0.024 0.981 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 3083.3 on 2231 degrees of freedom ## Residual deviance: 3081.9 on 2227 degrees of freedom ## AIC: 3091.9 ## ## Number of Fisher Scoring iterations: 3 This reduced model form suggests that neither year nor region are statistically significant. After fitting an interaction model for region and year, we can use a drop in deviance test to determine if the effect of year depends on region by comparing the interaction and no interaction models: \\[ H_0: \\ln(odds) = \\beta_0 + \\beta_1NE + \\beta_2S + \\beta_3W +\\beta_4Year2000 \\] \\[ H_A: \\ln(odds) = \\beta_0 + \\beta_1NE + \\beta_2S + \\beta_3W +\\beta_4Year2000 + \\beta_5NE:2000 + \\beta_6S:2000 + \\beta_7 W:2000 \\] &gt; anova(nes.glm2, nes.glm1, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: dem ~ region + year ## Model 2: dem ~ region * year ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2227 3081.9 ## 2 2224 3065.5 3 16.361 0.0009562 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The LRT stat equals \\[ LRT = 3081.9 - 3065.5 = 16.361 \\] The degrees of freedom for the test is 3, so the p-value is \\[ P(\\chi^2 &gt; 16.361) = 1-pchisq(16.361, 3) = 0.00096 \\] &gt; 1-pchisq(16.361, 3) ## [1] 0.0009562069 We can conclude that the full model is better than the smaller model. There is at least one region’s change in party affiliation between 1980 and 2000 that is different from the other regions. The statistical significance of the southern region show that this region’s change in affiliation between years is different than the change in the NC (baseline) region. 4.6 Checking Assumptions The two main assumptions that we can check with a binary logistic: independence of responses and linearity of the log odds of success. Independence: The probability of success for case \\(i\\) only depends on the predictors for case \\(i\\), and not on any unmodeled characteristics. Log-odds linearity: For quantitative predictors, we need a linear relationship between the log odds of success and the predictor (or a transformed version). To do this, plot the empirical (sample) log-odds against the predictor and look for linearity. Since the predictor is quantatitive, you often need to do this as follows: group cases into groups with similar predictor values within each group, compute the proportion of successes \\(\\tilde{\\pi}_{emp}\\) then compute the log odds of success \\(logit_{emp} = \\ln(\\dfrac{\\tilde{\\pi}_{emp}}{1-\\tilde{\\pi}_{emp}})\\) 4.6.1 Example: Boundary Waters Canoe Area (BWCA) blowdown A severe windstorm blew through northern MN the evening of July 4, 1999, impacting the a large portion of the BWCA. After the storm, foresters surveyed the area to assess damamge and understand how some trees survived the storm while others did not. The data set blowBF.csv contains data on 659 balsam fir trees: 426 survived the storm and while 233 did not. For this first simple logistic model we are interested in modeling the probability that a tree died (or survived) during the storm as a function of its diameter (inches). We are phrasing this in terms of “died” because of the way the response y is coded: a 1 means died and 0 means survived. The coding matters only so we interpret model coefficients correctly. The same conclusions about the relationship between diameter and died/survived would be the same regardless of the coding. To determine whether diameter needs to be transformed, we need to construct an empirical log odds plot. THere are 659 cases in the data set, so if we divide cases into 20 groups we will get just over 30 cases within each quantile bin. Here we use the dplyr function ntile to divide cases by diameter D into 20 groups based on the 5th, 10th, 15, …, 95th percentiles of D: &gt; blowBF &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/blowBF.csv&quot;) &gt; dim(blowBF) ## [1] 659 5 &gt; blowBF &lt;- mutate(blowBF, D.grps = ntile(D, n = 20)) &gt; table(blowBF$D.grps) ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 32 Then we group_by our binning D.grps variable and compute the summaries: (1) the median diameter in each group, the proportion of success (died) in each group and the log-odds of success (died) in each group. &gt; blowBF.empLO &lt;-blowBF %&gt;% + group_by(D.grps) %&gt;% + summarize(D.grps.med = median(D), # median D of groups + pi.emp = mean(y), # proportion died + log.odds.emp = log(pi.emp/(1-pi.emp))) # log odds A plot of the empirical log-odds against median diameter in each group shows some curvature: &gt; blowBF.empLO ## # A tibble: 20 x 4 ## D.grps D.grps.med pi.emp log.odds.emp ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 5 0.0606 -2.74 ## 2 2 5 0.0606 -2.74 ## 3 3 5 0.0909 -2.30 ## 4 4 6 0.0909 -2.30 ## 5 5 6 0.0303 -3.47 ## 6 6 6.5 0.182 -1.50 ## 7 7 7 0.212 -1.31 ## 8 8 7 0.152 -1.72 ## 9 9 8 0.182 -1.50 ## 10 10 8 0.152 -1.72 ## 11 11 9 0.212 -1.31 ## 12 12 9 0.424 -0.305 ## 13 13 10 0.364 -0.560 ## 14 14 10.5 0.485 -0.0606 ## 15 15 11 0.636 0.560 ## 16 16 13 0.727 0.981 ## 17 17 14 0.606 0.431 ## 18 18 15 0.788 1.31 ## 19 19 17 0.818 1.50 ## 20 20 21.5 0.812 1.47 &gt; ggplot(blowBF.empLO, aes(x=D.grps.med, y=log.odds.emp)) + + geom_point() Looking at diameter on the log-scale shows a more linear plot. This is our reason for using log(D) in our logistic model for tree survival status. &gt; ggplot(blowBF.empLO, aes(x=D.grps.med, y=log.odds.emp)) + + geom_point() + + scale_x_log10() 4.7 Residuals and Case influence The case influence statistics of leverage and Cook’s distance are used with logistic models, just as they are with regular linear models. Both can be obtained with the broom augment command and we can plot then against predictors using the ggnostic command from GGally. You can also see Cook’s distance against case number with plot(my.glm, which = 4). Residuals are not so clear cut for a GLM compared to a MLR. For now, we will just concern ourselves with response residuals which are a case’s binary response minus it’s predicted probabity of success (estimated mean value): \\[ r_i = y_i - \\hat{\\pi}(X_i) \\] We can get these residuals by requesting the “response” type of residual: resid(my.glm, type = \"response\") augment(my.glm, type.predict = \"response\", type.residuals = \"response\") These response residuals are always between -1 and 1, and should average out to 0. Values close to -1 or 1 are cases that could be of interest since they are poorly predicted. Using them in a “usual” residual plot doesn’t always lead to a usual visual model checking tool. Gelman and Hill suggest binning, or grouping, cases by a predictor and computing mean residual value within each group (much like the empirical log odds plot) group cases into groups with similar predictor values within each group, compute the mean response residual plot mean predictor value against mean response residual value for each group. A null plot will show mean residual values with no trend around the horizontal 0-line. We do not need to see constant variance in this plot because our Bernoulli response model does not assume constant variance. 4.7.1 Example: Boundary Waters Canoe Area (BWCA) blowdown Back to the BWCA data. Let’s fit the logistic regression of the binary indicator of death y against the log of diameter. We see that the odds of death increase as diameter increases, which agrees with our empirical log odds plot from Section 4.6.1 &gt; fir.glm&lt;- glm(y ~ log(D), family=binomial, data=blowBF) &gt; tidy(fir.glm) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -7.89 0.633 -12.5 9.92e-36 ## 2 log(D) 3.26 0.276 11.8 3.02e-32 We’ve already computed a binning variable D.grps for diameter in Section 4.6.1. &gt; head(blowBF) ## X D S y status D.grps ## 1 1 9 0.0242120 0 survived 11 ## 2 2 11 0.0305947 0 survived 14 ## 3 3 9 0.0305947 0 survived 11 ## 4 4 9 0.0341815 0 survived 11 ## 5 5 5 0.0341815 0 survived 1 ## 6 6 8 0.0341815 0 survived 9 We now need to augment the (entire) data set with response residuals &gt; blowBF.aug &lt;- augment(data=blowBF, fir.glm, type.predict = &quot;response&quot;, type.residuals = &quot;response&quot;) &gt; head(blowBF.aug) ## # A tibble: 6 x 13 ## X D S y status D.grps .fitted .se.fit .resid .hat .sigma ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 9 0.0242 0 survi~ 11 0.327 0.0216 -0.327 0.00212 0.999 ## 2 2 11 0.0306 0 survi~ 14 0.484 0.0258 -0.484 0.00267 0.998 ## 3 3 9 0.0306 0 survi~ 11 0.327 0.0216 -0.327 0.00212 0.999 ## 4 4 9 0.0342 0 survi~ 11 0.327 0.0216 -0.327 0.00212 0.999 ## 5 5 5 0.0342 0 survi~ 1 0.0667 0.0127 -0.0667 0.00261 0.999 ## 6 6 8 0.0342 0 survi~ 9 0.249 0.0204 -0.249 0.00223 0.999 ## # ... with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; Plotting these residuals against log diameter shows an “odd” looking residual plot that is hard to interpret: &gt; ggplot(blowBF.aug, aes(x=D, y=.resid)) + + geom_jitter(height = .05) + + scale_x_log10() + + geom_hline(yintercept = 0) To help understand if the average residual value for each diameter varies around 0 we can using the binning variable D.grps and find the mean residual value for each group. &gt; blowBF.resid &lt;- blowBF.aug %&gt;% + group_by(D.grps) %&gt;% + summarize(lnD.grps.med = median(log(D)), # median log(D) of groups + fitted.mean = mean(.fitted), # mean prob + resid.mean = mean(.resid)) # mean residual We then plot the mean binned residual against the median (middle) log diameter value for each group. Here we see mean residual values that roughly vary around 0 which suggests that our transformation choice for diameter looks okay. &gt; ggplot(blowBF.resid, aes(x=lnD.grps.med,y=resid.mean)) + + geom_point() + + geom_hline(yintercept = 0) What if we hadn’t logged diameter? The binned residual plot shows more of a trend then the log diameter model. Residuals are mostly negative for small diameters (trees that survived (0) that have probabilities of death that are “too high”) and mostly positive values for mid- to high-diameter bins (trees that died (1) that have probabilities of death that are “too low”). &gt; fir.glmBad &lt;- glm(y ~ D, family=binomial, data=blowBF) &gt; blowBF.augBad &lt;- augment(data=blowBF, fir.glmBad, type.predict = &quot;response&quot;, type.residuals = &quot;response&quot;) &gt; blowBF.residBad &lt;- blowBF.augBad %&gt;% + group_by(D.grps) %&gt;% + summarize(D.grps.med = median(D), # median D of groups + fitted.mean = mean(.fitted), # mean prob + resid.mean = mean(.resid)) # mean residual &gt; ggplot(blowBF.residBad, aes(x=D.grps.med,y=resid.mean)) + + geom_point() + + geom_hline(yintercept = 0) 4.8 Binomial responses Logistic models can also be used with responses that are Binomial counts. For each case \\(i\\) we define: \\(Y_i=\\) number of successes in \\(m_i\\) independent Bernoulli (success/failure) trials \\(X_i = (x_{1,i}, \\dotsc, x_{p,i})\\) be the predictors for this case \\(\\pi(X_i)\\) is the probability of success for each of the \\(m_i\\) trials For example, suppose we have \\(n\\) plots of land and we plant \\(m_i\\) seeds on each. A reponse \\(Y_i\\) could be the number of seeds that germinate in plot \\(i\\). Each seed has a \\(\\pi(X_i)\\) probability of germinating, and this probability could depend on plot-level predictors like amount of rain in plot \\(i\\) or amount of fertilizer applied to plot \\(i\\). Here is an example of what this data could look like for a sample of \\(n=3\\) plots plot ID Y (# germinate) m (# plants) Fertilizer (lbs/plot) Rain (inches) 1 2 4 1 3 2 4 5 2.4 1.1 3 1 4 1.6 2.9 Under the conditions listed above, each response is modeled by a Binomial distribution: \\[Y_i \\mid X_i \\overset{indep.}{\\sim} Binom(m_i, \\pi(X_i))\\] The mean response for each case is \\[ E(Y_i \\mid X_i) = \\mu_{y \\mid x} = m_i \\pi(X_i) \\] and the standard deviation is \\[ SD(Y_i \\mid X_i) = \\sigma_{y \\mid x} = \\sqrt{m_i \\pi(X_i)(1-\\pi(X_i))} \\] We again use a logistic function (kernel mean function) to express the probability of success as a function of predictors: \\[ \\pi(X_i) = \\dfrac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\dfrac{e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}} \\] and a logit (log odds) function to express the linear combination of predictors as a function of the probability of success: \\[ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}= \\ln \\left( \\dfrac{\\pi(X_i)}{1-\\pi(X_i)}\\right) \\] Interpretation of a logistic model for a binomial response is exactly the same as a binary response model (4.3.1). 4.8.1 Connection to binary responses When we only observed \\(m_i = 1\\) trials, then the Binomial count is exactly the same as a binary Bernoullie count since it can only take on the value of 0 or 1: \\[ Binom(m_i = 1, \\pi(X_i)) = Bern(\\pi(X_i)) \\] We can always “expand” binomial count data to represent all \\(\\sum m_i\\) Bernoulli trials that are recording success and failures across all trials. For example, in the plant scenario above, a “trial” represents the success or failure of one plant. So we could have recorded plant-level characteristics with 2 successes in plot 1 and 2 failures (out of \\(m_1 = 4\\) plants), etc. This binary data representation would look like where Binary_Y is a 1 if a plant germinates and a 0 if it did not: plot ID Binary_Y m (# plants) Fertilizer (lbs/plot) Rain (inches) 1 1 4 1 3 1 1 4 1 3 1 0 4 1 3 1 0 4 1 3 2 1 5 2.4 1.1 2 1 5 2.4 1.1 2 1 5 2.4 1.1 2 1 5 2.4 1.1 2 0 5 2.4 1.1 3 1 4 1.6 2.9 3 0 4 1.6 2.9 3 0 4 1.6 2.9 Notice in this representation of the data, we are just repeated the common plot-level predictor values that are common to all plants within a plot. This is an inefficient way to represent our data unless we have plant-level measurements too. This binary representation also is not a great way to store this data because it limits our ability to check the model assumptions about independence. We will expand on this idea in our “goodness-of-fit” discussion below. 4.9 Inference for Binomial response models Here are the similarities between binomial and binary response logistic models: Interpretation of \\(\\beta\\) in the two models is the same. “Wald” z-tests and confidence intervals for \\(\\beta\\) parameters are the same. Drop-in-deviance model comparison tests are the same. R functions of fitted, predict, augment are the same. Here are some key differences: The formula for deviance \\(G^2\\) is different because our probability model is (slightly) different. In binary models, Wald inference relies on “large \\(n\\)”. In binomial models, we either need “large \\(n\\)” and/or large \\(m_i\\) values. E.g. In the binomial model, \\(n=5\\) is fine if all \\(m_1 = 1000\\). The R function glm wants a response equal to the empirical proportion of successes, \\(Y_i/m_i\\), along with the number of binomial trials \\(m_i\\) in our glm specification: glm(y/m ~ x1 + x2, family = binomial, weights = m, data=) 4.9.1 Example: Krunnit Islands archipelago (Sleuth Case Study 21.1) In case study 21.1, we have data collected from 18 islands in this archipelago. In 1949, scientists recorded the number of “at risk” species on each island. Ten years later, they recorded how many of these species were extinct. We want to model the extinction rate for each island as a function of the island size. &gt; library(Sleuth3) &gt; island &lt;- case2101 &gt; summary(island) ## Island Area AtRisk Extinct ## Hietakraasukka: 1 Min. : 0.070 Min. : 6.00 Min. : 2.00 ## Isonkivenletto: 1 1st Qu.: 0.625 1st Qu.:22.00 1st Qu.: 3.25 ## Kraasukka : 1 Median : 2.150 Median :31.00 Median : 5.50 ## Lansiletto : 1 Mean : 19.804 Mean :35.11 Mean : 6.00 ## Luusiletto : 1 3rd Qu.: 4.725 3rd Qu.:42.25 3rd Qu.: 8.00 ## Maakrunni : 1 Max. :185.800 Max. :75.00 Max. :13.00 ## (Other) :12 &gt; nrow(island) ## [1] 18 Our variables are Island gives us an identifier of each island (case) Area is our predictor \\(x_i\\) for each island AtRisk is \\(m_i\\), the number of animals available for extinction Extinct is \\(y_i\\), the number (out of \\(m_i\\)) that went extinct 4.9.1.1 EDA Our model of interest is the logistic regression of the number extinct against area, meaning the log odds is a linear function of area (or some function of area). Our first model attempt will look like: \\[ \\log \\left( \\dfrac{\\pi(x_i)}{1-\\pi(x_i)}\\right) = \\beta_0 + \\beta_1 area_i \\] where \\(\\pi(X_i)\\) represents the probability of extinction (“success”) for island \\(i\\). We check to see if area is linearly related to the log odds by first computing the proportion of extinctions (“successes”) for each island, then using this to compute the empirical log odds. Here we do this by using mutate to add prop and logOdds to the island data set: &gt; library(dplyr) &gt; island &lt;- mutate(island, + prop = Extinct/AtRisk, + logOdds = log(prop/(1-prop))) Then we plot the logOdds against area, and see that we definitely don’t have a linear relationship: &gt; library(ggplot2) &gt; ggplot(island, aes(x = Area, y = logOdds)) + + geom_point() + + labs(title=&quot;empirical log odds vs. area&quot;) Since area is very right skewed, the first transformation choice could be log. We do find that log of area is linearly related to the log odds of extinction. &gt; ggplot(island, aes(x = Area, y = logOdds)) + + geom_point() + + scale_x_log10() + labs(title=&quot;empirical log odds vs. log(area)&quot;) 4.9.1.2 Model fit Our logistic model says that given an island size \\(x_i\\), \\(Y_i\\), the number of extinctions on island \\(i\\), is a binomial variable \\[ Y_i \\mid X_i \\sim Binom(\\pi(x_i), m_i) \\] where \\(\\pi(X_i)\\) is the probability of extinction for each of the \\(m_i\\) at risk species on island \\(i\\). Based on the EDA, we will fit the logit model: \\[ \\log \\left( \\dfrac{\\pi(x_i)}{1-\\pi(x_i)}\\right) = \\beta_0 + \\beta_1 \\ln(area_i) \\] The glm fit needs to use the ratio of \\(y\\) (Extinct) to \\(m\\) (AtRisk) as the response and weights equal to \\(m\\) (AtRisk): &gt; krunnit.glm &lt;- glm(Extinct/AtRisk ~ log(Area), family=&quot;binomial&quot;, weights=AtRisk, data=island) &gt; summary(krunnit.glm) ## ## Call: ## glm(formula = Extinct/AtRisk ~ log(Area), family = &quot;binomial&quot;, ## data = island, weights = AtRisk) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.71726 -0.67722 0.09726 0.48365 1.49545 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.19620 0.11845 -10.099 &lt; 2e-16 *** ## log(Area) -0.29710 0.05485 -5.416 6.08e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45.338 on 17 degrees of freedom ## Residual deviance: 12.062 on 16 degrees of freedom ## AIC: 75.394 ## ## Number of Fisher Scoring iterations: 4 The estimated odds log odds of extinction is \\(-1.1962 - 0.29710\\log(area)\\) and the estimated odds of extinction is \\[ \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = e^{-1.1962 - 0.29710\\ln(area)} = e^{-1.1962}Area^{- 0.29710} \\] The Wald z-test shows that the effect of area on extinction rates is statistically signicant (z=-5.416, p &lt; 0.0001). Doubling an island area is associated with an 18.6% reduction in the odds of extinction of at risk species. We are 95% confident that doubling the area of an island is associated with anywhere from a 12.3% to 24.5% decrease in extiction rates. \\[ OR = \\dfrac{\\hat{odds}(2 \\times Area)}{\\hat{odds}(Area)} = 2^{- 0.29710} = 0.814 \\] &gt; # factor change calcluations: &gt; 2^-.29710 # factor change in odds for double ## [1] 0.8138868 &gt; 2^(-.29710 + c(-1,1)*qnorm(.975)*0.05485) # CI for factor ## [1] 0.7554436 0.8768512 &gt; # percent change calculations: &gt; (2^-.29710 - 1)*100 # % change for double ## [1] -18.61132 &gt; (2^(-.29710 + c(-1,1)*qnorm(.975)*0.05485) - 1)*100 # CI for % change ## [1] -24.45564 -12.31488 We are 95% confident that a 10% increase in the area of an island is associated with a 1.8% to 3.8% decrease in the odds of extinction for at risk species. &gt; # increase area by 10% = multiply by factor of 1.1 &gt; 1.1^-.29710 ## [1] 0.9720805 &gt; (1.1^-.29710 - 1)*100 ## [1] -2.79195 &gt; # 95% CI for effect on odds &gt; 1.1^(-.29710 + c(-1,1)*qnorm(.975)*0.05485) ## [1] 0.9621712 0.9820919 &gt; (1.1^(-.29710 + c(-1,1)*qnorm(.975)*0.05485) - 1)*100 ## [1] -3.782880 -1.790814 The estimated log odds of extinction for a 5 km\\(^2\\) island is \\[ \\dfrac{\\hat{\\pi}(x=5)}{1-\\hat{\\pi}(x=5)} = e^{-1.1962 - 0.29710\\log(5)} = e^{-1.1962}5^{- 0.29710} = -1.674364 \\] The estimated probability of exinction for at risk species on a 5 km\\(^2\\) island is 15.7%. \\[ \\hat{\\pi}(x=5) = \\dfrac{e^{-1.1962 - 0.29710\\log(5)}}{1+e^{-1.1962 - 0.29710\\log(5)}}= \\dfrac{e^{-1.674364}}{1+e^{-1.674364}} = 0.1578432 \\] &gt; # estimated probability of extiction on a 5 km^2 island &gt; predict(krunnit.glm, newdata=list(Area=5), type=&quot;response&quot;) ## 1 ## 0.157843 &gt; # verify (not needed, just make sure you can also do these calculations by hand for exam 3) &gt; ( logit &lt;- -1.1962 - 0.2971*log(5) ) ## [1] -1.674364 &gt; ( prob &lt;- exp(logit)/(1+exp(logit)) ) ## [1] 0.1578432 4.9.1.3 Visualize We can add the estimated extinction probability curve using the geom_smooth function, adding glm as the smoothing method and family equal to Binomial. We also need to specify the \\(m_i\\) used in the glm fit and this is done as an aes argument with weight = AtRisk. If you forget the weight argument you won’t be seeing the correct probability curve! &gt; ggplot(island, aes(x=log(Area), y = Extinct/AtRisk, weight = AtRisk)) + + geom_point() + + geom_smooth(method=&quot;glm&quot;, se=FALSE, method.args = list(family=&quot;binomial&quot;)) + + labs(title=&quot;Extinction probability as a function of area&quot;) 4.10 Deviance for Binomial responses In Binomial logistic regression, the likelihood function is equal to \\[ L(\\beta) = \\prod_{i=1}^n \\binom{m_i}{y_i} \\pi(X_i)^{y_i} (1-\\pi(X_i))^{m_i-y_i} \\] In a binomial logistic regression the deviance is equal to \\[ G^2 = 2[\\ln L(\\bar{\\pi}) - \\ln L(\\hat{\\pi}(X))] = 2\\sum_{i=1}^n \\left[ y_i \\ln \\left( \\dfrac{y_i}{m_i\\hat{\\pi}(X_i)} \\right) + (m_i- y_i) \\ln \\left( \\dfrac{m_i-y_i}{m_i-m_i\\hat{\\pi}(X_i)} \\right) \\right] \\] Note that cases with large \\(m_i\\) can contribute the most to this summation of individual case-level deviance. Again, the two likelihoods that are used to construct residual deviance are: \\(L(\\hat{\\pi}(X)):\\) likelihood of the data that plugs in estimates \\(\\hat{\\pi}(X_i)\\) from the logistic model. \\(L(\\bar{\\pi}):\\) likelihood of the data that plugs in estimates \\(\\bar{\\pi} = y_i/m_i\\), basing a case’s “predicted” value soley on the response observed for that case. This is called a saturated model and it will always have a higher likelihood than the logistic model: \\(L(\\bar{\\pi}) \\geq L(\\hat{\\pi}(X))\\) Deviance for binomial models can be used for two types of hypothesis tests: Drop-in-deviance: Used to compare two models, just like in binary logistic models. Details are in Section 4.5. Goodness-of-fit: Used to test binomial response model adequacy. 4.10.1 Goodness-of-fit test In our binomial logistic model, we are assuming that an observed count for case \\(i\\) behaves like a binomial random variable: \\(Y_i \\mid X_i \\sim Binom(m_i, \\pi(X_i))\\) where \\(\\pi(X_i)\\) is a function of \\(p+1\\) \\(\\beta\\) parameters and the \\(p\\) predictor terms. A viable alternative model is the saturated “model” described above, which really isn’t a model that relates predictors to the response. Rather, it just uses the \\(n\\) empirical proportion of successes \\(\\bar{\\pi} = y_i/m_i\\) for each case as the probability of success for all \\(m_i\\) trials. (So there are \\(n\\) parameters in this “model”.) The saturated model will have the best “fit” (highest likelihood) of the two models, but if the “fit” (likelihood) of the logistic model is “close” then we can claim that the logistic model is adequate. This is the motivation behind a “goodness-of-fit” test. Our hypothese for the GOF test are: \\[ H_0: \\textrm{logistic model} \\] vs. \\[ H_A: \\textrm{saturated model} \\] The test statistic for this is equal to the residual deviance of the logistic model (difference in the \\(H_A\\) and \\(H_0\\) likelihoods): \\[ G^2 = 2[\\ln L(\\bar{\\pi}) - \\ln L(\\hat{\\pi}(X))] \\] If \\(H_0\\) is true and the data does fit the logistic model, then when \\(m_i\\)’s are large, \\(G^2\\) will have an approximate chi-square distribution with \\(n - (p+1)\\) (model) degrees of freedom. (Note that \\(n-(p+1)\\) is the difference in the number of parameters in the two models.) Since large differences in the two likelihoods suggest that the null model is not adequate, the p-value is the probability of getting residual deviance values larger than the observed value: \\[ p-value = 1-P(\\chi^2 &gt; G^2) = 1-pchisq(G^2, df=n-(p+1)) \\] The suggested rule of thumb for “large \\(m\\)” is that we want most \\(m_i\\)’s to be at least 5. For a GOF test, you should consider the following interpretations for “large” or “small” p-values: Do not reject the null: (large p-value) Your logistic model is adequate. You don’t have a large enough sample size \\(n\\) to have the power to detect inadequacies in your model. Reject the null: (small p-value) You have outlier(s) that are inflating the residual deviance. Your logistic model is inadequate. Your log-odds model is inadequate, it is ill-fitting and transformations are needed Extra-binomial variation: your response counts aren’t well modeled by a Binomial model. For each case: trials are not independent probability of success is not constant across trials your choice of predictors isn’t sufficient (i.e. you are missing key explanatory variables) 4.10.2 Example: Krunnit Islands archipelago (Sleuth Case Study 21.1) For our extinction model, our GOF test hypotheses are \\[ H_0: \\ln(odds) = \\beta_0 + \\beta_1 \\ln(area), \\ \\ \\ \\ \\ H_A: \\ln odds= \\alpha_i \\textrm{ (saturated model) } \\] We can conduct a GOF test because all our \\(m_i\\)’s (AtRisk), are above 5: &gt; summary(island$AtRisk) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 6.00 22.00 31.00 35.11 42.25 75.00 Our test stat is the residual deviance of \\(G^2 = 12.062\\) from the null model &gt; summary(krunnit.glm) ## ## Call: ## glm(formula = Extinct/AtRisk ~ log(Area), family = &quot;binomial&quot;, ## data = island, weights = AtRisk) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.71726 -0.67722 0.09726 0.48365 1.49545 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.19620 0.11845 -10.099 &lt; 2e-16 *** ## log(Area) -0.29710 0.05485 -5.416 6.08e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45.338 on 17 degrees of freedom ## Residual deviance: 12.062 on 16 degrees of freedom ## AIC: 75.394 ## ## Number of Fisher Scoring iterations: 4 We have \\(18-2 = 16\\) degrees of freedom for our model, so the p-value is equal to \\[ p-value = 1- P(\\chi^2 &gt; 12.062) = 1- pchisq(12.062, df=16) = 0.7397 \\] &gt; 1-pchisq(12.062, df=16) ## [1] 0.7397009 The large p-value means that we do not reject the null hypothesis. Our model for the probability of extinction given log-area looks to be adequate. Note that if we hadn’t used log of area in our model our conclusions from the GOF would suggest something different: &gt; krunnit.glmBad &lt;- glm(Extinct/AtRisk ~ Area, family=&quot;binomial&quot;, weights=AtRisk, data=island) &gt; summary(krunnit.glmBad) ## ## Call: ## glm(formula = Extinct/AtRisk ~ Area, family = &quot;binomial&quot;, data = island, ## weights = AtRisk) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6526 -1.0661 -0.1877 1.0038 2.1860 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.305957 0.117339 -11.130 &lt; 2e-16 *** ## Area -0.010121 0.002684 -3.771 0.000163 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45.338 on 17 degrees of freedom ## Residual deviance: 24.661 on 16 degrees of freedom ## AIC: 87.993 ## ## Number of Fisher Scoring iterations: 4 &gt; 1-pchisq(24.661, df=16) ## [1] 0.07602884 The residual deviance for this model is about double the deviance of the log-area model. The much smaller p-value of 0.076 suggest that the null model may not be adequate, which our initial EDA for the log-odds suggested. 4.11 Checking Assumptions Assumptions in a binomial logistic model include the same assumptions as binary models (Section @(logistic-assump)): cases are independent and log-odds are a linear function of predictors. Independence of cases takes an understanding of how the data was collected. Log-odds linearity can be checked with an empirical log-odds plot against quantitative predictors (as done in Section 4.9.1.1) and residual plots. A third assumption is that given predictor values, the counts of successes \\(Y_i\\) has a binomial distribution. This means that for each case \\(i\\) the \\(m_i\\) trials are independent events, and a success or failure for one trial doesn’t affect the outcome of another trial, and the probability of success \\(\\pi(X_i)\\) is the same for all \\(m_i\\) trials. If one, or both, of these assumptions is violated, then it often induces extra-binomial variation (a.k.a. overdispersion). This means that the actual variation in our response \\(SD(Y\\mid X_i)\\) is larger than the binomial SD of \\(\\sqrt{m_i\\pi(X_i)(1-\\pi(X_i))}\\) that our model assumes. So the response variation is more dispersed than what our model assumes, making our reported standard errors and p-values too small and we could be over-reporting statistical significance. We can use the goodness-of-fit test (4.10.1), when \\(m_i\\) are large enough, to check our binomial distribution assumption. If we do find evidence of lack-of-fit in our binomial model, then you should Check deviance residuals as case influence stats to see if an outlier(s) is affecting GOF results. Check the log odds form and see if transformations of quantitative predictors are needed If outliers and transformations aren’t a concern, then consider an alternative model: binary logistic model if trial-level predictors are available quasi-binmial logistic model (Section 4.13) a model that allows for correlated trials (like a mixed-effects logistic model) 4.11.1 Example: Krunnit Islands archipelago (Sleuth Case Study 21.1) How might the Krunnit Island extinction counts violate the binomial counts model assumptions: Independence: This assumption implies that the extinction, or not, of all at risk species on an island are independent events. This could be violated if the extinction of one species makes the extinction of a second more likely. This might occur if the prey of a predator species goes extinct, making extinction of the predator species more likely. Probability: This assumption implies that the probability \\(\\pi(X_i)\\) of extinction on island \\(i\\) is the same for all at risk species on island \\(i\\). This could be violated if, for example, species living primarily on the interior of the island had a lower chance of extinction than species living on the coastal region. Since location of each at risk species is unaccounted for in our model, then the overall number of extinct species on the island is not a binomial variable. 4.12 Residuals and case influence for binomial responses We have a few options for residuals for a binomial logistic model. The two most common residuals to consider are Pearson residuals and deviance residuals. Pearson residuals are basically response residuals standardized based on the binomial SD: \\[ pr_i = \\dfrac{ y_i - m_i\\hat{\\pi}(X_i)}{\\sqrt{m_i\\hat{\\pi}(X_i)(1-\\hat{\\pi}(X_i))}} \\] We can get these residuals by requesting the “pearson” type of residual: resid(my.glm, type = \"pearson\") augment(my.glm, type.residuals = \"pearson\") Deviance residuals are each case’s contribution to the residual deviance, with a \\(\\pm\\) based on whether we over- or under-estimate a case’s response (the \\(\\pm\\) is denoted by \\(sign(y_i - m_i\\hat{\\pi}(X_i))\\)): \\[ Dres_i = sign(y_i - m_i\\hat{\\pi}(X_i)) \\sqrt{2 \\left[ y_i \\ln \\left( \\dfrac{y_i}{m_i\\hat{\\pi}(X_i)} \\right) + (m_i- y_i) \\ln \\left( \\dfrac{m_i-y_i}{m_i-m_i\\hat{\\pi}(X_i)} \\right) \\right] } \\] We can get these residuals by requesting the default residual values: resid(my.glm) augment(my.glm) Pearson residuals are “easy” to interpret as the number of estimated SD’s a response is from it’s estimated mean. Deviance residuals are good to check if you find significant results in a GOF test. When \\(m_i\\)’s are large (at least 5), both types of residuals should be similar in value and have a \\(N(0,1)\\) distribution (approximately). This means that most cases (~95%) should have residual values no more extreme than \\(\\pm 2\\). Regardless of size of \\(m_i\\), we should plot residuals vs. quantitative predictors to assess linearity of the log odds. Case influence stats of leverage and Cook’s distance can also be used to look for outliers. In a GLM, leverage measures both a cases’s “extremeness” in terms of it’s predictor values and it’s extremeness in terms of it’s weight \\(m_i\\) in the model. A case with high value of \\(m_i\\) has a high number of trials and we are better able to estimate the probability of success for this case compared to a case with a smaller number of trials. For this reason, cases with higher value of \\(m_i\\) are given more weight, and hence higher leverage, in the fitted model (e.g. in the estiamtes of \\(\\hat{\\beta}\\)). The value of Cook’s distance also takes into account a cases leverage (measured both by predictor values and by \\(m_i\\) size) and a case’s residual value. You can get Pearson residuals, leverage and Cook’s distance with plot(my.glm, which = 5). You can also get leverage and Cook’s distance vs. predictor values with ggnostic(my.glm, columnsY = c(\".hat\",\".cooksd\")) from the GGally package. 4.12.1 Example: Krunnit Islands archipelago (Sleuth Case Study 21.1) Here we use the augment command to get both sets of residuals for the logistic regression of extinction on log area. Neither plot shows an obvious trend, so the linearity assumption looks to be met. &gt; island.aug &lt;- augment(krunnit.glm, type.residual=&quot;pearson&quot;) &gt; plotA &lt;- ggplot(island.aug, aes(x=log.Area., y=.resid)) + + geom_point() + + geom_hline(yintercept = 0) + + labs(title=&quot;Pearson residual plot&quot;) &gt; &gt; island.aug &lt;- augment(krunnit.glm) &gt; plotB &lt;- ggplot(island.aug, aes(x=log.Area., y=.resid)) + + geom_point() + + geom_hline(yintercept = 0) + + labs(title=&quot;Deviance residual plot&quot;) &gt; &gt; library(gridExtra) &gt; grid.arrange(plotA, plotB, ncol=2) Since all at risk counts are rather large (all cases are 6 or larger), we can expect the residuals to be approximately \\(N(0,1)\\). The residual plots above show no cases that are extreme is magnitude. A normal QQ plot shows rough normality with the min and max residuals case deviating a bit from the reference line. &gt; plot(krunnit.glm, which=2) We can plot residuals and case influence stats with all 18 cases identified with the plot command. Here we see that rows 1,2 and 17 have largest leverage while case 3 looks to have the highest Cook’s distance value. &gt; plot(krunnit.glm, which=5, id.n=18) Here we see that the case influence stats against log area and against the model weights, number at risk: &gt; library(GGally) &gt; ggnostic(krunnit.glm, columnsY = c(&quot;.hat&quot;,&quot;.cooksd&quot;)) Finally, we dig a bit deeper and look at the data for the four highlighted cases above. We also plot weight (number at risk) against log area and highlight the small residual cases in blue (cases 2 and 3) and the larger residual cases (1, 17). &gt; island.aug &lt;- augment(krunnit.glm, data=island, type.predict = &quot;response&quot;) &gt; island.aug %&gt;% slice(1,2,3,17) %&gt;% select(-.se.fit, -.sigma) ## # A tibble: 4 x 11 ## Island Area AtRisk Extinct prop logOdds .fitted .resid .hat .cooksd ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Ulkok~ 186. 75 5 0.0667 -2.64 0.0602 0.233 0.293 1.63e-2 ## 2 Maakr~ 106. 67 3 0.0448 -3.06 0.0704 -0.874 0.242 1.41e-1 ## 3 Risti~ 30.7 66 10 0.152 -1.72 0.0985 1.35 0.185 2.89e-1 ## 4 Tiira~ 0.2 40 13 0.325 -0.731 0.328 -0.0381 0.266 3.59e-4 ## # ... with 1 more variable: .std.resid &lt;dbl&gt; &gt; summary(select(island.aug, Area, AtRisk)) ## Area AtRisk ## Min. : 0.070 Min. : 6.00 ## 1st Qu.: 0.625 1st Qu.:22.00 ## Median : 2.150 Median :31.00 ## Mean : 19.804 Mean :35.11 ## 3rd Qu.: 4.725 3rd Qu.:42.25 ## Max. :185.800 Max. :75.00 &gt; ggplot(island.aug, aes(x=Area, y=AtRisk, size=.hat)) + + geom_point() + + scale_x_log10() + + geom_point(data=slice(island.aug, 1,17), color=&quot;red&quot;) + + geom_point(data=slice(island.aug, 2,3), color=&quot;blue&quot;)+ + labs(title=&quot;at risk vs. area, size denotes leverage&quot;) &gt; &gt; ggplot(island.aug, aes(x=Area, y=AtRisk, size=.cooksd)) + + geom_point() + + scale_x_log10() + + geom_point(data=slice(island.aug, 1,17), color=&quot;red&quot;) + + geom_point(data=slice(island.aug, 2,3), color=&quot;blue&quot;)+ + labs(title=&quot;at risk vs. area, size denotes Cook&#39;s distance&quot;) Case 1 (Ulkokrunni) is the island with the largest area and the largest number at risk (\\(m_i\\)). For this reason case 1 has largest leverage but it doesn’t have an large residual value so it doesn’t have high Cook’s distance. (See the fitted probability curve in Section 4.9.1.3). Case 17 (Tiirakari) has the second smallest area but a large number of at risk species given its small size. For this reason it has larger leverage than case 18 which has the smallest area but smaller number at risk. The fitted probability curve is very close to this case, though, so it has a small residual and low Cook’s distance. &gt; island.aug %&gt;% slice(18) %&gt;% select(-.se.fit, -.sigma) ## # A tibble: 1 x 11 ## Island Area AtRisk Extinct prop logOdds .fitted .resid .hat .cooksd ## &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Risti~ 0.07 6 3 0.5 0 0.400 0.496 0.0708 0.0103 ## # ... with 1 more variable: .std.resid &lt;dbl&gt; Case 3 (Ristikari) has the third largest area (30.7) but it’s number at risk (66) is only one smaller than case 2 (Maakrunni) which is the second largest area (106). Case 3 also has a much larger residual than case 2, which results in it having the highest Cook’s distance value in the data set. In the end, though, none of these cases is overly influential in the model it and removal of case 3, the highest Cook’s distance, changes the estamate of \\(\\beta_1\\) from 0.30 to 0.33 and it’s significance doesn’t change. No cases need to be omitted from our analysis. &gt; summary(glm(Extinct/AtRisk ~ log(Area), family=&quot;binomial&quot;, weights=AtRisk, data=island, subset=-3)) ## ## Call: ## glm(formula = Extinct/AtRisk ~ log(Area), family = &quot;binomial&quot;, ## data = island, weights = AtRisk, subset = -3) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.69885 -0.51675 0.02167 0.56804 1.47846 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.21921 0.11994 -10.165 &lt; 2e-16 *** ## log(Area) -0.33234 0.06148 -5.405 6.47e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 45.1373 on 16 degrees of freedom ## Residual deviance: 9.7483 on 15 degrees of freedom ## AIC: 69.087 ## ## Number of Fisher Scoring iterations: 4 4.13 Quasi-binomial logistic model A quasi-binomial logistic regression model is one model option when your GOF test suggests that your binomial model assumptions of independent trials and/or constant probability of success across trials are not met. The idea behind this model is to estimate the amount of “overdispersion” and use this value to correct your binimial model SEs. In a quasi-binomial model we let \\(\\psi\\) measure overdispersion so that the responce variance equals: \\[ V_{quasi}(Y_i \\mid X_i)= \\psi m_i \\pi(X_i)(1-\\pi(X_i)) =\\psi V_{binom}(Y_i \\mid X_i) \\] If \\(\\psi&gt;1\\), then our responses are more variable than binomial responses should be and \\(V_{quasi}(Y_i \\mid X_i) &gt; V_{binom}(Y_i \\mid X_i)\\). It is possible to have underdispersion, where \\(\\psi &lt; 1\\) in which case \\(V_{quasi}(Y_i \\mid X_i) &lt; V_{binom}(Y_i \\mid X_i)\\). A “quasi” model is not a real model in the sense that we are defining a probability model and likelihood for the \\(Y_i\\)’s. Rather, we are still going to fit a binomial model and use MLE estimates from the binomial likelihood for our estimated parameters. We then estimate \\(\\psi\\) and adjust our binomial model SE accordingly to adjust over overdispersion. Steps to fitting a quasi-binomial model: Fit the binomial logistic regression model, then estimate the dispersion parameter by comparing the model residual deviance to \\(n-(p+1)\\) which is it’s expected value if the model was adequate: \\[ \\hat{\\psi} = \\dfrac{G^2}{n-(p+1)} \\] Parameter estimates for \\(\\beta\\) are from the binomial model. Standard errors for \\(\\hat{\\beta}\\)’s are expanded (if \\(\\hat{\\psi}&gt;1\\)) to account for overdispersion by a factor of \\(\\sqrt{\\hat{\\psi}}\\): \\[ SE_{quasi}(\\hat{\\beta}_i) = \\sqrt{\\hat{\\psi}}SE_{binom}(\\hat{\\beta}_i) \\] Conduct “z”-inference (Wald tests/CI) using SEs equal to \\(SE_{quasi}(\\hat{\\beta}_i)\\) Compare quasi-binomial models using a F-test stat equal to \\[ F = \\dfrac{G^2/(\\textrm{# terms tested})}{\\hat{\\psi}} \\] using an F-distribution with degrees of freedom equal to the number of terms tested and \\(n-(p+1)\\). (The “usual” df for an ANOVA F test.) We can get inference results for steps 1-4 above by adding family = quasibinomial to our glm regression fit: glm(y/m ~ x1 + x2, family = quasibinomial, weights = m, data=mydata) We can get compare quasi-binomial models using deviance with the command anova(red.quasi, full.quasi, test = &quot;F&quot;) Note that the method of estimation dispersion as \\(G^2/(n-(p+1))\\) is one way to estimate this parameter. This idea is based on the fact that the \\(G^2\\) value will have an approximate \\(\\chi^2\\) distribution with expected value of \\(n-(p+1)\\) when the binomial model is adequate. Comparing the actual value of \\(G^2\\) to this expectation tells us our dispersion rate. Another way to compute this value is to look at the sum of the squared Pearson residuals, which should also have an expected value of \\(n-(p+1)\\). The ratio of these two quantities is actually what the glm function uses to compute the dispersion parameter. Often there is no pratical difference between these two ways of estimation the dispersion parameter. 4.13.1 Example: Moth predation (Case Study 21.2) Sleuth case study 21.2 models the probability that a moth is “removed” from a tree (eaten by a bird) as a function of the color of the moth (morph, light or dark) and the distance the moth was from Liverpool (Distance, one of 7 locations, measured in km). The number of moths available at each location and in each color group is measured by Place. &gt; library(Sleuth3) &gt; moth &lt;- case2102 &gt; dim(moth) ## [1] 14 4 &gt; summary(moth) ## Morph Distance Placed Removed ## dark :7 Min. : 0.00 Min. :52.00 Min. : 9.00 ## light:7 1st Qu.:11.43 1st Qu.:57.00 1st Qu.:16.25 ## Median :30.20 Median :60.00 Median :20.00 ## Mean :27.23 Mean :69.14 Mean :21.86 ## 3rd Qu.:40.23 3rd Qu.:83.00 3rd Qu.:23.75 ## Max. :51.20 Max. :92.00 Max. :40.00 To make our glm results match those of the textbook (which have an indicator for the dark level), we will relevel Morph to make dark the second level (Section C.4): &gt; library(forcats) &gt; moth$Morph2 &lt;- fct_relevel(moth$Morph, &quot;light&quot;, &quot;dark&quot;) &gt; levels(moth$Morph2) ## [1] &quot;light&quot; &quot;dark&quot; We will now fit the binomial logistic regression of number removed on the interaction of morph color and distance: &gt; moth.glm &lt;- glm(Removed/Placed ~ Morph2*Distance, + family=binomial, weights= Placed, data=moth) &gt; summary(moth.glm) ## ## Call: ## glm(formula = Removed/Placed ~ Morph2 * Distance, family = binomial, ## data = moth, weights = Placed) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.21183 -0.39883 0.01155 0.68292 1.31242 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.717729 0.190205 -3.773 0.000161 *** ## Morph2dark -0.411257 0.274490 -1.498 0.134066 ## Distance -0.009287 0.005788 -1.604 0.108629 ## Morph2dark:Distance 0.027789 0.008085 3.437 0.000588 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 13.230 on 10 degrees of freedom ## AIC: 83.904 ## ## Number of Fisher Scoring iterations: 4 A goodness-of-fit test doesn’t give a strong indication of overdispersion, but as the book suggests, given the design of the study, it is completely reasonable to assume there are other factors besides distance from Liverpool that could affect the probability of removal at each location. Since these factors aren’t accounted for in the model, we may want to use a quasi-binomial model that accounts for this unmodeled variation. &gt; 1-pchisq(13.23, df=10) # GOF p-value ## [1] 0.2110951 Using the glm results for the binomial fit, our estimated dispersion parameter is \\[ \\hat{\\psi} = \\dfrac{G^2}{n-(p+1)} = \\dfrac{13.23}{14-(3+1)} = 1.323 \\] In a quasi-binomial model, the SE’s will be larger than the binomial model SE’s by a factor of \\(\\sqrt{1.323} = 1.150\\), or about 15% larger. For example, the quasi-binomial SE for the interaction term will be \\[ SE_{quasi} = \\sqrt{1.323} \\times 0.008085 \\approx 0.0092995 \\] We can fit this quasi-binomial model in R, and get these dispersion and SE values as follows: &gt; moth.quasi &lt;- glm(Removed/Placed ~ Morph2*Distance, + family=quasibinomial, weights= Placed, data=moth) &gt; summary(moth.quasi) ## ## Call: ## glm(formula = Removed/Placed ~ Morph2 * Distance, family = quasibinomial, ## data = moth, weights = Placed) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.21183 -0.39883 0.01155 0.68292 1.31242 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.717729 0.214423 -3.347 0.0074 ** ## Morph2dark -0.411257 0.309439 -1.329 0.2134 ## Distance -0.009287 0.006525 -1.423 0.1851 ## Morph2dark:Distance 0.027789 0.009115 3.049 0.0123 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 1.270859) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 13.230 on 10 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 &gt; sqrt(1.270859) ## [1] 1.127324 Notice that the dispersion parameter is estimated as 1.27 by the glm function, using the ratio of the sum of squared Pearson residuals against degrees of freedom: &gt; sum(resid(moth.glm, type=&quot;pearson&quot;)^2)/10 ## [1] 1.270859 Both dispersion estimates, either 1.32 or 1.27, are valid estimates and both indicate that the binomial model SEs are expanded by a factor of about 1.13-1.15. The quasi-binomial SE for the interaction term is reported to be about 0.009 which is what we computed by hand above. We can compare the model with and without the interaction term with a quasi-binomial F test comparison: &gt; moth.quasi2 &lt;- glm(Removed/Placed ~ Morph2 + Distance, + family=quasibinomial, weights= Placed, data=moth) &gt; summary(moth.quasi2) ## ## Call: ## glm(formula = Removed/Placed ~ Morph2 + Distance, family = quasibinomial, ## data = moth, weights = Placed) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.28292 -1.16122 0.00237 1.03757 1.98945 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.136742 0.235365 -4.830 0.000528 *** ## Morph2dark 0.404052 0.209269 1.931 0.079680 . ## Distance 0.005314 0.006008 0.884 0.395404 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasibinomial family taken to be 2.25436) ## ## Null deviance: 35.385 on 13 degrees of freedom ## Residual deviance: 25.161 on 11 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 &gt; anova(moth.quasi2, moth.quasi, test=&quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: Removed/Placed ~ Morph2 + Distance ## Model 2: Removed/Placed ~ Morph2 * Distance ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 11 25.161 ## 2 10 13.230 1 11.931 9.3885 0.01196 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F test stat is computed as the drop in deviance between the two models, divided by the number of terms tested and the dispersion parameter: \\[ F = \\dfrac{(25.161 - 13.230)/1}{1.323} = \\dfrac{11.931/1}{1.323} \\approx 9.0181406 \\] This test stat yields a p-value of 0.013 which, along with the Wald z-test, supports the conclusion that the interaction term is statistically significant. &gt; 1-pf(9.02, 1, 10) #1=numerator df, 10=denominator df ## [1] 0.01326791 "],
["poisson.html", "Chapter 5 Poisson Regression 5.1 The Poisson distribution 5.2 The Poisson model form 5.3 EDA 5.4 Inference and estimation 5.5 Deviance for Binomial responses 5.6 Checking Assumptions 5.7 Residuals and case influence for binomial responses 5.8 Quasi-Poisson logistic model", " Chapter 5 Poisson Regression This chapter covers material from chapter 23 of Sleuth. 5.1 The Poisson distribution The Poisson distribution is a probability model for a random variable that counts the number of “events” (or successes) that occur in a fixed period of time and/or space. For example: the number of accidents that occur per month at an intersection the number of cancer cases per year in a county the number of potholes per mile on interstate 35 To model these types of counts with a Poisson distribution, we must assume that two events can’t occur at exactly the same time/location (e.g. two different accidents can’t occur at exactly the same time) events occur independently over time/space (e.g. the occurance of one accident isn’t going to make another more or less likely) If \\(Y\\) is a random variable with a Poisson distribution, then the probability that it equals some number \\(y\\) is \\[ P(Y = y) = \\dfrac{e^{\\lambda}\\lambda^y}{y!} \\ \\ \\textrm{ for any } y = 0,1,2,\\dotsc \\] Note that Poisson counts \\(Y\\) don’t have an “upper limit” like Binomial counts do so it can take on any integer value from 0 on up. The parameter \\(\\lambda\\) (“lambda”) in this model tells us the mean response value, or the expected number of events per unit of time/space: \\[ E(Y) = \\mu = \\lambda \\] We need to have \\(\\lambda &gt;0\\) since our counts can’t take on negative values. The variance of \\(Y\\) is also equal to the mean: \\[ Var(Y) = \\sigma^2 = \\lambda \\] and the standard deviation is then equal to \\[ SD(Y) = \\sigma = \\sqrt{\\lambda} \\] So the expected rate of events \\(\\mu = \\lambda\\) also tells us how variable our observed counts are likely to be. The larger the mean rate of events, the more variable our counts will be. Hence, our Poisson regression model will not have a “constant variance” assumption. The Poisson distribution is also skewed right for small values of the mean rate but becomes more symmetric as \\(\\mu\\) gets bigger. The plots below display Poisson distributions for a few values of \\(\\mu\\). 5.2 The Poisson model form We will assume that our response \\(Y_i\\) for case \\(i\\) is modeled by a Poisson distribution with mean (expected rate) that we will denote as \\(\\mu(Y_i \\mid X_i)\\) (instead of \\(\\lambda\\), just to be consistent with earlier models): \\[ Y_i \\mid X_i \\overset{indep.}{\\sim} Pois(\\mu(Y_i \\mid X_i)) \\] Our modeling goal is once again to model the mean response \\(\\mu(Y_i \\mid X_i)\\) as a function of predictor \\(X_i = (x_{1,i}, \\dotsc, x_{p,i})\\). To do this we need to ensure that we have a model that always maps values of \\(X_i\\) to positive values of \\(\\mu(Y_i \\mid X_i)\\). We will do this by using another generalized linear model. The link function for our model is the log function defines the linear combination of predictors as the log of the mean response: \\[ \\ln(\\mu(Y_i \\mid X_i)) = \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i} \\] The kernel mean function for our model, which is the inverse of the link, is the exponential function so that \\[ \\mu(Y_i \\mid X_i) = e^{\\eta_i} =e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}} \\] This ensures that for any value of \\(\\eta_i\\), we get a positive value for our mean response. This is an example of a log-linear model. The scatterplots below show an example of data generated from a SLR model and a Poisson model. Notice that the mean line in a SLR is a linear function of \\(x\\) and we see symmetric, constant variation around the mean line. In the Poisson model data, we see a non-linear increase in the mean response as \\(x\\) grows, along with increasing variation around the mean line. The variation around the mean line is right-skewed for smaller mean values but becomes more symmetric for larger mean values. 5.2.1 Interpretation If we have untransformed predictors, we can say that a 1 unit increase in \\(x_1\\), for example, is associated with a \\(e^{\\beta_1}\\) multiplicative change in the mean response \\(\\mu\\): \\[ \\mu(Y \\mid X+1) = e^{\\beta_0 + \\beta_1 (x_{1}+1) + \\dotsm + \\beta_p x_{p}} = \\mu(Y \\mid X)e^{\\beta_1} \\] Notice that this is similar to interpretation of an exponential model in SLR/MLR (Section 2.10), except that in the Poisson model we are setting our linear combination of terms equal to the logged-mean response \\(\\ln(\\mu_Y \\mid X)\\). In the SLR/MLR model, we are setting our linear combination of terms equal to the mean of the logged-responses \\(\\mu(\\ln(y) \\mid X)\\). Interpretation of the Poisson log-linear model is easier because exponentiating the linear combination returns to us the the mean response. If we took the natural log of \\(x_1\\), we can say that a multiplicative change of \\(m\\) in \\(x_1\\) is associated with a \\(m^{\\beta_1}\\) multiplicative change in the mean response \\(\\mu\\): \\[ \\mu(Y \\mid mX) = e^{\\beta_0 + \\beta_1 \\ln(mx_{1}) + \\dotsm + \\beta_p x_{p}} = \\mu(Y \\mid X)m^{\\beta_1} \\] 5.3 EDA The Poisson model assumes that the quantitative predictors are linearly related to the log of the mean response. EDA to check this assume consists of a scatterplot of \\(\\ln(y)\\) vs. \\(x\\), looking for a transformation of \\(x\\), if needed, that displays a linear relationship. Constant variance is not assumed. 5.3.1 Example: Possums The data in possums was collected from a sample of \\(n = 151\\) 3-hectare sites in Australia. Our goal is to determine which factors are associated with good habitat for possums. The response recored for each site is y= the number of possum species found on the site. We will start by just considering the predictors Bark = bark quality index (low to high quality). &gt; possums &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/possums.csv&quot;) &gt; summary(possums$y) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 0.000 1.000 1.477 2.000 5.000 &gt; summary(possums$Bark) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 6.000 8.000 8.914 10.000 30.000 At least 25% of sites had no possum species observed, and about 75% of sites had 2 or fewer species observed. The middle 50% of sites had bark indices between 6 and 10. To check if Bark needs to be transformed, we need look at log-y vs Bark. Since y has many 0 species counts we will need to add a small amount (.5) to the vector y before plotting. As bark index increases, there is a slightly non-linear increase in number of species observed for untransformed Bark. The plot of log(y) vs. log(bark) looks more linear. A small amount of jitter was added in both directions to avoid overplotting issues. &gt; plotA &lt;- ggplot(possums, aes(x=Bark, y = y + .5)) + + geom_jitter(width = .01, height=.01) + geom_smooth(se=FALSE) + + scale_y_log10() + + labs(title=&quot;Log-Number species vs. Bark index&quot;) &gt; &gt; plotB &lt;- ggplot(possums, aes(x=Bark, y = y + .5)) + + geom_jitter(width = .01, height=.01) + geom_smooth(se=FALSE) + + scale_y_log10() + scale_x_log10() + + labs(title=&quot;Log-number species vs. log-Bark index&quot;) &gt; &gt; &gt; grid.arrange(plotA, plotB, nrow=1) 5.4 Inference and estimation Estimation of Poisson model parameters \\(\\beta_0, \\dotsc, \\beta_p\\) is done using maximum likelihood estimation (again!). The likelihood function is the probability of the observed data, writen as a function of our unknown \\(\\beta\\)’s where here \\(\\mu(X_i) = e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}\\) \\[ L(\\beta) = \\prod_{i=1}^n \\dfrac{e^{\\mu(X_i)}\\mu(X_i)^{y_i}}{y_i!} \\] Like other GLMs, these MLE estimates of \\(\\beta\\) parameters are approximately normally distributed and unbiased when n is “large enough” or when \\(\\mu(X_i)\\)’s are “large enough.” 5.4.1 Confidence intervals for \\(\\pmb{\\beta_i}\\) A \\(C\\)% confidence interval for \\(\\beta_i\\) equals \\[ \\hat{\\beta}_i \\pm z^*SE(\\hat{\\beta}_i) \\] where \\(z^*\\) is the \\((100-C)/2\\) percentile from the \\(N(0,1)\\) distribution. To get the multiplicative change in the mean response, we just exponentiate the CI: \\[ e^{\\hat{\\beta}_i \\pm z^*SE(\\hat{\\beta}_i)} \\] 5.4.2 Hypothesis tests for \\(\\pmb{\\beta_i}\\) We can test the hypothesis \\[ H_0: \\beta_i = \\beta^*_i \\] with the following z-test statistic: \\[ z =\\dfrac{\\hat{\\beta}_i - \\beta^*_i}{SE(\\hat{\\beta}_i)} \\] where \\(\\beta^*_i\\) is our hypothesized value of \\(\\beta_i\\) . The \\(N(0,1)\\) is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. The usual test results given by standard regression output tests whether a parameter value (intercept or slope) is equal to 0 vs. not equal to 0: \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with a test stat of \\[ z =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] 5.4.3 R glm We fit a Poisson regression model in R with the glm function. The basic syntax is glm(y ~ x1 + x2, family = poisson, data= ) Careful not to forget the family=poisson argument! If you omit this, you will just be trying to fit a regular MLR model which is not appropriate for a categorical response. Once you fit a glm model, you can extract attributes of the model fitted(my.glm) gives the estimated mean response \\(\\hat{\\mu}_i\\) for each case in your data predict(my.glm) gives estimated log-mean \\(\\hat{\\eta}_i\\) for each case in your data. Add newdata= to get predicted log-mean for new data. predict(my.glm, type = \"response\") gives estimated mean \\(\\hat{\\mu}_i\\) for each case in your data. Add newdata= to get predicted means for new data. The broom package also allows us to get fitted probabilities or log odds for all cases in the data, or for new data: augment(my.glm) gets estimated log-mean \\(\\hat{\\eta}_i\\) added to the variables used in the glm fit. add data=my.data to get estimated log-means added to the full data set my.data used in the glm fit add newdata= new.data to get predicted log-means added to the new data set new.data augment(my.glm, type.predict= \"response\") gets estimated mean \\(\\hat{\\mu}_i\\) added to the variables used in the glm fit. add data=my.data to get estimated means added to the full data set my.data used in the glm fit add newdata= new.data to get predicted means added to the new data set new.data 5.4.4 Example: Possums Our EDA above used log(y) just to explore whether we needed to transform our predictors bark. (Since log of our response should be linearly related to our predictor in Poisson regression.) But our Poisson model is just fit with y, not log(y) since our Poisson model already assumes log-linearity. We will fit the following Poisson regression of number of species on log-bark index: \\[ \\ln(\\mu(Y \\mid x)) = \\beta_0 + \\beta_1 \\ln(Bark) \\] The model fit is below. &gt; pos.glm &lt;- glm(y ~ log(Bark), family=poisson, data=possums) &gt; summary(pos.glm) ## ## Call: ## glm(formula = y ~ log(Bark), family = poisson, data = possums) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.18523 -1.26246 -0.07764 0.55078 2.11368 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8801 0.3027 -2.907 0.00365 ** ## log(Bark) 0.5945 0.1335 4.453 8.45e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 187.49 on 150 degrees of freedom ## Residual deviance: 167.51 on 149 degrees of freedom ## AIC: 452.31 ## ## Number of Fisher Scoring iterations: 5 The estimated log mean function is \\[ \\ln(\\hat{\\mu}(y \\mid x)) = -0.8801 + 0.5945 \\ln(Bark) \\] and the estimated mean function is \\[ \\hat{\\mu}(Y \\mid x) = e^{-0.8801 + 0.5945 \\ln(Bark)} = e^{-0.8801}Bark^{0.5945} \\] The estimated mean number of possums per 3-hectare when Bark quality is 10 is 1.63 species: \\[ \\hat{\\mu}(Y \\mid x=10) = e^{-0.8801}10^{0.5945} =1.6302 \\] &gt; predict(pos.glm, newdata=list(Bark=10)) # log mean ## 1 ## 0.4887215 &gt; exp(predict(pos.glm, newdata=list(Bark=10))) # mean ## 1 ## 1.630231 &gt; predict(pos.glm, newdata=list(Bark=10), type=&quot;response&quot;) # mean ## 1 ## 1.630231 With \\(n=151\\), we have a large enough sample size to trust normal-inference methods for our MLE estimates. If \\(n\\) was smaller, you would want to check summaries of the estimated mean values. Here we see that they range from an estimated 0.4 species per plot to 3.1 species per plot. &gt; summary(fitted(pos.glm)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.4147 1.2033 1.4277 1.4768 1.6302 3.1325 Bark index has a statistically significant effect on the mean number of species (z=4.45, p &lt; 0.0001). Doubling bark index is associated with a 51% increase in the estimated mean number of species per 3-hectare plot (95% CI 26% to 81%). \\[ 2^{0.5945} = 1.5099492 \\ \\ \\ 2^{0.5945 + c(-1,1)*qnorm(.975)*0.1335} = 1.2594945, 1.8102076 \\] &gt; 2^(.5945) # double bark index ## [1] 1.509949 &gt; 100*(2^(.5945) - 1) ## [1] 50.99492 &gt; # 95% CI for beta1 &gt; 0.5945 + c(-1,1)*qnorm(.975)*0.1335 ## [1] 0.3328448 0.8561552 &gt; # 95% CI for factor change in mu (doubling bark) &gt; 2^(0.5945 + c(-1,1)*qnorm(.975)*0.1335) ## [1] 1.259494 1.810208 &gt; 100*(2^(0.5945 + c(-1,1)*qnorm(.975)*0.1335) - 1) ## [1] 25.94945 81.02076 We can visualize the mean response \\(\\hat{\\mu}(Y \\mid x) = e^{-0.8801 + 0.5945 \\ln(Bark)}\\) on a plot log-bark and number of species by first plotting (un-logged) y against the log of Bark. Then we add the geom_smooth with glm smoother method and a poisson family listed as the argument. Here we also include a smaller amount of jitter to avoid overplotting. $as a function of Bark index &gt; ggplot(possums, aes(x=Bark, y=y)) + + geom_jitter(width = .01, height=.1) + scale_x_log10() + + geom_smooth(method=&quot;glm&quot;, method.args = list(family=poisson), se=FALSE) + + labs(title=&quot;Poisson regression of # species on log-bark index&quot;, + y = &quot;# of species&quot;, x=&quot;bark index&quot;) 5.5 Deviance for Binomial responses In Poisson regression, the deviance is equal to \\[ G^2 = 2[\\ln L(\\bar{\\mu}) - \\ln L(\\hat{\\mu}(X))] = 2\\sum_{i=1}^n \\left[ y_i \\ln \\left( \\dfrac{y_i}{\\hat{\\mu}(X_i)} \\right) - (y_i- \\hat{\\mu}(X_i)) \\right] \\] Note that cases with large \\(y_i\\) can contribute the most to this summation of individual case-level deviance. Again, the two likelihoods that are used to construct residual deviance are: \\(L(\\hat{\\mu}(X)):\\) likelihood of the data that plugs in estimates \\(\\hat{\\mu}(X_i)\\) from the Poisson model. \\(L(\\bar{\\mu}):\\) likelihood of the data that plugs in estimates \\(\\bar{\\mu} = y_i\\), basing a case’s “predicted” value soley on the response observed for that case. This again is called a saturated model and it will always have a higher likelihood than the logistic model: \\(L(\\bar{\\mu}) \\geq L(\\hat{\\mu}(X))\\). Deviance for binomial models can be used for two types of hypothesis tests: 5.5.1 Drop-in-deviance test This test is used to compare two models, just like in logistic models. Details are in Section 4.5. One difference between the logistic model and Poisson is the “large sample size” condition needed to trust the drop-in-deviance p-value approximation. In Poisson models, we need either a large sample size \\(n\\) or \\(\\hat{\\mu}\\)’s that are large. Two compare two Poisson models using deviance, run anova(pois.red, pois.full, test = &quot;Chisq&quot;) 5.6 Checking Assumptions Assumptions in a Poisson model include Cases are independent: Independence of cases takes an understanding of how the data was collected. Log-mean linearity: Log-mean linearity can be checked with an a plot of log-response against quantitative predictors and residual plots. A third assumption is that given predictor values, the counts of events \\(Y_i\\) has a Poisson distribution. This means that for each case \\(i\\) we assume the following: The events occur independently, and there is no clustering of events across time or space. Clustering of event occurrences induces more variation in our responses than our Poisson model assumes. For example, if one rush hour accident makes more accidents more likely in that time period, then we could see big swings in monthly accident counts. Some months with no rush hour accidents will see low counts while months with a rush hour accident could see a big uptick in total accident counts. This idea is demonstrated in the code below with the y.clus variable (10 accidents per month in winter months and 0 in other months) which has the same mean as a Poisson vector of accident counts, but which has a much larger measure of SD. &gt; # monthly accident counts from a Poisson with mean around 3: &gt; y.pois &lt;- c(5, 4, 1, 5, 3, 2, 6, 1, 4, 5, 0, 4) &gt; mean(y.pois) ## [1] 3.333333 &gt; sd(y.pois) ## [1] 1.922751 &gt; # monthly accident counts clustered in winter with mean around 3: &gt; y.clus &lt;- c(10,10,10,0,0,0,0,0,0,0,0,10) &gt; mean(y.clus) # same average ## [1] 3.333333 &gt; sd(y.clus) # much bigger variance than poisson counts! ## [1] 4.92366 We have enough explanatory power in our predictors to adequately model the mean response. If we lack this, then we can’t adequately explain variations in our observed counts and we our residual deviance will be very large. If one, or both, of these assumptions is violated, then it often induces extra-Poisson variation (a.k.a. overdispersion). This means that the actual variation in our response \\(SD(Y\\mid X_i)\\) is larger than the Poisson SD of \\(\\sqrt{\\mu(X_i)}\\) that our model assumes. So the response variation is more dispersed than what our model assumes, making our reported standard errors and p-values too small and we could be over-reporting statistical significance. We can use another goodness-of-fit test, when \\(\\mu_i\\) are large enough, to check our Poisson distribution assumption. If we do find evidence of lack-of-fit in our model, then you should Check deviance residuals as case influence stats to see if an outlier(s) is affecting GOF results. Check the log mean form and see if transformations of quantitative predictors are needed If outliers and transformations aren’t a concern, then consider an alternative model: quasi-Poisson logistic model a model that allows for correlated trials (like a mixed-effects Poisson model) 5.6.1 Goodness-of-fit test In our Poisson model, we are assuming that an observed count for case \\(i\\) behaves like a Poisson random variable. We compare our Poisson model’s estimated mean counts for each case to the observed counts from the saturated model described above. The saturated model will have the best “fit” (highest likelihood) of the two models, but if the “fit” (likelihood) of the logistic model is “close” then we can claim that the logistic model is adequate. This is the motivation behind a “goodness-of-fit” test. Our hypothese for the GOF test are: \\[ H_0: \\textrm{Poisson model} \\] vs. \\[ H_A: \\textrm{saturated model} \\] The test statistic for this is equal to the residual deviance of the Poisson model (difference in the \\(H_A\\) and \\(H_0\\) likelihoods): \\[ G^2 = 2[\\ln L(\\bar{\\mu}) - \\ln L(\\hat{\\mu}(X))] \\] If \\(H_0\\) is true and the data does fit the Poisson model, then when estimated \\(\\hat{\\mu}_i\\)’s are large, \\(G^2\\) will have an approximate chi-square distribution with \\(n - (p+1)\\) (model) degrees of freedom. (Note that \\(n-(p+1)\\) is the difference in the number of parameters in the two models.) The p-value is the probability of observing deviance larger than our model’s value: \\[ p-value = 1-P(\\chi^2 &gt; G^2) = 1-pchisq(G^2, df=n-(p+1)) \\] The suggested rule of thumb for “large \\(\\hat{\\mu}_i\\)”\" is that we want most \\(\\hat{\\mu}_i\\)’s to be at least 5. For a GOF test, you should consider the following interpretations for “large” or “small” p-values: Do not reject the null: (large p-value) Your Poisson model is adequate. You don’t have a large enough sample size \\(n\\) to have the power to detect inadequacies in your model. Reject the null: (small p-value) You have outlier(s) that are inflating the residual deviance. Your logistic model is inadequate. Your mean model is inadequate, it is ill-fitting and transformations are needed Extra-Poisson variation: your response counts aren’t well modeled by a Poisson model. For each case: events occurances are not independent, usually clustered in some way your choice of predictors isn’t sufficient (i.e. you are missing key explanatory variables) 5.6.2 GOF alternative If the GOF p-value approximation is suspect due to small estimated means, then one ad hoc method of assessing the “Poisson-ness” of the data would be to compare the sample mean and variances of cases with similar predictor values. Since a Poisson variable should have mean and variances that are similar in size, this can be used to detect if our counts are overdispersed. The basic algorithm is similar to that used in the BWCA empirical log-odds plot outlined in Section @ref(#example-bwca1). Group cases into groups with similar predictor values Within each group, compute the sample mean and sample variance of the observed counts y Plot means against variances, if they following the y=x line then the Poisson assumption looks adequate. 5.6.3 Example: Possums In Section 5.4.4 we tested the statistical significance of log-Bark with a Wald z-test. We could have instead used a drop in deviance test for this one term. Hypotheses are the same but the mechanics of each are slightly different. Either method is ok since n=151 is fairly large so their conclusions should be similar. Using anova for this one-term model will test the significance of the one term, log-Bark, against the “null” model with only the intercept term. Here our test statistic is \\(G^2 = 19.979\\) since this is the amount that the residual deviance decreases when adding log-Bark. The p-value, comptuted from a chi-square distribution with 1 degree of freedom, is extremely small indicating the strong significance of this term. &gt; anova(pos.glm, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 150 187.49 ## log(Bark) 1 19.979 149 167.51 7.828e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.6.3.1 Goodness-of-fit We might be tempted to take the deviance from our log-Bark model and run a GOF test to assess Poisson model adequacy: &gt; summary(pos.glm) ## ## Call: ## glm(formula = y ~ log(Bark), family = poisson, data = possums) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.18523 -1.26246 -0.07764 0.55078 2.11368 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.8801 0.3027 -2.907 0.00365 ** ## log(Bark) 0.5945 0.1335 4.453 8.45e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 187.49 on 150 degrees of freedom ## Residual deviance: 167.51 on 149 degrees of freedom ## AIC: 452.31 ## ## Number of Fisher Scoring iterations: 5 &gt; 1-pchisq(167.51, df=149) # p-value?? ## [1] 0.1425203 While the goodness of fit test gives a p-value of 0.1425, suggesting adequacy, we should be suspect because most estimated means are small and well below our “large \\(\\hat{\\mu}_i\\)” threshold of 5. &gt; summary(fitted(pos.glm)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.4147 1.2033 1.4277 1.4768 1.6302 3.1325 Our alternative method of checking whether our response is behaving like Poisson counts is the EDA check of sample means vs variances. We need to group cases by similar bark amounts. We can first note that the bark index is a rather discrete measure with many values (especially in mid-range values) &gt; table(possums$Bark) ## ## 1 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 23 24 27 30 ## 2 2 11 15 26 19 16 10 13 5 5 4 3 4 3 4 2 3 1 1 1 1 We could simply try grouping by each individual bark value and getting the mean and variance of the observed counts for each. This will result in some 1 case bark values having a variance of NA (since we can’t measure variability of one data value): &gt; pos.byBark &lt;- possums %&gt;% + group_by(Bark) %&gt;% + summarize(nY = n(), meanY = mean(y), varY = var(y)) &gt; pos.byBark %&gt;% print(n=Inf) ## # A tibble: 22 x 4 ## Bark nY meanY varY ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 0.5 0.5 ## 2 3 2 0 0 ## 3 4 11 1 0.6 ## 4 5 15 0.933 0.781 ## 5 6 26 1.08 1.11 ## 6 7 19 1.79 1.06 ## 7 8 16 1.12 0.917 ## 8 9 10 1.8 1.96 ## 9 10 13 1.69 2.23 ## 10 11 5 1.8 1.7 ## 11 12 5 2 2 ## 12 13 4 1 0.667 ## 13 14 3 1.33 2.33 ## 14 15 4 3 3.33 ## 15 16 3 2.33 2.33 ## 16 17 4 3.5 1.67 ## 17 18 2 2 2 ## 18 19 3 1.33 2.33 ## 19 23 1 2 NA ## 20 24 1 3 NA ## 21 27 1 1 NA ## 22 30 1 3 NA Then plot mean count vs the variance of count. The means and variances for each value of bark are similar, so we have no graphical evidence of lack of fit. &gt; ggplot(pos.byBark, aes(x=meanY, y=varY)) + + geom_point() + + geom_abline(intercept=0, slope=1, linetype=2) Alternatively, we could first group the cases according to the ntile function. Here we divide cases up into 12 similar groups based on bark, then get stats for each group and plot. A similar conclusion is made with this visual. No strong evidence of a trend in variance always being bigger (or smaller) than the mean count. &gt; library(tidyverse) &gt; pos.byBark2 &lt;- possums %&gt;% + mutate(Bark.grp = ntile(Bark, n=12)) %&gt;% + group_by(Bark.grp) %&gt;% + summarize(nY = n(), meanY = mean(y), varY = var(y)) &gt; pos.byBark2 %&gt;% print(n=Inf) ## # A tibble: 12 x 4 ## Bark.grp nY meanY varY ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 13 0.769 0.526 ## 2 2 13 0.923 0.577 ## 3 3 13 1.08 0.910 ## 4 4 13 1.08 1.41 ## 5 5 13 1.46 1.10 ## 6 6 13 1.77 1.53 ## 7 7 13 1.08 0.744 ## 8 8 12 1.67 1.88 ## 9 9 12 1.92 2.27 ## 10 10 12 1.67 1.33 ## 11 11 12 2.08 2.63 ## 12 12 12 2.42 2.08 &gt; ggplot(pos.byBark2, aes(x=meanY, y=varY)) + + geom_point() + + geom_abline(intercept=0, slope=1, linetype=2) 5.7 Residuals and case influence for binomial responses We have a few options for residuals for a binomial logistic model. The two most common residuals to consider are Pearson residuals and deviance residuals. Pearson residuals are basically response residuals standardized based on the binomial SD: \\[ pr_i = \\dfrac{ y_i - \\hat{\\mu}(X_i)}{\\sqrt{\\hat{\\mu}(X_i)}} \\] We can get these residuals by requesting the “pearson” type of residual: resid(my.glm, type = \"pearson\") augment(my.glm, type.residuals = \"pearson\") Deviance residuals are each case’s contribution to the residual deviance, with a \\(\\pm\\) based on whether we over- or under-estimate a case’s response (the \\(\\pm\\) is denoted by \\(sign(y_i - m_i\\hat{\\pi}(X_i))\\)): \\[ Dres_i = sign(y_i - \\hat{\\mu}(X_i)) \\sqrt{2 \\left[ y_i \\ln \\left( \\dfrac{y_i}{\\hat{\\mu}(X_i)} \\right) - (y_i- \\hat{\\mu}(X_i)) \\right] } \\] We can get these residuals by requesting the default residual values: resid(my.glm) augment(my.glm) As with model GLMs: Pearson residuals are “easy” to interpret as the number of estimated SD’s a response is from it’s estimated mean. Deviance residuals are good to check if you find significant results in a GOF test. When \\(\\hat{\\mu}(X_i)\\)’s are large (at least 5), both types of residuals should be similar in value and have a \\(N(0,1)\\) distribution (approximately). This means that most cases (~95%) should have residual values no more extreme than \\(\\pm 2\\). Regardless of size of \\(m_i\\), we should plot residuals vs. quantitative predictors to assess linearity of the log odds. Case influence stats of leverage and Cook’s distance can also be used to look for outliers. In a GLM, leverage measures both a cases’s “extremeness” in terms of it’s predictor values and it’s extremeness in terms of it’s weight. In a Poisson model, a case’s weight is \\(\\hat{\\mu}(X_i)\\). A case with higher values of \\(\\hat{\\mu}(X_i)\\) are given more weight, and hence higher leverage, in the fitted model (e.g. in the estiamtes of \\(\\hat{\\beta}\\)). The value of Cook’s distance also takes into account a cases leverage and a case’s residual value. You can get Pearson residuals, leverage and Cook’s distance with plot(my.glm, which = 5). You can also get leverage and Cook’s distance vs. predictor values with ggnostic(my.glm, columnsY = c(\".hat\",\".cooksd\")) from the GGally package. 5.7.1 Example: Possums The ggnostic plot options gives you deviance residuals against your predictor(s). &gt; library(GGally) &gt; ggnostic(pos.glm, columnsY = c(&quot;.resid&quot;)) Alternatively, you can plot indivdidually by hand: &gt; possums.aug &lt;- augment(pos.glm, data=possums) &gt; plotA &lt;- ggplot(possums.aug, aes(x=Bark, y=.resid)) + + geom_jitter(height = .01, width = .05) + + scale_x_log10() + + geom_hline(yintercept = 0) + labs(title=&quot;Deviance residuals&quot;) &gt; possums.aug &lt;- augment(pos.glm, data=possums, type.resid = &quot;pearson&quot;) &gt; plotB &lt;- ggplot(possums.aug, aes(x=Bark, y=.resid)) + + geom_jitter(height = .01, width = .05) + + scale_x_log10() + + geom_hline(yintercept = 0) + labs(title=&quot;Pearson residuals&quot;) &gt; grid.arrange(plotA, plotB, nrow=1) We can get Cook’s distance and leverage values from plot: &gt; plot(pos.glm, which=4, id.n = 10) &gt; plot(pos.glm, which=5, id.n = 10) or use ggnostic: &gt; ggnostic(pos.glm, columnsY=c(&quot;.hat&quot;, &quot;.cooksd&quot;)) Case 3 stands out with the highest Cook’s distance, likely because it has the second highest leverage (and Bark value) and it is overestimated by a decent amount. Because it has a higher estimated mean response (around 3) then all but one case in the data. Case 1 has highest leverage because it has the highest bark index and highest estimated mean. The two cases with the lowest bark index (101, 147) do not have the highest leverage values because their estimated means are the smallest in the data set. The other higher leverage cases have higher bark values and estimated means. The other higher Cook’s distance cases (75,86,114) have higher predicted values and are poorly predicted with actual counts of either 0 or 5. A quick check shows that case 3 is not all that influential in changing the effect of bark. &gt; possums.aug &lt;- augment(pos.glm, data=possums, type.predict=&quot;response&quot;) &gt; plotA &lt;- ggplot(possums.aug, aes(x=Bark, y=.hat, size=.fitted)) + + geom_point() + labs(title=&quot;Leverage vs Bark (size=mu)&quot;) &gt; plotB &lt;- ggplot(possums.aug, aes(x=Bark, y=.cooksd, size=.fitted)) + + geom_point() + labs(title=&quot;Cook&#39;s D vs Bark (size=mu)&quot;) &gt; grid.arrange(plotA, plotB, nrow=1) 5.8 Quasi-Poisson logistic model A quasi-Poisson model is similar in motivation and fit as a quasi-Binomial model from Section 4.13. A quasi-Poisson logistic regression model is one model option when your GOF test suggests that your Poisson model assumptions of independent event occurance and constant rate of occurances is not met. The idea behind this model is to estimate the amount of “overdispersion” and use this value to correct your Poisson model SEs. In a quasi-Poisson model we let \\(\\psi\\) measure overdispersion so that the responce variance equals: \\[ V_{quasi}(Y_i \\mid X_i)= \\psi \\mu(X_i) =\\psi V_{poisson}(Y_i \\mid X_i) \\] If \\(\\psi&gt;1\\), then our responses are more variable than Poisson responses should be and \\(V_{quasi}(Y_i \\mid X_i) &gt; V_{poisson}(Y_i \\mid X_i)\\). It is possible to have underdispersion, where \\(\\psi &lt; 1\\) in which case \\(V_{quasi}(Y_i \\mid X_i) &lt; V_{poisson}(Y_i \\mid X_i)\\). Steps to fitting a quasi-Poisson model: Fit the Poisson regression model, then estimate the dispersion parameter by comparing the model residual deviance to \\(n-(p+1)\\) which is it’s expected value if the model was adequate: \\[ \\hat{\\psi} = \\dfrac{G^2}{n-(p+1)} \\] Parameter estimates for \\(\\beta\\) are from the Poisson model. Standard errors for \\(\\hat{\\beta}\\)’s are expanded (if \\(\\hat{\\psi}&gt;1\\)) to account for overdispersion by a factor of \\(\\sqrt{\\hat{\\psi}}\\): \\[ SE_{quasi}(\\hat{\\beta}_i) = \\sqrt{\\hat{\\psi}}SE_{binom}(\\hat{\\beta}_i) \\] Conduct “z”-inference (Wald tests/CI) using SEs equal to \\(SE_{quasi}(\\hat{\\beta}_i)\\) Compare quasi-Poisson models using a F-test stat equal to \\[ F = \\dfrac{G^2/(\\textrm{# terms tested})}{\\hat{\\psi}} \\] using an F-distribution with degrees of freedom equal to the number of terms tested and \\(n-(p+1)\\). (The “usual” df for an ANOVA F test.) We can get inference results for steps 1-4 above by adding family = quasipoisson to our glm regression fit: glm(y ~ x1 + x2, family = quasipoisson, data=mydata) We can get compare quasi-Poisson models using deviance with the command anova(red.quasi, full.quasi, test = &quot;F&quot;) Again, the method of estimation dispersion as \\(G^2/(n-(p+1))\\) is one way to estimate this parameter by comparing the actual value of \\(G^2\\) to this expectation tells us our dispersion rate. Another way to compute this value is to look at the sum of the squared Pearson residuals, which should also have an expected value of \\(n-(p+1)\\). The ratio of these two quantities is actually what the glm function uses to compute the dispersion parameter. Often there is no pratical difference between these two ways of estimation the dispersion parameter. "],
["rrstudio.html", "A R and Rstudio A.1 Running Rstudio A.2 Installing R A.3 Installing Rstudio A.4 Installing R packages", " A R and Rstudio R is a free professional statistical software that is available for use on windows, mac and linux computers. R is a popular tool for researchers from many fields so acquiring basic R skills from our stats classes will be beneficial for this course and career plans! RStudio is a free software that provides a user-friendly interface with R. We will be running R through RStudio in our stats classes. R can be more challenging for a brand new user than other software (like Excel, SPSS, etc) because analyzes are done using written commands rather than using a drop down (point-and-click) menu. But R is very powerful because of the huge variety of statistical methods that it supports (due to the addition of free user contributed packages) and the user’s ability to customize their experience (graphics, new functions, data manipulation, etc). Because R is based on written commands that can be recorded in a variety of ways, it is easy for a user to reproduce, re-do or continue analyzes that were started at a different point in time. This is much harder to do when you are using a bunch of drop-down menu commands to run your analysis! We will emphasize reproducibility in this course by using R Markdown scripts (Section D) to complete our analyzes. A.1 Running Rstudio Browser: You can access an online version of Rstudio from maize.mathcs.carleton.edu. If you are new to using Rstudio, I encourage you to use maize rather than installing R/Rstudio on your computer. You can access maize from anywhere on campus using Eduroam wireless if on a laptop, but from off campus you will first need to turn on Carleton’s VPN prior to logging in. Please install this software! Why use maize? Any work you create (R scripts, Markdown, pdf, or word docs) are saved on your account located on this server (file path /Accounts/username). Maize also has most R packages that we use in our classes preinstalled so you don’t need to install them before using them. Getting files off of maize: If you want to download a file from this account, check the button next to the file in the Files pane (lower right panel). (Don’t use the “File” dropdown menu for this!) Then select the More &gt; Export drop down menu from this pane and click Download. The file should be located in the default download location for your browser. Getting files onto maize: I will give you many files (usually .Rmd) in this class that you will want to upload, or add, to your maize Rstudio account. To do this, download the file to your computer to a place you know about like Downloads or Desktop. Then in your Math245 project folder on maize, click the upload button from the Files pane (lower right panel). (Don’t use the “File” dropdown menu for this!) Click Choose file and navigate to the location of the file on your computer. Click Open and Ok to upload this file to your maize account. Personal computer: You can download R (Section A.2) and Rstudio (Section A.3) software onto your personal computer. You then open Rstudio to start an R session. You will need install certain R packages that we used in our class that aren’t part of the default package installation. See Section A.4. A.2 Installing R Follow the appropriate link below and complete the default installation. Windows: http://cran.r-project.org/bin/windows/base/ Mac: http://cran.r-project.org/bin/macosx/ A.3 Installing Rstudio Follow the link below and download the free RStudio Desktop Open Source Edition. Windows or Mac: http://www.rstudio.com/ide/download/ A.4 Installing R packages R packages provided added analysis tools that R users contribute to the R community. The default R installation only provides us with a fraction of the available packages. The most straightforward way to install a needed package in Rstudio is to click the Packages tab in the lower right pane. Click the Install button and start typing the package name into Packages field, then click Install to add the package to your available package library. You should only need to install a package once. Alternatively, you can install a package by typing an install.packages command into the Console window. For example, the following command installs the R data package Sleuth3 for The Statistical Sleuth textbook: &gt; install.packages(&quot;Sleuth3&quot;) "],
["renviron.html", "B The R enviroment B.1 Workspace B.2 Working directory B.3 Rstudio projects", " B The R enviroment B.1 Workspace Anything that you load or create in Rstudio (data, vectors, models) are called objects. You can see your current objects in your RStudio Environment pane (upper right). These objects, along with your command history, are contained in what R calls your “Workspace”. When you exit Rstudio, you may be asked if you want to save your workspace as a .RData file before exiting. I strongly encourage you not to do this since it can make it very hard to reproduce your workflow (and it can slow down Rstudio start up). Instead, use R markdown (Chapter D) to document your workflow so you can redo any analysis that you’ve previously done. To change Rstudio’s default startup and exit workspace behavior: From the Tools drop down menu (at the top), select Global Options… Under the General tab, uncheck the “Restore .RData into workspace at startup” In the “Save workspace to .RData on exit:” dropdown, select Never B.2 Working directory The default location that R looks for “stuff” or saves “stuff” is called your Working Directory. For example, the default location of this folder is typically your Documents folder for a Windows machine. You can run the getwd() command to see where your current working directory is located. For my desktop Windows computer, the default working directory location is &gt; getwd() [1] &quot;C:/Users/kstclair/Documents&quot; Or from the maize server, &gt; getwd() [1] &quot;/Accounts/kstclair&quot; B.3 Rstudio projects RStudio (not standalone R) has a feature called Projects that make it easy to start RStudio in a particular working directory. You can create different Projects for different classes (or research projects). Your first task in Rstudio for Math 245 will be to create a Math 245 project: Find the Project button in the upper right-hand corner of Rstudio. Select New Project. Click New Directory from the New Project dialog box. Click on New project (again), then enter Math245 as your Directory name (no spaces). Use the Browse button to put this project in a good spot on your computer or accept the default location on maize. Click on Create Project. Your Rstudio session should not change in looks all that much but check your computer should now contain a Math245 folder in the location you chose. This folder will contain a Math245.Rproj icon. Your working directory is now set to this Math245 folder. Check this with the getwd() command. Starting your Math245 project: Rstudio default settings are to start up your last project when you reopen Rstudio. So just opening Rstudio (or loggin onto maize) usually opens your Math245 project. You will see your project name in the upper right-hand project button location. Alternatively, start your R project by double clicking on the project icon in your Math245 folder. You will know you are in You can also open or change projects with the drop-down project menu. "],
["rreview.html", "C R for basic data analysis C.1 Basics C.2 Data C.3 EDA C.4 Factor variables", " C R for basic data analysis C.1 Basics C.1.1 Quick Tips In the Console window: You type commands after the prompt &gt; and hit Enter to execute the command. The results of the command will be displayed below the command and preceded by a line counter enclosed in brackets (e.g. [1]). You can scroll back to, edit, and execute previous commands using the up arrow on your keyboard. If you prematurely hit enter before a command is completed to R’s satisfaction you will see a + prompt which asks you to finish the command. Simply complete the command after the + and hit Enter. If you get stuck in a cycle of + prompts you can get back to a fresh command prompt &gt; by hitting the Esc key. For example, try: &gt; 1+3 &lt;ENTER&gt; [1] 4 &gt; 1+ + 3 &lt;ENTER&gt; [1] 4 &gt; 1+ + &lt;ESC&gt; User interrupt requested For information about a command type ?commandname. C.1.2 Objects Everything in R is an object. You can assign a name to an object by using the assignment operator &lt;-. You can see the current list of objects in your R workspace by typing ls() or by looking in the Environment tab in Rstudio. You can remove objects from your workspace with the command rm(name) where name is the name of the object you want to delete. C.1.3 Vectors Vectors are a simple type of object and even single numbers (called scalars) are vectors with a length of 1. Here are a few ways to create vectors. The : operator creates a sequence of numbers that increase or decrease by 1. &gt; 3:10 &gt; 0:-3 &gt; 3:10 ## [1] 3 4 5 6 7 8 9 10 &gt; 0:-3 ## [1] 0 -1 -2 -3 The seq function also creates sequences of a given length or increment. &gt; seq(1,3,by=.5) ## [1] 1.0 1.5 2.0 2.5 3.0 &gt; seq(1,3,length=10) ## [1] 1.000000 1.222222 1.444444 1.666667 1.888889 2.111111 2.333333 2.555556 ## [9] 2.777778 3.000000 Use the combine function c() to create any particular arrangement: &gt; c(5,2,4,-1) ## [1] 5 2 4 -1 &gt; x &lt;- c(5,2,4,-1) &gt; x ## [1] 5 2 4 -1 &gt; y &lt;- c(x,0:3,x) &gt; y ## [1] 5 2 4 -1 0 1 2 3 5 2 4 -1 The rep command allows repetition: &gt; rep(0,5) ## [1] 0 0 0 0 0 &gt; rep(c(0,1),c(3,2)) ## [1] 0 0 0 1 1 You can also create vectors of characters (letters or words): &gt; c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &gt; c(&quot;abc&quot;,&quot;de&quot;) ## [1] &quot;abc&quot; &quot;de&quot; Logical (true/false) vectors do not use double quotes: &gt; c(T,T,F) ## [1] TRUE TRUE FALSE C.1.4 Arithmetic Here’s some basic commands: &gt; x &lt;- 1:4 &gt; x*2 ## [1] 2 4 6 8 &gt; x^2 ## [1] 1 4 9 16 &gt; (x-2)/3 ## [1] -0.3333333 0.0000000 0.3333333 0.6666667 Arithmetic involving vectors is done elementwise. &gt; y &lt;- c(-1,1,-1,1) &gt; x*y ## [1] -1 2 -3 4 &gt; x-y ## [1] 2 1 4 3 Special functions are available that work elementwise or on the entire vector. Here are a few: &gt; sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 &gt; log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 &gt; exp(x) ## [1] 2.718282 7.389056 20.085537 54.598150 &gt; sum(x) ## [1] 10 &gt; mean(x) ## [1] 2.5 &gt; sd(x) ## [1] 1.290994 &gt; sqrt(sum(x^2)) ## [1] 5.477226 More summary stats can be found with the commands min, max, median, quantile, summary, and var. You usually use the special functions above with numeric vectors (int or num), but you can use these functions with logical vectors too. The logical vector is coerced into a integer vector with 1 for TRUE and 0 for FALSE. &gt; y &lt;- c(T,T,F) &gt; y ## [1] TRUE TRUE FALSE &gt; sum(y) # number of TRUE&#39;s in y ## [1] 2 &gt; mean(y) # proportion of TRUE&#39;s in y ## [1] 0.6666667 C.1.5 Subsetting You can access one or more elements in a vector by specifying a certain index of elements within the vector: vector[index]. &gt; w&lt;-c(30,50,20,60,40,20) &gt; length(w) ## [1] 6 The first element of w: &gt; w[1] ## [1] 30 The first and third elements: &gt; w[c(1,3)] ## [1] 30 20 All elements except the first and third: &gt; w[-c(1,3)] ## [1] 50 60 40 20 The elements that are at most 40: &gt; w[w &lt;= 40] ## [1] 30 20 40 20 The position of these elements that are at most 40: &gt; which(w &lt;= 40) ## [1] 1 3 5 6 The mean of w and the mean of only the elements in w that are less than or equal to 4: &gt; mean(w) ## [1] 36.66667 &gt; mean(w[w &lt;= 40]) ## [1] 27.5 Expressions involving inequalities create a logical vector which is TRUE when the expression is true: &gt; w &lt;= 40 ## [1] TRUE FALSE TRUE FALSE TRUE TRUE &gt; w == 40 ## [1] FALSE FALSE FALSE FALSE TRUE FALSE So when a vector is indexed by a TRUE/FALSE vector only the TRUE entries will be displayed (and used in any command involving this vector). Here is the logical vector for entries in w that are not equal to 40: &gt; w != 40 ## [1] TRUE TRUE TRUE TRUE FALSE TRUE Here are the values of the entries of w excluding those equal to 40: &gt; w[w != 40] ## [1] 30 50 20 60 20 Here is the sum of the values of the entries of w excluding those equal to 40: &gt; sum(w[w != 40]) ## [1] 180 Adding a logical (T/F) vector tells you how many elements in the vector are equal to TRUE. Here is the number of entries in w that are not equal to 40: &gt; sum(w != 40) ## [1] 5 Finally, the vector operators | and &amp; mean OR and AND, respectively. We can find the entries in w that are less than 30 OR greater than 50 with &gt; (w &lt; 30) | (w &gt; 50) ## [1] FALSE FALSE TRUE TRUE FALSE TRUE We can find the entries that are at most 50 AND at least 30 with &gt; (w &gt;= 30) &amp; (w &lt;= 50) ## [1] TRUE TRUE FALSE FALSE TRUE FALSE C.2 Data C.2.1 Reading Data into R The most common way to read data into R is by storing it in a comma separated values (.csv) format. Non-textbook data files for this class will either be on my webpage http://people.carleton.edu/~kstclair/data. You can read a .csv file into R using its URL or file path (which is system dependent): &gt; mydata &lt;- read.csv(&quot;&lt;data file path&gt;/mydata.csv&quot;) Alternatively, you can download (then upload if using maize) a needed data set to your data folder located in your Mathxxx folder. Once this is done, and your Mathxxx project is started, you can easily read the data set into R using the command &gt; mydata &lt;- read.csv(&quot;data/mydata.csv&quot;) You don’t need an extended file path name because your project should set your working directory to your Mathxxx folder and the data folder containing you .csv is a subfolder in this working directory. Many textbooks have R packages that contain data sets used in the book. Here I’ll use the SDaA packge used in my Math 255 (Sampling) course. Once you load this library you have automatic access to add textbook data files identified by the name given in the book. &gt; # install.packages(&quot;SDaA&quot;) # only run this once, ever &gt; library(SDaA) &gt; class(agstrat) ## [1] &quot;data.frame&quot; The object agstrat is called a data frame. The rest of this handout will explain how R data frames can be explored, used and changed. C.2.2 Investigating a Data Frame You can see an entire data frame by typing its name. You can see the first or last 5 rows of a data frame with the following commands: &gt; head(agstrat) ## county state acres92 acres87 acres82 farms92 farms87 farms82 ## 1 PIERCE COUNTY NE 297326 332862 319619 725 857 865 ## 2 JENNINGS COUNTY IN 124694 131481 139111 658 671 751 ## 3 WAYNE COUNTY OH 246938 263457 268434 1582 1734 1866 ## 4 VAN BUREN COUNTY MI 206781 190251 197055 1164 1278 1464 ## 5 OZAUKEE COUNTY WI 78772 85201 89331 448 483 527 ## 6 CLEARWATER COUNTY MN 210897 229537 213105 583 699 693 ## largef92 largef87 largef82 smallf92 smallf87 smallf82 region rn weight ## 1 54 54 42 58 67 48 NC 805 10.23301 ## 2 14 13 14 42 36 38 NC 241 10.23301 ## 3 20 19 16 175 186 184 NC 913 10.23301 ## 4 23 17 9 56 66 55 NC 478 10.23301 ## 5 6 5 5 56 49 48 NC 1028 10.23301 ## 6 34 32 23 8 19 13 NC 496 10.23301 &gt; tail(agstrat) ## county state acres92 acres87 acres82 farms92 farms87 farms82 ## 295 FRANKLIN COUNTY WA 670149 660813 632519 857 894 856 ## 296 LEA COUNTY NM 2149450 2220431 2178568 544 561 534 ## 297 THURSTON COUNTY WA 59890 56799 67628 811 806 856 ## 298 CARSON CITY (IC) NV 5361 17859 18780 28 37 34 ## 299 BANNOCK COUNTY ID 325338 358189 352306 588 655 617 ## 300 LA PLATA COUNTY CO 587339 613579 589167 709 682 625 ## largef92 largef87 largef82 smallf92 smallf87 smallf82 region rn weight ## 295 127 140 120 107 109 101 W 371 10.29268 ## 296 208 205 191 59 67 63 W 259 10.29268 ## 297 5 4 4 171 143 151 W 394 10.29268 ## 298 3 2 2 15 15 17 W 295 10.29268 ## 299 79 81 83 98 112 106 W 148 10.29268 ## 300 67 79 66 25 39 33 W 112 10.29268 You can get the dimensions (# rows by # columns) and variable names is a data frame with &gt; dim(agstrat) ## [1] 300 17 You can see the variable names with &gt; names(agstrat) ## [1] &quot;county&quot; &quot;state&quot; &quot;acres92&quot; &quot;acres87&quot; &quot;acres82&quot; &quot;farms92&quot; ## [7] &quot;farms87&quot; &quot;farms82&quot; &quot;largef92&quot; &quot;largef87&quot; &quot;largef82&quot; &quot;smallf92&quot; ## [13] &quot;smallf87&quot; &quot;smallf82&quot; &quot;region&quot; &quot;rn&quot; &quot;weight&quot; or variable names and types with the structure command &gt; str(agstrat) ## &#39;data.frame&#39;: 300 obs. of 17 variables: ## $ county : chr &quot;PIERCE COUNTY&quot; &quot;JENNINGS COUNTY&quot; &quot;WAYNE COUNTY&quot; &quot;VAN BUREN COUNTY&quot; ... ## $ state : chr &quot;NE&quot; &quot;IN&quot; &quot;OH&quot; &quot;MI&quot; ... ## $ acres92 : int 297326 124694 246938 206781 78772 210897 507101 332358 402202 535359 ... ## $ acres87 : int 332862 131481 263457 190251 85201 229537 552844 337990 396638 503582 ... ## $ acres82 : int 319619 139111 268434 197055 89331 213105 541015 355823 400466 513458 ... ## $ farms92 : int 725 658 1582 1164 448 583 321 986 1249 488 ... ## $ farms87 : int 857 671 1734 1278 483 699 371 1065 1251 518 ... ## $ farms82 : int 865 751 1866 1464 527 693 341 1208 1320 571 ... ## $ largef92: int 54 14 20 23 6 34 163 56 86 216 ... ## $ largef87: int 54 13 19 17 5 32 180 36 78 204 ... ## $ largef82: int 42 14 16 9 5 23 176 42 69 193 ... ## $ smallf92: int 58 42 175 56 56 8 10 90 42 16 ... ## $ smallf87: int 67 36 186 66 49 19 24 115 38 37 ... ## $ smallf82: int 48 38 184 55 48 13 16 132 28 24 ... ## $ region : chr &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; ... ## $ rn : int 805 241 913 478 1028 496 969 42 676 383 ... ## $ weight : num 10.2 10.2 10.2 10.2 10.2 ... You can view the data frame in Rstudio’s viewer window with &gt; View(agstrat) C.2.3 Accessing Data You can also access and edit information in a data frame by subscripting the data frame. Suppose you want to look at the variable farms92 (the number of farms per county in 1992). This variable is the 6th column in the data frame. You can access its contents with either command: &gt; agstrat[,6] ## [1] 725 658 1582 1164 448 583 321 986 1249 488 1308 657 974 780 293 ## [16] 201 362 309 500 530 491 305 1383 740 325 783 440 832 682 198 ## [31] 283 1000 547 953 771 979 427 963 545 942 949 544 822 955 1421 ## [46] 532 272 1669 308 401 171 480 1433 900 378 58 760 1216 1086 833 ## [61] 682 1280 1262 1029 1190 618 554 497 744 698 786 1305 1058 855 741 ## [76] 828 1140 509 759 1080 658 663 1447 1398 511 1529 623 742 386 468 ## [91] 738 759 746 680 792 327 777 205 1190 629 426 395 721 1367 659 ## [106] 249 550 440 438 74 668 147 488 1367 395 940 602 716 0 433 ## [121] 451 142 537 427 689 179 14 547 872 1444 549 345 235 406 40 ## [136] 705 169 394 219 774 74 561 290 414 781 1037 992 342 179 2760 ## [151] 56 315 49 1226 389 226 334 303 1152 403 2086 946 1342 612 407 ## [166] 986 199 17 704 1120 127 261 642 348 1360 297 404 114 1582 328 ## [181] 404 34 330 132 151 966 146 374 694 455 838 915 812 732 540 ## [196] 108 419 1609 417 560 1903 1956 270 433 617 910 298 288 456 199 ## [211] 507 772 476 113 440 110 1004 199 339 818 689 491 659 215 301 ## [226] 606 355 376 695 889 1234 532 195 711 515 1547 90 651 361 147 ## [241] 747 33 128 137 940 477 445 278 447 162 280 640 1579 29 1031 ## [256] 1006 320 849 1232 267 1441 23 850 612 733 451 179 641 233 661 ## [271] 495 195 508 3157 442 358 107 149 1696 1027 415 490 418 134 257 ## [286] 525 599 366 16 874 419 1054 1257 110 857 544 811 28 588 709 &gt; agstrat$farms92 ## [1] 725 658 1582 1164 448 583 321 986 1249 488 1308 657 974 780 293 ## [16] 201 362 309 500 530 491 305 1383 740 325 783 440 832 682 198 ## [31] 283 1000 547 953 771 979 427 963 545 942 949 544 822 955 1421 ## [46] 532 272 1669 308 401 171 480 1433 900 378 58 760 1216 1086 833 ## [61] 682 1280 1262 1029 1190 618 554 497 744 698 786 1305 1058 855 741 ## [76] 828 1140 509 759 1080 658 663 1447 1398 511 1529 623 742 386 468 ## [91] 738 759 746 680 792 327 777 205 1190 629 426 395 721 1367 659 ## [106] 249 550 440 438 74 668 147 488 1367 395 940 602 716 0 433 ## [121] 451 142 537 427 689 179 14 547 872 1444 549 345 235 406 40 ## [136] 705 169 394 219 774 74 561 290 414 781 1037 992 342 179 2760 ## [151] 56 315 49 1226 389 226 334 303 1152 403 2086 946 1342 612 407 ## [166] 986 199 17 704 1120 127 261 642 348 1360 297 404 114 1582 328 ## [181] 404 34 330 132 151 966 146 374 694 455 838 915 812 732 540 ## [196] 108 419 1609 417 560 1903 1956 270 433 617 910 298 288 456 199 ## [211] 507 772 476 113 440 110 1004 199 339 818 689 491 659 215 301 ## [226] 606 355 376 695 889 1234 532 195 711 515 1547 90 651 361 147 ## [241] 747 33 128 137 940 477 445 278 447 162 280 640 1579 29 1031 ## [256] 1006 320 849 1232 267 1441 23 850 612 733 451 179 641 233 661 ## [271] 495 195 508 3157 442 358 107 149 1696 1027 415 490 418 134 257 ## [286] 525 599 366 16 874 419 1054 1257 110 857 544 811 28 588 709 &gt; agstrat[,&quot;farms92&quot;] ## [1] 725 658 1582 1164 448 583 321 986 1249 488 1308 657 974 780 293 ## [16] 201 362 309 500 530 491 305 1383 740 325 783 440 832 682 198 ## [31] 283 1000 547 953 771 979 427 963 545 942 949 544 822 955 1421 ## [46] 532 272 1669 308 401 171 480 1433 900 378 58 760 1216 1086 833 ## [61] 682 1280 1262 1029 1190 618 554 497 744 698 786 1305 1058 855 741 ## [76] 828 1140 509 759 1080 658 663 1447 1398 511 1529 623 742 386 468 ## [91] 738 759 746 680 792 327 777 205 1190 629 426 395 721 1367 659 ## [106] 249 550 440 438 74 668 147 488 1367 395 940 602 716 0 433 ## [121] 451 142 537 427 689 179 14 547 872 1444 549 345 235 406 40 ## [136] 705 169 394 219 774 74 561 290 414 781 1037 992 342 179 2760 ## [151] 56 315 49 1226 389 226 334 303 1152 403 2086 946 1342 612 407 ## [166] 986 199 17 704 1120 127 261 642 348 1360 297 404 114 1582 328 ## [181] 404 34 330 132 151 966 146 374 694 455 838 915 812 732 540 ## [196] 108 419 1609 417 560 1903 1956 270 433 617 910 298 288 456 199 ## [211] 507 772 476 113 440 110 1004 199 339 818 689 491 659 215 301 ## [226] 606 355 376 695 889 1234 532 195 711 515 1547 90 651 361 147 ## [241] 747 33 128 137 940 477 445 278 447 162 280 640 1579 29 1031 ## [256] 1006 320 849 1232 267 1441 23 850 612 733 451 179 641 233 661 ## [271] 495 195 508 3157 442 358 107 149 1696 1027 415 490 418 134 257 ## [286] 525 599 366 16 874 419 1054 1257 110 857 544 811 28 588 709 If you just want the first two entries in farms92: &gt; agstrat[1:2,6] ## [1] 725 658 &gt; agstrat$farms92[1:2] ## [1] 725 658 The variable region is a categorical variable, or factor variable to R. We can see the levels of region with &gt; str(agstrat$region) ## chr [1:300] &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; ... &gt; levels(agstrat$region) ## NULL So region has 4 levels called NC, NE, S, and W. Note that these levels are ordered alphabetically, which is typically done with factor variables from data sets that are read into R. C.2.4 Subsetting a Data Frame You can subset a data frame just as you can subset a vector (see the Basics handout). We might want to subset a data frame to extract certain columns (variables), or we may want to extract certain rows (observations), or some combination of both. Suppose you want a data frame that only contains the variables region and farms92: One way to do this is with the select command from the dplyr package: &gt; library(dplyr) &gt; agstrat2 &lt;- select(agstrat, region, farms92) &gt; str(agstrat2) ## &#39;data.frame&#39;: 300 obs. of 2 variables: ## $ region : chr &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; ... ## $ farms92: int 725 658 1582 1164 448 583 321 986 1249 488 ... Suppose you want a data frame that only contains data from the north central (NC) and west (W) regions. Here we use the dplyr command filter to specify the criteria that tells us that region should be either \"W\" or \"NC\": &gt; agstrat3 &lt;- filter(agstrat, region %in% c(&quot;W&quot;, &quot;NC&quot;)) &gt; str(agstrat3) ## &#39;data.frame&#39;: 144 obs. of 17 variables: ## $ county : chr &quot;PIERCE COUNTY&quot; &quot;JENNINGS COUNTY&quot; &quot;WAYNE COUNTY&quot; &quot;VAN BUREN COUNTY&quot; ... ## $ state : chr &quot;NE&quot; &quot;IN&quot; &quot;OH&quot; &quot;MI&quot; ... ## $ acres92 : int 297326 124694 246938 206781 78772 210897 507101 332358 402202 535359 ... ## $ acres87 : int 332862 131481 263457 190251 85201 229537 552844 337990 396638 503582 ... ## $ acres82 : int 319619 139111 268434 197055 89331 213105 541015 355823 400466 513458 ... ## $ farms92 : int 725 658 1582 1164 448 583 321 986 1249 488 ... ## $ farms87 : int 857 671 1734 1278 483 699 371 1065 1251 518 ... ## $ farms82 : int 865 751 1866 1464 527 693 341 1208 1320 571 ... ## $ largef92: int 54 14 20 23 6 34 163 56 86 216 ... ## $ largef87: int 54 13 19 17 5 32 180 36 78 204 ... ## $ largef82: int 42 14 16 9 5 23 176 42 69 193 ... ## $ smallf92: int 58 42 175 56 56 8 10 90 42 16 ... ## $ smallf87: int 67 36 186 66 49 19 24 115 38 37 ... ## $ smallf82: int 48 38 184 55 48 13 16 132 28 24 ... ## $ region : chr &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; &quot;NC&quot; ... ## $ rn : int 805 241 913 478 1028 496 969 42 676 383 ... ## $ weight : num 10.2 10.2 10.2 10.2 10.2 ... Note one problem with this new data frame: the region variable still thinks it has 4 levels even though S and NE are not in this data frame &gt; levels(agstrat3$region) ## NULL &gt; table(agstrat3$region) ## ## NC W ## 103 41 This could create a problem when we want to use the region variable in future analyzes. An easy solution exists using the droplevels command on the data frame &gt; agstrat3 &lt;- droplevels(agstrat3) &gt; table(agstrat3$region) ## ## NC W ## 103 41 C.2.5 Creating a data frame One way to create a data frame is to create vectors that will form the variables (columns), then binding them together in a data frame: &gt; x &lt;- 1:10 &gt; y &lt;- rep(c(&quot;a&quot;,&quot;b&quot;),c(5,5)) &gt; my.data &lt;- data.frame(x=x,y=y) &gt; str(my.data) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ x: int 1 2 3 4 5 6 7 8 9 10 ## $ y: chr &quot;a&quot; &quot;a&quot; &quot;a&quot; &quot;a&quot; ... &gt; my.data ## x y ## 1 1 a ## 2 2 a ## 3 3 a ## 4 4 a ## 5 5 a ## 6 6 b ## 7 7 b ## 8 8 b ## 9 9 b ## 10 10 b C.2.6 Adding a new column to a data frame Suppose you want to add a variable called w to the data frame my.data. &gt; w &lt;- rnorm(10, mean=0, sd=1) &gt; my.data &lt;- data.frame(my.data,w=w) &gt; my.data ## x y w ## 1 1 a 0.4225894 ## 2 2 a -1.4431160 ## 3 3 a 0.7034792 ## 4 4 a -1.3228236 ## 5 5 a -0.7072092 ## 6 6 b 0.4225562 ## 7 7 b -0.5278992 ## 8 8 b 0.4161620 ## 9 9 b -0.2602577 ## 10 10 b -0.3578865 C.2.7 Missing Data The missing data value in R is NA. Any blank field (or NA field) in the .csv file will be recognized as a missing value when the data is read into R with the read.csv command. But suppose we have missing data in a data set we’ve entered by hand &gt; u &lt;- c(NA,2,3,4,5,NA,7,8,9,10) &gt; v &lt;- c(rep(NA,5), 1:5) &gt; my.data &lt;- data.frame(my.data,u=u, v=v) &gt; my.data ## x y w u v ## 1 1 a 0.4225894 NA NA ## 2 2 a -1.4431160 2 NA ## 3 3 a 0.7034792 3 NA ## 4 4 a -1.3228236 4 NA ## 5 5 a -0.7072092 5 NA ## 6 6 b 0.4225562 NA 1 ## 7 7 b -0.5278992 7 2 ## 8 8 b 0.4161620 8 3 ## 9 9 b -0.2602577 9 4 ## 10 10 b -0.3578865 10 5 We can see which entries in u are missing with the is.na command &gt; is.na(my.data$u) ## [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE or with the summary command: &gt; summary(my.data) ## x y w u ## Min. : 1.00 Length:10 Min. :-1.4431 Min. : 2.00 ## 1st Qu.: 3.25 Class :character 1st Qu.:-0.6624 1st Qu.: 3.75 ## Median : 5.50 Mode :character Median :-0.3091 Median : 6.00 ## Mean : 5.50 Mean :-0.2654 Mean : 6.00 ## 3rd Qu.: 7.75 3rd Qu.: 0.4210 3rd Qu.: 8.25 ## Max. :10.00 Max. : 0.7035 Max. :10.00 ## NA&#39;s :2 ## v ## Min. :1 ## 1st Qu.:2 ## Median :3 ## Mean :3 ## 3rd Qu.:4 ## Max. :5 ## NA&#39;s :5 We can use the drop_na command from the tidyr package to create an NA-free version of our data frame. Applying it to the entire data frame returns only rows that have observations for all variables: &gt; library(tidyr) &gt; my.data.noNA &lt;- drop_na(my.data) &gt; my.data.noNA ## x y w u v ## 1 7 b -0.5278992 7 2 ## 2 8 b 0.4161620 8 3 ## 3 9 b -0.2602577 9 4 ## 4 10 b -0.3578865 10 5 There are times when you only want to remove NA’s for a limited number of variables. Add these variable names as arguments to the drop_na command to only remove rows with NA’s for those variables. Here we only remove NAs from u (rows 1 and 6): &gt; my.data.noNAu &lt;- drop_na(my.data, u) &gt; my.data.noNAu ## x y w u v ## 1 2 a -1.4431160 2 NA ## 2 3 a 0.7034792 3 NA ## 3 4 a -1.3228236 4 NA ## 4 5 a -0.7072092 5 NA ## 5 7 b -0.5278992 7 2 ## 6 8 b 0.4161620 8 3 ## 7 9 b -0.2602577 9 4 ## 8 10 b -0.3578865 10 5 Sometimes data sets (especially “read-world”\" data) do not use blank fields to indicate missing data. For example, perhaps an unrealistic value is given as filler for a missing data point, like -99 for a positive integer variable or 9999 for a smaller scale variable. The source where you find your data should tell you if special fields (like -99 or 9999) are used to indicate missing data. Once you determine what the missing data indicator is, you can import the data set using the read.csv command with the added argument na.strings = c(\"-99\",\" \"). This argument tells R that missing data is coded either as an NA, a blank entry or as a -99 entry. &gt; mydata &lt;- read.csv(&quot;&lt;file path&gt;&quot;, na.strings = c(&quot;NA&quot;, &quot; &quot;, &quot;-99&quot;)) C.3 EDA We are using the agstrat data frame from the SDaA package (see Section 1.5.2.1). C.3.1 Categorical: The table command is useful when summarizing a categorical variable like region &gt; table(agstrat$region) ## ## NC NE S W ## 103 21 135 41 There are 103 counties in the north central region, 21 in the northeast, 135 in the south, and 41 in the west. We get a contingency table by entering two categorical variables &gt; table(agstrat$state,agstrat$region) ## ## NC NE S W ## AL 0 0 5 0 ## AR 0 0 9 0 ## AZ 0 0 0 1 ## CA 0 0 0 1 ## CO 0 0 0 5 ## CT 0 1 0 0 ## FL 0 0 4 0 ## GA 0 0 15 0 ## HI 0 0 0 2 ## IA 10 0 0 0 ## ID 0 0 0 5 ## IL 16 0 0 0 ## IN 9 0 0 0 ## KS 11 0 0 0 ## KY 0 0 15 0 ## LA 0 0 3 0 ## MA 0 1 0 0 ## MD 0 0 2 0 ## ME 0 1 0 0 ## MI 6 0 0 0 ## MN 9 0 0 0 ## MO 10 0 0 0 ## MS 0 0 6 0 ## MT 0 0 0 7 ## NC 0 0 16 0 ## ND 2 0 0 0 ## NE 12 0 0 0 ## NJ 0 1 0 0 ## NM 0 0 0 2 ## NV 0 0 0 2 ## NY 0 8 0 0 ## OH 4 0 0 0 ## OK 0 0 7 0 ## OR 0 0 0 4 ## PA 0 8 0 0 ## SC 0 0 4 0 ## SD 7 0 0 0 ## TN 0 0 6 0 ## TX 0 0 31 0 ## UT 0 0 0 4 ## VA 0 0 5 0 ## VT 0 1 0 0 ## WA 0 0 0 7 ## WI 7 0 0 0 ## WV 0 0 7 0 ## WY 0 0 0 1 So, for example, the data contains 5 counties in Alabama that are classified as southern. (Note that this isn’t a very interesting summary of two categorical variables, just an easy one to demonstrate the table command using this data set.) C.3.2 Quantitative: Basic summary stats commands are &gt; summary(agstrat$farms92) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 326.5 544.5 637.4 840.8 3157.0 &gt; mean(agstrat$farms92) ## [1] 637.3833 &gt; median(agstrat$farms92) ## [1] 544.5 &gt; sd(agstrat$farms92) ## [1] 448.2621 &gt; min(agstrat$farms92) ## [1] 0 &gt; max(agstrat$farms92) ## [1] 3157 We can explore which county(s) have the highest number of farms (3157) in 1992 with &gt; which(agstrat$farms92 == 3157) ## [1] 274 &gt; agstrat[274,] ## county state acres92 acres87 acres82 farms92 farms87 farms82 ## 274 HAWAII COUNTY HI 926607 1007287 1172448 3157 2810 2539 ## largef92 largef87 largef82 smallf92 smallf87 smallf82 region rn weight ## 274 55 60 58 1960 1602 1468 W 142 10.29268 The 0.05 and 0.95 quantiles (i.e. 5th and 95th percentiles) of farms92 are &gt; quantile(agstrat$farms92, c(.05, .95)) ## 5% 95% ## 89.20 1441.15 meaning that 5% of counties have fewer than 89.2 farms and 95% of counties have fewer than 1441.15 farms. C.3.3 Quantitative grouped by a categorical Suppose we want to know the average number of farms per county for each region. The R function tapply(var, grp, fun) will apply the function fun to the variable var for each group in grp and produces a table of output (hence the t in tapply). Here is this command in action for the farms variable &gt; tapply(agstrat$farms92, agstrat$region, summary) ## $NC ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 58.0 489.5 738.0 750.7 968.5 1669.0 ## ## $NE ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 395.0 451.0 528.1 659.0 1367.0 ## ## $S ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 14.0 265.5 440.0 578.6 777.5 2760.0 ## ## $W ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.0 257.0 495.0 602.3 733.0 3157.0 The average number of farms per county in the northeast region is 528.1. The R package dplyr can also be used to get numerical summaries by groups using the group_by and summarize commands. Here we string together these two commands with the piping command %&gt;% to get the mean and standard deviation of farms92 for each level of region: &gt; library(dplyr) &gt; agstrat %&gt;% + group_by(region) %&gt;% + summarize(mean(farms92), sd(farms92)) ## # A tibble: 4 x 3 ## region `mean(farms92)` `sd(farms92)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NC 751. 358. ## 2 NE 528. 359. ## 3 S 579. 472. ## 4 W 602. 558. The output produced by this string of commands is actually a type of a data frame called a tibble. C.3.4 Graphs R has very sophisticated graphing capabilities. This handout just gives you a summary of some of the most basic graphs. More complicated graphic features will be explained as needed in class. R has high-level plotting commands that create a complete graph, low-level commands which add to an existing graph, and a graphing window layout par command. Use the help command to see these options, e.g. ?hist for the histogram options. A bar graph of the categorical variable region is given by &gt; barplot(table(agstrat$region)) The southern region contains the most counties in our sample (135) and the north east region the fewest counties (21). We can add a label to the y-axis and a title to the plot by adding the arguments &gt; barplot(table(agstrat$region), ylab=&quot;count&quot;, main=&quot;Number of counties per region&quot;) A histogram and boxplot of farms92 are given by &gt; hist(agstrat$farms92, main = &quot;Number of farms per county in 1992&quot;) &gt; boxplot(agstrat$farms92, main = &quot;Number of farms per county in 1992&quot;) We can get a side-by-side boxplot of farms92 by region with &gt; boxplot(farms92 ~ region, data = agstrat, main = &quot;Number of farms per county in 1992&quot;) Suppose we want to look at the distribution of counties across regions grouped by counties with fewer than 500 farms vs. 500 or more farms. First we need to create a factor variable that identifies counties as having less or more than 500 farms: &gt; agstrat$farms500 &lt;- ifelse(agstrat$farms92 &lt; 500, &quot;fewer than 500 farms&quot;, &quot;500 or more farms&quot;) &gt; table(agstrat$farms500) ## ## 500 or more farms fewer than 500 farms ## 164 136 The we create the stacked bar graph for farms500 grouped by region using ggplot2: &gt; library(ggplot2) &gt; ggplot(agstrat, aes(x=region, fill = farms500)) + + geom_bar(position = &quot;fill&quot;) + + labs(y=&quot;proportion&quot;, fill = &quot;Number of farms&quot;, + title = &quot;Number of farms (categorized) by region&quot;) &gt; prop.table(table(agstrat$region, agstrat$farms500), 1) ## ## 500 or more farms fewer than 500 farms ## NC 0.7281553 0.2718447 ## NE 0.4285714 0.5714286 ## S 0.4444444 0.5555556 ## W 0.4878049 0.5121951 Of the 103 counties in the North Central region, about 72.8% have 500 or more farms. Of the 135 counties in the Southern region, about 44.4% have 500 or more farms. We can also use ggplot2 to create histograms of farms92 by region: &gt; ggplot(agstrat, aes(x=farms92)) + + geom_histogram() + + facet_wrap(~region) + + labs(title = &quot;Number of farms by region&quot;) We can also use the ggplot2 package to get side-by-side boxplots grouped by a third variable. Here we can compare the distribution of total farm acreage in 1992 (acres92) by region for counties that have fewer than 500 farms vs. 500 or more farms: &gt; ggplot(agstrat, aes(x = farms500, y=acres92)) + + geom_boxplot() + + facet_wrap(~region) + + labs(title = &quot;Farm acres by number of farms and region&quot;) The relationship between median acreage across the four regions looks similar regardless of how many farms are present in a county (with western counties having the highest acreage). But for all four regions, it looks like the median acreage is highest for counties with fewer than 500 farms. Counties with fewer farms may tend to have larger farms than counties with more (smaller) farms across all four regions. C.3.5 Reporting Results Homework and reports should be done using an R Markdown document in RStudio (see Section D). If you do need to copy a graph from Rstudio into another document, use the Copy Plot to Clipboard option in the Export menu. C.4 Factor variables Section C.2.3 showed how to determine the levels of a factor variable. There are many more things you may want to do with a categorical variable that is a factor type. Here are a few hints for manipulating a factor. C.4.1 Renaming factor levels The R package forcats has a fct_recode command to rename the levels of your factor variable &gt; library(forcats) &gt; mydata &lt;- data.frame(myfac=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)) &gt; table(mydata$myfac) ## ## a b c d e ## 1 1 2 1 1 &gt; mydata$new_myfac &lt;- fct_recode(mydata$myfac, + &quot;Aman&quot; = &quot;a&quot;, + &quot;Barb&quot; = &quot;b&quot;, + &quot;Chad&quot; = &quot;c&quot;, + &quot;Daryl&quot; = &quot;d&quot;, + &quot;Eliza&quot; = &quot;e&quot;) &gt; table(mydata$new_myfac) # check work ## ## Aman Barb Chad Daryl Eliza ## 1 1 2 1 1 C.4.2 Recode a categorical variable with many levels Suppose you have a variable var with response levels strongly agree, agree, disagree, and strongly disagree. You want to create a new version of this variable by combining all agree and all disagree answers. Here is one way to do this: &gt; mydata$new_var &lt;- ifelse(mydata$var %in% c(&quot;strongly agree&quot;, &quot;agree&quot;), &quot;agree&quot;, &quot;disagree&quot;) Any row in the dataset where var is in the set of responses listed (c(\"strongly agree\", \"agree\")) will be coded at agree in the newvar. All other responses (disagree, and strongly disagree) will be recoded as disagree in the newvar. If you have lots of levels that you want to collapse into fewer (or you just don’t want to use the ifelse command), then you should use the forcats package command fct_collapse. Here we have a variable called myfac that has levels a-e that we want to collapose into new groups low (just level a), mid (levels b and c) and high (levels d and e) &gt; mydata &lt;- data.frame(myfac=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)) &gt; mydata$new_myfac &lt;- fct_collapse(mydata$myfac, + low = c(&quot;a&quot;), + mid = c(&quot;b&quot;,&quot;c&quot;), + high = c(&quot;d&quot;,&quot;e&quot;)) &gt; table(mydata$myfac,mydata$new_myfac) ## ## low mid high ## a 1 0 0 ## b 0 1 0 ## c 0 2 0 ## d 0 0 1 ## e 0 0 1 Just make sure that original factor levels of myfac are correctly spelled in the right-hand side of the assignment expressions. C.4.3 Converting some factor levels to NAs Sometimes you have too many levels to handle in a factor variable. Collapsing many levels into fewer is one solution (3.1), or we can create a version of the data that ignores the levels we don’t want to analyze. One way to do this is to turn those levels in NA (missing values) that R usually ignores. We can do this in the read.csv command (see section 1.3) or we can do this in the fct_collapse command or fct_recode Here we convert the d and e responses in myfac to missing values, while all other levels stay the same: &gt; mydata$try1 &lt;- fct_recode(mydata$myfac, NULL = &quot;d&quot;, NULL = &quot;e&quot;) &gt; summary(mydata$try1) ## a b c NA&#39;s ## 1 1 2 2 We can use similar syntax in the fct_collapse to both collapse levels and turn d and e into NA: &gt; mydata$try2 &lt;- fct_collapse(mydata$myfac, + low = c(&quot;a&quot;), + mid = c(&quot;b&quot;,&quot;c&quot;), + NULL = c(&quot;d&quot;,&quot;e&quot;)) &gt; summary(mydata$try2) ## low mid NA&#39;s ## 1 3 2 C.4.4 Changing the order of levels You can reorder the levels of a factor variable. Suppose newmyfac has responses that are ordered low, mid, and high. You can rearrange the order of these levels using the forcats package is fct_relevel command: &gt; table(mydata$new_myfac) # first check original order and exact spelling ## ## low mid high ## 1 3 2 &gt; mydata$new_myfac2 &lt;- fct_relevel(mydata$new_myfac, &quot;high&quot;,&quot;mid&quot;,&quot;low&quot;) &gt; table(mydata$new_myfac2) ## ## high mid low ## 2 3 1 C.4.5 Recode a numerically coded categorical variable Suppose you have a variable quant that is a categorical variable that was numerically coded (e.g. a 1=a, 2=b, 3=c, etc). You will need to convert this to a factor variable to analyze it correctly. Here is one way to do this: &gt; library(dplyr) &gt; mydata$quant &lt;- c(1,2,3,3,4,5) &gt; mydata$quant &gt; mydata$quant_fac &lt;- fct_recode(factor(mydata$quant), + &quot;a&quot; = &quot;1&quot;, + &quot;b&quot; = &quot;2&quot;, + &quot;c&quot; = &quot;3&quot;, + &quot;d&quot; = &quot;4&quot;, + &quot;e&quot; = &quot;5&quot;) &gt; mydata$quant_fac C.4.6 Recode a factor into a numeric There are times that a quantitative variable (like age) turns up as a factor after you read your data into R. This is due to at least one response in the column being a text response (non-numeric). R then defaults this column to the factor type. Suppose you’ve identified all character (text) entries in a variable that need to be either recoded into a number or turned into an NA to be ignored. You can use the readr package’s command parse_number to convert a factor variable into a numeric variable with a “best guess” at how to do this. For the ages variable with “over 90”, we see that parse_number strips away the “over” text and just leaves the number 90: &gt; library(readr) &gt; ages &lt;- factor(c(20, 18, 45, 34,&quot;over 90&quot;)) &gt; ages ## [1] 20 18 45 34 over 90 ## Levels: 18 20 34 45 over 90 &gt; new.ages &lt;- parse_number(as.character(ages)) &gt; new.ages ## [1] 20 18 45 34 90 For this version of ages, the function pulls the numbers that occur prior to the first character (-): &gt; ages &lt;- factor(c(20, 18, 45, 34,&quot;90-100&quot;)) &gt; ages ## [1] 20 18 45 34 90-100 ## Levels: 18 20 34 45 90-100 &gt; new.ages &lt;- parse_number(as.character(ages)) &gt; new.ages ## [1] 20 18 45 34 90 Rather than 90, we may want the entry to be the midpoint between 90 and 100: &gt; library(dplyr) &gt; ages2 &lt;- recode_factor(ages, &#39;90-100&#39; = &quot;95&quot;) &gt; ages2 ## [1] 20 18 45 34 95 ## Levels: 95 18 20 34 45 &gt; new.ages &lt;- parse_number(as.character(ages2)) &gt; new.ages ## [1] 20 18 45 34 95 Finally, if there is no numeric value in an entry then parse_number will recode it automatically into an NA and give you a warning that lets you know it did this action: &gt; ages &lt;- factor(c(20, 18, 45, 34,&quot;way old&quot;)) &gt; ages ## [1] 20 18 45 34 way old ## Levels: 18 20 34 45 way old &gt; new.ages &lt;- parse_number(as.character(ages)) &gt; new.ages ## [1] 20 18 45 34 NA ## attr(,&quot;problems&quot;) ## # A tibble: 1 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5 NA a number way old &gt; summary(new.ages) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 18.00 19.50 27.00 29.25 36.75 45.00 1 "],
["markdown.html", "D R Markdown D.1 How to write an R Markdown document D.2 Changing R Markdown chunk evaluation behavior D.3 Creating a new R Markdown document D.4 Extra: Graph formatting D.5 Extra: Table formatting", " D R Markdown An R Markdown (.Rmd) file will allow you to integrate your R commands, output and written work in one document. You write your R code and explanations in the .Rmd file, the knit the document to a Word, HTML, or pdf file. A basic R Markdown file has the following elements: Header: this is the stuff in between the three dashes --- located at the top of your .Rmd file. A basic header should specify your document title, author and output type (e.g. word_document). Written work: Write up your work like you would in any word/google doc. Formatting is done with special symbols. E.g. to bold a word or phrase, place two asterisks ** at the start and end of the word or phrase (with no spaces). To get section headers use one or more hash tags # prior to the section name. R code: Your R commands are contained in one or more chunks that contains one or more R commands. A chunk starts with three backticks (to the left of your 1 key) combined with {r} and a chunk ends with three more backticks. See the image below for an example of a chunk that reads in the data files HollywoodMovies2011.csv. A R chunk that reads in a data file Important!! A common error that students run into when first using R Markdown is forgetting to put the read.csv command in their document. An R Markdown document must contain all commands needed to complete an analysis. This includes reading in the data! Basically, what happens during the knitting process is that a fresh version of an Rstudio environment is created that is completely separate from the Rstudio you see running in front of you. The R chunks are run in this new environment, and if you will encounter a Markdown error if you, say, try to use the movies data frame without first including the read.csv chunk shown in Figure 1. D.1 How to write an R Markdown document Write your commands in R chunks, not in the console. Run chunk commands using the suggestions in the Hints section below. Knit your document often. This allows you to catch errors/typos as you make them. You can knit a .Rmd by pressing the Knit button at the top of the doc. You can change output types (e.g. switch from HTML to Word) by typing in the preferred doc type in the header, or by using the drop down menu option found by clicking the down triangle to the right of the Knit button. You can run a line of code in the R console by putting your cursor in the line and selecting Run &gt; Run Selected Line(s). You can run all commands in a chunk by clicking the green triangle on the right side of the chunk. URLs can be embeded between &lt; and &gt; symbols. The image below shows a quick scrolling menu that is available by clicking the double triangle button at the bottom of the .Rmd. This menu shows section headers and available chunks. It is useful for navagating a long .Rmd file. Quick scroll through Markdown document D.2 Changing R Markdown chunk evaluation behavior The default setting in Rstudio when you are running chunks is that the “output” (numbers, graphs) are shown “inline” within the Markdown Rmd. For a variety of reasons, my preference is to have commands run in the console. To see the difference between these two types of chunk evaluation option, you can change this setting as follows: Select Tools &gt; Global Options. Click the R Markdown section and uncheck (if needed) the option Show output inline for all R Markdown documents. Click OK. Now try running R chunks in the .Rmd file to see the difference. You can recheck this box if you prefer the default setting. D.3 Creating a new R Markdown document I suggest using old .Rmd HW file as a template for a new HW assignment. But if you want to create a completely new docment: Click File &gt; New File &gt; R Markdown…. A window like the one shown below should appear. The default settings will give you a basic Markdown (.Rmd) file that will generate an HTML document. Click OK on this window. Opening a Markdown document You should now have an “Untitled1” Markdown file opened in your document pane of Rstudio. Save this file, renamed as “FirstMarkdown.Rmd”, somewhere on your computer. (Ideally in a Math215 folder!) On Mirage, save the file in the default location (which is your account folder on the mirage server). Now click the Knit HTML button on the tool bar at the top of your Markdown document. This will generate a “knitted” (compiled) version of this document. Check that there is now an HTML file named “FirstMarkdown.html” in the same location as your “FirstMarkdown.Rmd” file. D.4 Extra: Graph formatting The markdown .Rmd for this graph formatting section is linked here: https://kstclair.github.io/Rmaterial/Markdown/Markdown_GraphFormatting.Rmd The data set Cereals contains information on cereals sold at a local grocery store. &gt; # load the data set &gt; Cereals &lt;- read.csv(&quot;http://math.carleton.edu/Stats215/RLabManual/Cereals.csv&quot;) D.4.1 Adding figure numbers and captions To add captions to the figures you make you need to add the argument fig.cap=\"my caption\" to your R chunk that creates the figure. If you have two or more figures created in the R chunk then give the fig.cap argument a vector of captions. If you are knitting to a pdf, you don’t need to add “Figure 1”, etc. numbering to the figure captions (they will be numbered automatically). For HTML and Word output types, you need to manually number figures. D.4.2 Resizing graphs in Markdown Suppose we want to create a boxplot of calories per gram grouped by cereal type and a scatterplot of calories vs. carbs per gram. Here are the basic commands without any extra formatting that create Figures 1 and 2: ```{r, fig.cap=\"Figure 1: Distributions of calories per gram by cereal type\"} boxplot(calgram ~ type, data=Cereals, main=\"Calories by type\", ylab=\"Calories per gram\") ``` Figure D.1: Distributions of calories per gram by cereal type ```{r, fig.cap=\"Figure 2: Calories vs. Carbs per gram\"} plot(carbsgram ~ calgram, data=Cereals, main=\"Carbs vs Calories\") ``` Figure D.2: Calories vs. Carbs per gram We can add fig.height and fig.width parameters to the Markdown R chunk to resize the output size of the graph. The size inputs used here are a height of 3.5 inches and a width of 6 inches. The command below creates Figures 3 and 4. ```{r, fig.height=3.5, fig.width=5, fig.cap=c(\"Figure 3: Distributions of calories per gram by cereal type\",\"Figure 4: Calories vs. Carbs per gram\")} boxplot(calgram ~ type, data=Cereals, main=\"Calories by type\", ylab=\"Calories per gram\") plot(carbsgram ~ calgram, data=Cereals, main=\"Carbs vs Calories\") ``` &gt; boxplot(calgram ~ type, data=Cereals, main=&quot;Calories by type&quot;, ylab=&quot;Calories per gram&quot;) Figure D.3: Distributions of calories per gram by cereal type &gt; plot(carbsgram ~ calgram, data=Cereals, main=&quot;Carbs vs Calories&quot;) Figure D.4: Calories vs. Carbs per gram D.4.3 Changing graph formatting in R You can use the par command to change R’s graphical parameter settings for plots that are not made from ggplot2. There are many options that can be changed, but one of the most useful is to change the layout of the graphical output display. The argument mfrow (multi-frame row) is given a vector c(nr, nc) that draws figures in an nr (number of rows) by nc (number of columns) array. We can arrange our two graphs in a 1 by 2 display (1 row, 2 columns) with the command: &gt; par(mfrow=c(1,2)) &gt; boxplot(calgram ~ type, data=Cereals, main=&quot;Calories by type&quot;, ylab=&quot;Calories per gram&quot;) &gt; plot(carbsgram ~ calgram, data=Cereals, main=&quot;Carbs vs Calories&quot;) Figure D.5: Distribution of calories per gram by cereal type and calories vs. carbs per gram. D.4.4 Hiding R commands You can omit R commands from your final document by adding echo=FALSE to your R chunk argument. Any output produced by your command (graphical or numerical) will still be displayed. For example, the following command creates Figure 6, a boxplot of carbs per gram by cereal type. ```{r, echo=FALSE, fig.cap=\"Figure 6: Distributions of calories per gram and shelf placement by cereal type\", fig.height=3, fig.width=4} boxplot(carbsgram ~ type, data=Cereals, main=\"Carbs by type\", ylab=\"Carbs per gram\") ``` Figure D.6: Distributions of calories per gram and shelf placement by cereal type D.4.5 Global changes in graph format The R chunk options that control graph sizes and output features (like echo) can be set globally for all R chunks either in the header (like with fig.caption) or in an opts_chunk$set() command at the start of the .Rmd file. I usually opt for setting global features with the opts_chunk command which you often see at the start of my .Rmd files. Any global settings, like echo or fig.height, can be overridden locally by changing them in individual chunks. D.4.6 Comments: Markdown is very sensitive to spaces, or lack-there-of. If you get odd formatting issues, try adding a spaces between R chunks, paragrahs, lists, section headers, etc. For example, you always need a space between an R chunk or text and a section header. D.5 Extra: Table formatting The markdown .Rmd for this table formatting section is linked here: https://kstclair.github.io/Rmaterial/Markdown/Markdown_TableFormatting.Rmd This handout gives some basic ways to format numerical output produced in your R chunks. Some of the methods mentioned below might only work when knitting to a PDF. Additional info about formatting text in R Markdown can be found online: http://rmarkdown.rstudio.com/authoring_basics.html http://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf http://rmarkdown.rstudio.com/pdf_document_format.html In homework or R appendices, I expect to see both the commands and output produced by those commands as your “work” for a problem. But in reports, as in any formal research paper, you should not include R commands or output (except for graphs). This handout is designed, primarily, to help you format numerical results to present into your written reports. The data set Cereals contains information on cereals sold at a local grocery store. &gt; # load the data set &gt; Cereals &lt;- read.csv(&quot;http://math.carleton.edu/Stats215/RLabManual/Cereals.csv&quot;) D.5.1 Hiding R commands and R output As mentioned in the graph formatting handout, adding the chunk option echo=FALSE will display output (like graphs) produced by a chunk but not show the commands used in the chunk. You can stop both R commands and output from being displayed in a document by adding the chunk option include=FALSE. As you work through a report analysis, you may initially want to see all of your R results as you are writing your report. But after you’ve summarized results in paragraphs or in tables, you can then use the include=FALSE argument to hid your R commands and output in your final document. If you ever need to rerun or reevaluate your R work for a report, you can easily recreate and edit your analysis since the R chunks used in your original report are still in your R Markdown .Rmd file. D.5.2 Markdown tables The Markdown language allows you to construct simple tables using vertical lines | to separate columns and horizontal lines - to create a header. Make sure to include at least one space before and after your Markdown table or it will not format correctly. I can’t find an easy way to attached an automatic table number and caption to this type of table, so I’ve simply written (and centered) the table number and caption by hand for the table below. Suppose we want to present the 5-number summary of calories per gram by cereal type. The tapply command can be used to obtain these numbers. &gt; tapply(Cereals$calgram, Cereals$type, summary) ## $adult ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 3.208 3.519 3.399 3.667 4.600 ## ## $children ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.636 3.931 4.000 4.028 4.074 4.483 We can construct a table of stats by type “by hand” using simple markdown table syntax in our .Rmd file that is shown below: Type | Min | Q1 | Median | Q3 | Max ---- | --- | --- | --- | --- | --- Adult | 2.0 | 3.2 | 3.5 | 3.7 | 4.6 Children | 3.6 | 3.9 | 4.0 | 4.1 | 4.5 The knitted table produced is shown below: Type Min Q1 Median Q3 Max Adult 2.0 3.2 3.5 3.7 4.6 Children 3.6 3.9 4.0 4.1 4.5 D.5.3 Markdown tables via kable The R package knitr contains a simple table making function called kable. You can use this function to, say, show the first few rows of a data frame: &gt; library(knitr) &gt; kable(head(Cereals), digits=3, caption=&quot;Table 1: Cereals data (first 6 cases)&quot;) &gt; library(knitr) &gt; kable(head(Cereals), digits=3, caption=&quot;Cereals data (first 6 cases)&quot;) Table D.1: Cereals data (first 6 cases) brand type shelf cereal serving calgram calfatgram totalfatgram sodiumgram carbsgram proteingram GM children bottom Lucky Charms 30 4.000 0.333 0.033 0.007 0.833 0.067 GM adult bottom Cheerios 30 3.667 0.500 0.067 0.007 0.733 0.100 Kellogs children bottom Smorz 30 4.000 0.667 0.067 0.005 0.833 0.033 Kellogs children bottom Scooby Doo Berry Bones 33 3.939 0.303 0.030 0.007 0.848 0.030 GM adult bottom Wheaties 30 3.667 0.333 0.033 0.007 0.800 0.100 GM children bottom Trix 30 4.000 0.500 0.050 0.006 0.867 0.033 Or you can use kable on a two-way table of counts or proportions: &gt; kable(table(Cereals$brand, Cereals$type), caption=&quot;Table 2: Cereal brand and type&quot;) Table D.2: Cereal brand and type adult children GM 4 11 Kashi 6 0 Kellogs 4 13 Quaker 1 2 WW 2 0 D.5.4 The pander package The R package pander creates simple tables in R that do not need any additional formatting in Markdown. The pander() function takes in an R object, like a summary table or t-test output, and outputs a Markdown table. You can add a caption argument to include a table number and title. Here is a table for the summary of calories per gram: &gt; library(pander) &gt; pander(summary(Cereals$calgram), caption=&quot;Table 3: Summary statistics for calories per gram.&quot;) Table 3: Summary statistics for calories per gram. Min. 1st Qu. Median Mean 3rd Qu. Max. 2 3.636 3.929 3.779 4.031 4.6 Pander can format tables and proportion tables. Here is the table for cereal type and shelf placement (Table 4), along with the distribution of shelf placement by cereal type (Table 5). &gt; my.table &lt;- table(Cereals$type,Cereals$shelf) &gt; pander(my.table,round=3, caption=&quot;Table 4: Cereal type and shelf placement&quot;) Table 4: Cereal type and shelf placement bottom middle top adult 2 1 14 children 7 18 1 &gt; pander(prop.table(my.table,1),round=3, caption=&quot;Table 5: Distribution of shelf placement by cereal type&quot;) Table 5: Distribution of shelf placement by cereal type bottom middle top adult 0.118 0.059 0.824 children 0.269 0.692 0.038 Here are t-test results for comparing mean calories for adult and children cereals (Table 6): &gt; pander(t.test(calgram ~ type, data=Cereals), caption=&quot;Table 6: Comparing calories for adult and children cereals&quot;) Table 6: Comparing calories for adult and children cereals (continued below) Test statistic df P value Alternative hypothesis -4.066 18.45 0.0006942 * * * two.sided mean in group adult mean in group children 3.399 4.028 Here are chi-square test results for testing for an association between shelf placement and cereal type (Table 7). Note that the simulate.p.value option was used to give a randomization p-value since the sample size criteria for the chi-square approximation was not met. &gt; pander(chisq.test(my.table, simulate.p.value = TRUE),caption=&quot;Table 7: Chi-square test for placement and type&quot;) Table 7: Chi-square test for placement and type Test statistic df P value 28.63 NA 0.0004998 * * * Here are the basic results for the regression of carbs on calories (Table 8). &gt; pander(lm(carbsgram ~ calgram, data=Cereals), caption=&quot;Table 8: Regression of carbs on calories&quot;) Table 8: Regression of carbs on calories Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.1021 0.0804 1.27 0.2111 calgram 0.1798 0.02108 8.528 1.264e-10 D.5.5 The stargazer package The stargazer package, like pander, automatically generates Markdown tables from R objects. The stargazer function has more formatting options than pander and can generate summary stats from a data frame table. It can also provide nicely formatted comparisons between 2 or more regression models. See the help file ?stargazer for more options. You will need to add the R chunk option results='asis' to get the table formatted correctly. I also include the message=FALSE option in the chunk below that runs the library command to suppress the automatic message created when running the library command with stargazer. When you give stargazer a data frame, it gives you summary stats for all numeric variables in the data frame (Table 10): ```{r, results='asis', message=FALSE} library(stargazer) stargazer(Cereals, type=\"html\", title=\"Table 9: Default summary stats using stargazer\") ``` Table 9: Default summary stats using stargazer Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max serving 43 36.953 10.542 27 30 50 60 calgram 43 3.779 0.517 2 3.6 4.0 5 calfatgram 43 0.490 0.261 0.000 0.328 0.600 1.034 totalfatgram 43 0.053 0.031 0.000 0.033 0.063 0.121 sodiumgram 43 0.005 0.002 0.000 0.003 0.006 0.007 carbsgram 43 0.782 0.116 0.280 0.767 0.850 0.920 proteingram 43 0.082 0.057 0.030 0.034 0.097 0.267 The default table type is \"latex\" which is the format you want when knitting to a pdf document. When knitting to an html document we need to change type to \"html\". Unfortunately, there is no type that works nicely with Word documents so you would be better off using pander if you want a Word document. Note: When using the latex type and knitting to a pdf, you will get an annoying stargazer message about the creation of your latex table. Include the argument header=FALSE in the stargazer command to suppress this message when knitting to a pdf. You can subset the Cereals data frame to only include the variables (columns) that you want displayed. In Table 11 we only see calories and carbs. You can also edit the summary stats displayed by specifying them in the summary.stat argument. See the stargazer help file for more stat options. &gt; stargazer(Cereals[,c(&quot;calgram&quot;,&quot;carbsgram&quot;)], + type=&quot;html&quot;, + title=&quot;Table 10: Five number summary stats&quot;, + summary.stat=c(&quot;max&quot;,&quot;p25&quot;,&quot;median&quot;,&quot;p75&quot;,&quot;max&quot;)) Table 10: Five number summary stats Statistic Max Pctl(25) Median Pctl(75) Max calgram 5 3.6 3.9 4.0 5 carbsgram 0.920 0.767 0.800 0.850 0.920 The stargazer package was created to display results of statistical models. Here is the basic display for the regression of carbs on calories (Table 12). The argument single.row puts estimates and standard errors (in parentheses) in one row. There are many options that can be tweaked, like including p-values or confidence intervals. &gt; my.lm &lt;- lm(carbsgram ~ calgram, data=Cereals) &gt; stargazer(my.lm, type=&quot;html&quot;, + title=&quot;Table 11: Regression of carbs on calories&quot;, + single.row=TRUE) Table 11: Regression of carbs on calories Dependent variable: carbsgram calgram 0.180*** (0.021) Constant 0.102 (0.080) Observations 43 R2 0.639 Adjusted R2 0.631 Residual Std. Error 0.071 (df = 41) F Statistic 72.721*** (df = 1; 41) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Table 13 adds the argument keep.stat to specify that only sample size and \\(R^2\\) should be included in the table. See the help file for more options to this argument. &gt; stargazer(my.lm, type=&quot;html&quot;, + title=&quot;Table 12: Regression of carbs on calories&quot;, + single.row=TRUE, + keep.stat=c(&quot;n&quot;,&quot;rsq&quot;)) Table 12: Regression of carbs on calories Dependent variable: carbsgram calgram 0.180*** (0.021) Constant 0.102 (0.080) Observations 43 R2 0.639 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 "],
["mathreview.html", "E Math review E.1 Linear equations E.2 Logarithms E.3 Exercises", " E Math review E.1 Linear equations E.2 Logarithms Let \\(b&gt;0\\) and \\(x&gt;0\\). The logarithm (base-\\(b\\)) of \\(x\\) is denoted \\(\\log_b(x)\\) and equal to \\[ \\log_b(x) = a \\] where \\(a\\) tells us what power we must raise \\(b\\) to to obtain the value \\(x\\): \\[ b^a = x \\] Easy examples are: \\(b=2\\), \\(x=8\\) and \\(a=3\\), \\[ \\log_2(8) = 3 \\] since \\(2^3 = 8\\). Or using base \\(b=10\\), then \\[ \\log_{10}(0.01) = -2 \\] since \\(10^{-2} = 0.01\\). Some basic facts logarithm facts are \\[ \\log_b(b) = 1 \\] since \\(b^1 = b\\) and \\[ \\log_b(1) = 0 \\] since \\(b^0 = 1\\). E.2.1 Interpreting logged variables Multiplicative changes in \\(x\\) result in additive changes in \\(\\log_b(x)\\). If \\(m&gt;0\\), then \\[ \\log_b(mx) = \\log_b(m) + \\log_b(x) \\] For example, a doubling of \\(x=8\\) results in an increase in \\(\\log_2(8)\\) of one unit: \\[ \\log_2(16) = \\log_2(2\\times 8) = \\log_2(2) + \\log_2(8) = 1 + 3 = 4 \\] More generally if we use a base-2 logarithm, a doubling of \\(x\\) results in an additive increase in \\(\\log(x)\\) of 1 unit: \\[ \\log_2(2\\times x) = \\log_2(2) + \\log_2(x) = 1 + \\log_2(x) \\] E.2.2 Inverse (i.e. reversing the log, getting rid of the log, …) The logarithm and exponential functions are inverses of one another. This means we can “get rid” of the log by calculating \\(b\\) raised to the logged-function: \\[ b^{\\log_b(x)} = x \\] This will be useful in regression when we have a linear relationship between logged-response \\(y\\) and a set of predictors. For example, suppose we know that \\[ \\log_2(y) = 3 + 5x + \\epsilon \\] To return this to an expression on the original (unlogged) scale of \\(y\\), we need take both sides raised to the base 2: \\[ 2^{\\log_2(y)} = 2^{3 + 5x+ \\epsilon} \\] Simplifying both sides gives \\[ y = 2^3 \\times 2^{5x} \\times x^{\\epsilon} \\] E.2.3 Logarithms in R The R function log gives the natural logarithm (base-\\(e\\)): &gt; log(2) ## [1] 0.6931472 &gt; log(exp(1)) ## [1] 1 Other common logarithm bases are base-2 and base-10: &gt; log2(8) ## [1] 3 &gt; log10(100) ## [1] 2 General bases can be added as an argument: &gt; log(49, base = 7) ## [1] 2 E.3 Exercises Write the following as the sum of two logarithms. Simplify as much as possible: \\(\\log_2(2x)\\) \\(\\log_2(0.5x)\\) \\(\\ln(2x)\\) where \\(\\ln\\) is the natural log (base-\\(e\\)) Write the following expressions in terms of \\(y\\), not \\(\\log(y)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3x\\) \\(\\log_{10}(y) = -2 + 0.4x\\) \\(\\ln(y) = 1 - 3x\\) Write the following expressions in terms of \\(y\\) and \\(x\\), not \\(\\log(y)\\) and \\(\\log(x)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3\\log_2(x)\\) \\(\\ln(y) = -2 + 0.4\\ln(x)\\) \\(\\ln(y) = 1 - 3\\log_2(x)\\) Logarithmic model: Regression of \\(Y\\) on \\(\\log(x)\\) obtains the following estimated mean of \\(Y\\): \\[ \\hat{\\mu}(Y \\mid x) = 1 - 3 \\log_2(x) \\] What is the change in estimated mean response if we double the value of \\(x\\)? What is the change in estimated mean response if we triple the value of \\(x\\)? What is the change in estimated mean response if we reduce the value of \\(x\\) by 20%? Exponential model: Regression of \\(\\log_2(Y)\\) on \\(x\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = -2 + 0.4x \\] Write the median in terms of \\(Y\\) instead of \\(\\log_2(Y)\\). Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 1 unit? What is the percent change in estimated median response if we increase \\(x\\) by 1 unit? What is the multiplicative change in estimated median response if we decrease \\(x\\) by 2 units? What is the percent change in estimated median response if we decrease \\(x\\) by 2 units? Power model: Regression of \\(\\log_2(Y)\\) on \\(\\log_2(x)\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = 1 -3\\log_2(x) \\] Write the median in terms of \\(Y\\) and \\(x\\) instead of \\(\\log\\)s. Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 50%? What is the percent change in estimated median response if we increase \\(x\\) by 50%? What is the multiplicative change in estimated median response if we reduce the value of \\(x\\) by 20%? What is the percent change in estimated median response if we reduce the value of \\(x\\) by 20%? Here is a link to the solution "],
["references.html", "References", " References "]
]
