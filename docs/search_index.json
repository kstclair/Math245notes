[
["logistic.html", "Chapter 4 Logistic Regression 4.1 The variables 4.2 The Bernoulli distribution 4.3 The logistic model form 4.4 Inference and estimation", " Chapter 4 Logistic Regression This chapter covers material from chapters 20-21 of Sleuth. 4.1 The variables Suppose we have a categorical response variable \\(Y\\) that can take one of two values, which we will generically call a success or failure. We want to relate the probability of success to \\(p\\) explantory variables (aka predictors, covariates) \\(x_1, \\dotsc, x_p\\). There is no restriction on the type of covariates, they can be both quantitative and categorical variables. 4.1.1 Example: Donner party EDA Sleuth case study 20.1 looks at data from the infamous Donner party. This wagon train was migrating to the west coast of the US in the mid-1800s and became snow-bound in the Sierra Nevada mountains during the winter of 1846-7. We are interested in modeling the categorical variable “survival” (or not) as a function of individual covariates like age or sex. &gt; library(Sleuth3) &gt; donner &lt;- case2001 &gt; summary(donner) ## Age Sex Status ## Min. :15.0 Female:15 Died :25 ## 1st Qu.:24.0 Male :30 Survived:20 ## Median :28.0 ## Mean :31.8 ## 3rd Qu.:40.0 ## Max. :65.0 A stacked bar graph shows that females had a higher surival rate than males: &gt; library(ggplot2) &gt; ggplot(donner, aes(fill = Status, x = Sex)) + + geom_bar(position=&quot;fill&quot;) + + labs(y=&quot;proportion&quot;, title=&quot;Surival rates by Sex&quot;) We can use the dplyr package’s group_by function to divide the data into the two Sex groups and compute the proportion who Survived within each group. Here we see that 2/3 of females survived while only 1/3 of males survived. &gt; library(dplyr) &gt; donner %&gt;% + group_by(Sex) %&gt;% # for each Sex group + summarize(mean(Status == &quot;Survived&quot;)) # proportion who survived ## # A tibble: 2 x 2 ## Sex `mean(Status == &quot;Survived&quot;)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 Female 0.667 ## 2 Male 0.333 A side-by-side boxplot shows that people who survived tended to be younger: &gt; library(ggplot2) &gt; ggplot(donner, aes(x = Status, y = Age)) + + geom_boxplot() + + coord_flip() We can get stats by status group: &gt; donner %&gt;% + group_by(Status) %&gt;% # for each status group + summarize(mean(Age), sd(Age), median(Age)) # get summary stats ## # A tibble: 2 x 4 ## Status `mean(Age)` `sd(Age)` `median(Age)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Died 35.5 14.3 30 ## 2 Survived 27.2 8.00 25 But these stats are not quite what we want when trying to model survival as a function of age. E.g. of people who survived, we know that the mean age was 27.2 while of people who died, the mean age was 35.5. But this is the wrong direction of conditioning, we would like to say how survival rates change as we increase age by a year, for example. We could employ a scatterplot for this purpose, but we need to recode (in dplyr) Status into a binary variable that recodes survival as a 1 and death as a 0: &gt; donner$Ind_surv &lt;- recode(donner$Status, Survived = 1, Died = 0) Then use a jitter plot to avoid overplotting: &gt; ggplot(donner, aes(x = Age, y = Ind_surv)) + + geom_jitter(aes(color=Status), height = .01) As we increase age, try vertically “slicing” these points. The proportion of survivals in these similar age groups will just be the mean of the binary 0’s and 1’s in each slice. We have more blue points than red in the low age slices than in high age, so we see that the probability of survival tends to decrease as age increases. We will next consider how to construct a model that gives us a “best fit” curve for this probability of surival. 4.2 The Bernoulli distribution The Bernoulli distribution is a probability model for a random trial that has two possible outcomes: success or failure. A Bernoulli random variable \\(Y\\) “counts” the number of successes in a Bernoulli random trial. If a “success” occurred then \\(Y=1\\) and if a “failure” occurred then \\(Y=0\\). We will let \\(\\pi\\) be the probability of success: \\[ \\pi = P(Y=1) = P(success), \\ \\ \\ \\ \\ 1-\\pi = P(Y=0) = P(failure) \\] If \\(Y\\) is a Bernoulli random variable, then we can use the shorthand notation \\(Y \\sim Bern(\\pi)\\) to denote this. The expected value, or mean, of \\(Y\\) is equal to \\[ E(Y) = \\mu = \\pi \\] and the standard deviation of \\(Y\\) is equal to \\[ SD(Y) = \\sigma = \\sqrt{\\pi(1-\\pi)} \\] The expected value (or mean) of a random variable measures the “long run” average value that we would see from \\(Y\\) if we were to repeat the random trial many, many times. The standard deviation tells us how these values of \\(Y\\) will vary over these repeated trials. 4.3 The logistic model form The population, or data generating, model for a logistic regression model for \\(Y\\) assumes that each \\(Y_i\\) is a Bernoulli random variable whose probability of success depends on covariates \\(\\pmb{x_{1,i}, \\dotsc, x_{p,i}}\\). Specifically, \\(Y_i \\mid X_i \\overset{indep.}{\\sim} Bern(\\pi(X_i))\\) binary response: \\(Y_i\\)’s are categorical with only two options independence: Given \\(X_i\\) values, \\(Y_i\\)’s are independent We connect the linear combination of predictors \\[ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i} \\] to the probability of success using the logistic function form: \\[ \\pi(X_i) = \\dfrac{e^{\\eta_i}}{1 + e^{\\eta_i}} = \\dfrac{e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}}{1 + e^{\\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}}} \\] This function form is used because its inverse is equal to \\[ \\eta_i = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i}= \\ln \\left( \\dfrac{\\pi(X_i)}{1-\\pi(X_i)}\\right) \\] This function is called the logit function: \\(logit(\\pi) = \\ln \\left( \\dfrac{\\pi}{1-\\pi}\\right)\\). This means that a one unit increase in \\(x_1\\) can be interpreted as an additive \\(\\beta_1\\) change in in the logit function, holding other terms fixed. But what does this mean? The odds of success is defined as the ratio of the probability of success to the probability of failure: \\[ odds = \\dfrac{\\pi(X)}{1-\\pi(X)} \\] For example, if the probability of success is 0.6 then the odds of success is \\(0.6/0.4 = 1.5\\). Meaning for every 6 successes, we see 4 failures. If the probability of success is 0.1, then the odds of success is \\(0.1/0.9 \\approx 0.111\\), meaning for every 1 success we see 9 failures. Odds greater than 1 indicate the probability of success is above 50% while odds less than 1 indicate the probability of success is less than 50%. So, we now can see that the logit function equals the log-odds of success: \\[ \\ln \\left( \\dfrac{\\pi(X_i)}{1-\\pi(X_i)}\\right) = \\beta_0 + \\beta_1 x_{1,i} + \\dotsm + \\beta_p x_{p,i} \\] This model form is an example of a generalized linear model which relates the response \\(Y\\) to predictors through a linear combination \\(\\eta\\) of predictors. Is does this by defining the following functions: The kernel mean function defines the expected value (mean) of \\(Y\\) as a function of \\(\\eta\\). in a logistic model, the kernel mean function is the logistic function \\(E(Y \\mid X) = \\pi(X) = \\dfrac{e^{\\eta}}{1+e^{\\eta}}\\) The link function defines the linear combination \\(\\eta\\) as a function of the mean of \\(Y\\). in a logistic model, the link function is the logit function \\(\\eta = \\ln(\\pi/(1-\\pi))\\) These two functions are inverses of one another. 4.3.1 Interpretation Changes in predictors can be interpreted as changes in the odds of success (we can’t make general statement about changes in the probability of success). Specifically, we “unlog” the logit equation to get an expression for the odds of success for predictors \\(x_1, \\dotsc, x_p\\): \\[ odds(x_1, \\dotsc, x_p) = \\dfrac{\\pi(X)}{1-\\pi(X)} = e^{\\beta_0 + \\beta_1 x_{1} + \\dotsm + \\beta_p x_{p}} \\] What happens if we increase \\(x_1\\) by one unit, holding other predictors fixed? \\[ odds(x_1+1, \\dotsc, x_p) = e^{\\beta_0 + \\beta_1 (x_{1}+1) + \\dotsm + \\beta_p x_{p}} = e^{\\beta_0 + \\beta_1 x_{1} + \\dotsm + \\beta_p x_{p}} \\times e^{\\beta_1} \\] Increasing \\(x_1\\) by one unit has a multiplicative change of \\(e^{\\beta_1}\\) in the odds of success. Note that this is a similar interpretation to the exponential model in SLR or MLR. The multiplicative change of \\(e^{\\beta_1}\\) is also called the odds ratio for a one unit increase in \\(x_1\\). An odds ratio is just the ratio of the odds for two different groups, here groups with \\(x_1+1\\) vs. \\(x_1\\): \\[ \\dfrac{\\textrm{odds of succes at } x_1+1}{\\textrm{odds of succes at } x_1} = \\dfrac{odds(x_1+1, \\dotsc, x_p) }{odds(x_1, \\dotsc, x_p) } = e^{\\beta_1} \\] What is we have a predictor that is logged? \\[ odds(x_1, \\dotsc, x_p) = e^{\\beta_0 + \\beta_1 \\ln(x_{1}) + \\dotsm + \\beta_p x_{p}} = e^{\\beta_0}x_1^{\\beta_1} e^{\\beta_2 x_2 + \\dotsm + \\beta_p x_{p}} \\] Then our interpretation is similar to a power model. Changing \\(x_1\\) by a factor of \\(m\\): \\[ odds(mx_1, \\dotsc, x_p) = e^{\\beta_0}(mx_1)^{\\beta_1} e^{\\beta_2 x_2 + \\dotsm + \\beta_p x_{p}} = e^{\\beta_0}x_1^{\\beta_1} e^{\\beta_2 x_2 + \\dotsm + \\beta_p x_{p}} \\times m^{\\beta_1} \\] results in a multiplicative change of \\(m^{\\beta_1}\\) in the odds of success. 4.4 Inference and estimation Estimation of logistic model parameters \\(\\beta_0, \\dotsc, \\beta_p\\) is done using maximum likelihood estimation (MLE). The likelihood function is the probability of the observed data, writen as a function of our unknown \\(\\beta\\)’s *(which are used to compute \\(\\pi(X_i)\\)’s) \\[ L(\\beta) = \\prod_{i=1}^n \\pi(X_i)^{y_i} (1-\\pi(X_i))^{1-y_i} \\] Notice that for each case \\(i\\), the term in this product is equal to just \\(\\pi(X_i)\\) when case \\(i\\) is a success (\\(y_i=1\\)) and equal to \\(1-\\pi(X_i)\\) when case \\(i\\) is a failure (\\(y_i=0\\)). Our MLE method says to find the \\(\\beta\\)’s that maximize the likelihood \\(L(\\beta)\\) of the observed data. Unlike SLR or MLR, there is no “closed form” for these MLE \\(\\hat{\\beta}_i\\) estimates (meaning we can’t write down a formula for the estimates). Rather, software uses a numerical optimization method to compute the MLEs \\(\\hat{\\beta}_i\\) and the standard errors \\(SE(\\hat{\\beta}_i)\\). (The R function glm uses Iterative reweighted least squares.) These MLE estimates of \\(\\beta\\) parameters are approximately normally distributed and unbiased when n is “large enough.” Much like with “intro stats” inference, when your response variable is categorical (or, equivalently, binary 0/1), we usually use the normal distribution for inference (CI and tests). When your response variable is quantitative (like in SLR/MLR models), we usually use the t-distribution for inference. 4.4.1 Confidence intervals for \\(\\pmb{\\beta_i}\\) A \\(C\\)% confidence interval for \\(\\beta_i\\) equals \\[ \\hat{\\beta}_i \\pm z^*SE(\\hat{\\beta}_i) \\] where \\(z^*\\) is the \\((100-C)/2\\) percentile from the \\(N(0,1)\\) distribution. 4.4.2 Hypothesis tests for \\(\\pmb{\\beta_i}\\) We can test the hypothesis \\[ H_0: \\beta_i = \\beta^*_i \\] with the following z-test statistic: \\[ z =\\dfrac{\\hat{\\beta}_i - \\beta^*_i}{SE(\\hat{\\beta}_i)} \\] where \\(\\beta^*_i\\) is our hypothesized value of \\(\\beta_i\\) . The \\(N(0,1)\\) is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. The usual test results given by standard regression output tests whether a parameter value (intercept or slope) is equal to 0 vs. not equal to 0: \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with a test stat of \\[ z =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] 4.4.3 R glm We fit a logistic regression model in R with the glm function. The basic syntax is glm(y ~ x1 + x2, family = binomial, data= ) Careful not to forget the family=binomial argument! If you omit this, you will just be trying to fit a regular MLR model which is not appropriate for a categorical response. The variable y can be either form: y can be binary 0/1 coded response where 1 is a “success” y can be a factor variable with two levels. The second level is what R will call a “success” Once you fit a glm model, you can extract attributes of the model fitted(my.glm) gives the estimated probabilities of success for each case in your data predict(my.glm) gives estimated log-odds of success for each case in your data. Add newdata= to get predicted log-odds for new data. predict(my.glm, type = &quot;response&quot;) gives estimated probabilities of success for each case in your data. Add newdata= to get predicted log-odds for new data. The broom package also allows us to get fitted probabilities or log odds for all cases in the data, or for new data: augment(my.glm) gets estimated log-odds of success added to the variables used in the glm fit. add data=my.data to get estimated log-odds added to the full data set my.data used in the glm fit add newdata= new.data to get predicted log-odds added to the new data set new.data augment(my.glm, type.predict= &quot;response&quot;) gets estimated probabilities of success added to the variables used in the glm fit. add data=my.data to get estimated probabilities added to the full data set my.data used in the glm fit add newdata= new.data to get predicted probabilities added to the new data set new.data 4.4.4 Example: Donner party model Let’s revist the Donner party data and start with considering the logistic regression of survival status on age (only). We can add the fitted logistic model probability curve the scatterplot we created in Section 4.1.1. We use the glm smoothing method with an args that specifies the binomial family: &gt; ggplot(donner, aes(x = Age, y = Ind_surv)) + + geom_jitter(aes(color=Status), height = .01) + + geom_smooth(method=&quot;glm&quot;, method.args = list(family=binomial), se=FALSE) + + labs(y = &quot;probability of survival&quot;) We can see that an age of about 30 yields an estimated survival probability of 50% while an age of about 45 yields an estimated survival probability of 25%. We can better quantify these values by fitting the model using the glm function. Here is our simple model, the logistic regression of survival on age: \\[ logit(\\pi) = \\log(\\dfrac{\\pi}{1-\\pi}) = \\beta_0 + \\beta_1 Age \\] &gt; levels(donner$Status) # second level = Surived ## [1] &quot;Died&quot; &quot;Survived&quot; &gt; donner.glm1 &lt;- glm( Status ~ Age , family=binomial, data=donner) &gt; summary(donner.glm1) ## ## Call: ## glm(formula = Status ~ Age, family = binomial, data = donner) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5401 -1.1594 -0.4651 1.0842 1.7283 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.81852 0.99937 1.820 0.0688 . ## Age -0.06647 0.03222 -2.063 0.0391 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 61.827 on 44 degrees of freedom ## Residual deviance: 56.291 on 43 degrees of freedom ## AIC: 60.291 ## ## Number of Fisher Scoring iterations: 4 &gt; confint(donner.glm1) ## 2.5 % 97.5 % ## (Intercept) -0.005987258 3.99016010 ## Age -0.139737905 -0.01016096 The estimated log odds of survival is \\[ logit(\\hat{\\pi}) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = 1.81852 -0.06647 Age \\] and the estimated odds of survival is \\[ \\hat{odds}(age) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = e^{1.81852}e^{-0.06647 Age} \\] and the estimated probability of survival is \\[ \\hat{\\pi}(age) = \\dfrac{e^{1.81852 -0.06647 Age}}{1+e^{1.81852 -0.06647 Age}} \\] A one year increase in age will have a \\(e^{-0.06647} = 0.936\\) multiplicative change on the odds of survival. A one year increase in age decreases the odds of survival by 6.4% (95% CI 0.3% to 12.2%). &gt; exp(-0.06647) # factor change ## [1] 0.935691 &gt; 100*(exp(-0.06647) - 1) # percent change ## [1] -6.430901 &gt; exp(-0.06647 + c(-1,1)*qnorm(0.975)*0.03222) # factor change CI ## [1] 0.8784291 0.9966855 &gt; 100*(exp(-0.06647 + c(-1,1)*qnorm(0.975)*0.03222) - 1) # % change CI ## [1] -12.1570864 -0.3314455 The broom package’s tidy function can also be used to get estimates, SEs and confidence intervals. If we add exponentiate=TRUE, then we we get exponentiated estiamtes and confidence intervals (but SE, test stat and p-values are untouched). &gt; library(broom) &gt; tidy(donner.glm1, conf.int=TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 1.82 0.999 1.82 0.0688 -0.00599 3.99 ## 2 Age -0.0665 0.0322 -2.06 0.0391 -0.140 -0.0102 &gt; tidy(donner.glm1, conf.int=TRUE, exponentiate = TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.16 0.999 1.82 0.0688 0.994 54.1 ## 2 Age 0.936 0.0322 -2.06 0.0391 0.870 0.990 We can use the predict command with response type values to get predicted survival rates for 30 and 45 year olds: &gt; new.ages &lt;- data.frame(Age = c(30, 45)) &gt; predict(donner.glm1, newdata = new.ages, type=&quot;response&quot;) ## 1 2 ## 0.4562149 0.2363774 &gt; exp(1.81852 - 0.06647*30)/(1+exp(1.81852 - 0.06647*30)) # prob age=30 ## [1] 0.4562174 &gt; exp(1.81852 - 0.06647*45)/(1+exp(1.81852 - 0.06647*45)) # prob age=45 ## [1] 0.2363799 So we have \\[ \\hat{\\pi}(age = 30) = \\dfrac{e^{1.81852 -0.06647(30)}}{1+e^{1.81852 -0.06647(30)}} \\approx 0.456 \\] and \\[ \\hat{\\pi}(age = 45) = \\dfrac{e^{1.81852 -0.06647(45)}}{1+e^{1.81852 -0.06647(45)}} \\approx 0.236 \\] Finally, what if we want to understand how the odds of death change as a function of age? Well, odds of death is equal to the ratio death to survival probabilities: \\[ \\hat{odds.death}(age) = \\dfrac{1-\\hat{\\pi}}{\\hat{\\pi}} = \\dfrac{1}{e^{1.81852}e^{-0.06647 Age}} = e^{-1.81852}e^{0.06647 Age} \\] A one year increase in age will have a \\(e^{0.06647} = 1.069\\) multiplicative change on the odds of survival. A one year increase in age increases the odds of death by 6.9% (95% CI 0.3% to 13.8%). &gt; exp(0.06647) # factor change in odds of death ## [1] 1.068729 &gt; 100*(exp(0.06647) - 1) # percent change ## [1] 6.87289 &gt; exp(0.06647 + c(-1,1)*qnorm(0.975)*0.03222) # factor change CI ## [1] 1.003325 1.138396 &gt; 100*(exp(0.06647 + c(-1,1)*qnorm(0.975)*0.03222) - 1) # % change CI ## [1] 0.3325478 13.8395756 We can verify our mathematical work by refitting a glm with an indicator of death: &gt; donner$Ind_death &lt;- recode(donner$Status, Survived = 0, Died = 1) &gt; tidy(glm(Ind_death ~ Age, family = binomial, data=donner)) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -1.82 0.999 -1.82 0.0688 ## 2 Age 0.0665 0.0322 2.06 0.0391 4.4.5 Example: Donner party, adding sex We will fit the logistic regression of survival on age and sex: \\[ logit(\\pi) = \\log(\\dfrac{\\pi}{1-\\pi}) = \\beta_0 + \\beta_1 Age + \\beta_2 Male \\] The estimated mode fit is: &gt; donner.glm2 &lt;- glm( Status ~ Age + Sex, family=binomial, data=donner) &gt; donner.glm2 ## ## Call: glm(formula = Status ~ Age + Sex, family = binomial, data = donner) ## ## Coefficients: ## (Intercept) Age SexMale ## 3.2304 -0.0782 -1.5973 ## ## Degrees of Freedom: 44 Total (i.e. Null); 42 Residual ## Null Deviance: 61.83 ## Residual Deviance: 51.26 AIC: 57.26 &gt; tidy(donner.glm2, conf.int=TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.23 1.39 2.33 0.0198 0.851 6.43 ## 2 Age -0.0782 0.0373 -2.10 0.0359 -0.162 -0.0141 ## 3 SexMale -1.60 0.755 -2.11 0.0345 -3.23 -0.195 &gt; tidy(donner.glm2, conf.int=TRUE, exponentiate = TRUE) ## # A tibble: 3 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 25.3 1.39 2.33 0.0198 2.34 618. ## 2 Age 0.925 0.0373 -2.10 0.0359 0.850 0.986 ## 3 SexMale 0.202 0.755 -2.11 0.0345 0.0396 0.823 The estimated log odds of survival is \\[ logit(\\hat{\\pi}) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = 3.23041 -0.07820 Age -1.59729 Male \\] and the estimated odds of survival is \\[ odds(Sex, Age) = \\dfrac{\\hat{\\pi}}{1-\\hat{\\pi}} = e^{3.23041}e^{-0.07820 Age}e^{-1.59729 Male} = (25.3)(0.925)^{Age}(0.202)^{Male} \\] and the estimated probability of survival is \\[ \\hat{\\pi}(Sex, Age) = \\dfrac{e^{3.23041 -0.07820 Age -1.59729 Male}}{1+e^{3.23041 -0.07820 Age -1.59729 Male}} \\] The exponentiated coefficient estimates give the odds ratios for a one unit increase in age or for males (compated to females). Holding gender constant, a one year increase in age decreases the odds of survival by 7.5% (95% CI 0.5% to 14.0%). &gt; exp(-0.07820) # age effect on odds ## [1] 0.9247795 &gt; 100*(exp(-0.07820) - 1) # % change ## [1] -7.522055 &gt; exp(-0.07820 + c(-1,1)*qnorm(0.975)*0.03728) # CI for factor ## [1] 0.8596178 0.9948806 &gt; 100*(exp(-0.07820 + c(-1,1)*qnorm(0.975)*0.03728) - 1) # CI for % change ## [1] -14.0382243 -0.5119394 Holding age constant, males had a 79.8% lower odds of survival compared to females (95% CI 11.0% to 95.4%). \\[ \\textrm{estimated odds ratio of survival for males vs females} = \\dfrac{\\hat{odds}(Sex=male,Age)}{\\hat{odds}(Sex=female,Age)} = e^{-1.5973} \\] &gt; exp(-1.59729) ## [1] 0.2024444 &gt; 100*(exp(-1.59729) -1 ) ## [1] -79.75556 &gt; exp(-1.59729 + c(-1,1)*qnorm(0.975)*0.75547) ## [1] 0.0460520 0.8899447 &gt; 100*(exp(-1.59729 + c(-1,1)*qnorm(0.975)*0.75547) - 1) ## [1] -95.39480 -11.00553 What if we wanted the odds ratio for comparing females to males? \\[ \\textrm{estimated odds ratio of survival for females vs males} = \\dfrac{\\hat{odds}(Sex=female,Age)}{\\hat{odds}(Sex=male,Age)} = \\dfrac{1}{e^{-1.5973}} = e^{1.5973} \\] Holding age constant, females had a 4.9-fold increased odds of survival (95% CI 1.1 to 21.7). &gt; # odds ratio Female/Male &gt; 1/exp(-1.59729) ## [1] 4.939628 &gt; 1/exp(-1.59729 + c(-1,1)*qnorm(0.975)*0.75547) ## [1] 21.714581 1.123665 There are two separate log-odds, odds and probability functions for the two levels of Sex in the model. Since we do not have an interaction between Sex and Age, we have a parallel line log-odds model. To plot the probabilities for each Sex as Age varies, we need to get the fitted probabities for each case in the data set: &gt; library(broom) &gt; donner.aug2 &lt;- augment(data=donner, donner.glm2, type.predict = &quot;response&quot;) &gt; head(donner.aug2) ## # A tibble: 6 x 12 ## Age Sex Status Ind_surv Ind_death .fitted .se.fit .resid .hat ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23 Male Died 0 1 0.459 0.111 -1.11 0.0492 ## 2 40 Fema~ Survi~ 1 0 0.526 0.160 1.13 0.103 ## 3 40 Male Survi~ 1 0 0.183 0.0921 1.84 0.0567 ## 4 30 Male Died 0 1 0.329 0.0924 -0.893 0.0387 ## 5 28 Male Died 0 1 0.364 0.0949 -0.952 0.0389 ## 6 40 Male Died 0 1 0.183 0.0921 -0.636 0.0567 ## # ... with 3 more variables: .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; Then add geom_line with the .fitted probabilities for each Sex (by linetype) added to the y-axis of the jittered scatterplot: &gt; ggplot(donner.aug2, aes(x = Age, y = Ind_surv)) + + geom_jitter(aes(color=Status), height = .01) + + geom_line(aes(y=.fitted, linetype=Sex)) + + labs(y = &quot;probability of survival&quot;) Note that these lines are not parallel on the probability scale while they are on the log-odds scale. This is because we apply the logistic function to the log-odds to produce our probabilities. The logistic function is not a linear function, so it does not produce parallel lines (or even lines at all). "]
]
