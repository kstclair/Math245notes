[
["index.html", "Math 245 Notes Getting Started Getting help", " Math 245 Notes Katie St. Clair, Carleton College 2019-10-06 Getting Started This manual is designed to provide students in Math 245 (Applied Regression Analysis) with supplemental notes and examples. These notes are not intended to replace notes that you take by hand or computer during class or when reading assigned material. Most examples/activities will have an .Rmd available that you can download to use as a template for completing the activity. Save these files to your class project folder. Getting help Try to start your homework or projects early enough to seek help from me! You can do this in a variety of ways: Office visit: Stop by office hours or drop by anytime my door is open. This is the easiest way for me to answer any class questions that you have or help you debug troublesome R code. Email: I can usually clarify any homework questions over email. If you are running in an R problem, you should snapshot your R code and error and send it to me via email. I can often figure out your code issue this way! Stats Lab: Visit the stats lab assistants in CMC 201 for help using R. "],
["review.html", "Chapter 1 Review of Statistical Inference 1.1 Sampling Distribution 1.2 Central Limit Theorem 1.3 Hypothesis testing 1.4 Confidence Intervals 1.5 Review activity (day 2)", " Chapter 1 Review of Statistical Inference 1.1 Sampling Distribution Suppose we take a random sample of size \\(n\\) from a large population of individuals. We record a variable and compute a statistic like a sample mean or sample proportion from this data. The sampling distribution of the statistic describes how the stat is distributed over many random samples: take a random sample of size \\(n\\) record the variable of interest (data) for the \\(n\\) sampled cases compute the statistic from the data repeat 1-3 many, many times plot the distribution of your statistics from step 4. 1.2 Central Limit Theorem Under the right conditions, the distribution of many sample means \\(\\bar{x}\\) or proportions \\(\\hat{p}\\) will look like a normal distribution that is centered at the true population parameter (mean \\(\\mu\\) or proportion \\(p\\)) value. Shape: normally distributed (“bell-shaped”) Center: true population parameter value Variation: called the standard error of the statistic which measures the standard deviation of your statistic over many samples The conditions for “normality” (symmetry) have to do with both the sample sized and the distribution of your variable. If your measurements are very symmetric (e.g. heights of woman in an adult population) then the sample size \\(n\\) can be very small and the sample mean will behave normally. As your measurements get more skewed (e.g. income per person in a large city), then the sample size \\(n\\) needs to get larger for the sample mean to behave normally. Outliers are always a problem, even if you have a large sample size, since means can be easily influenced by one or two very unusual cases. 1.2.1 Standard error The standard error of the sample mean \\(\\bar{x}\\) measures the variability of this statistic and is the standard deviation of the sampling distribution for \\(\\bar{x}\\). Roughly, the SE of any statistic tells us how it will vary from sample to sample. For the sample mean statistic, the estimated SE is equal to \\[ SE(\\bar{x}) = \\dfrac{s}{\\sqrt{n}} \\] where \\(s\\) is the sample standard deviation of your variable. For the difference of two sample means, \\(\\bar{x}_1 - \\bar{x}_2\\), the standard error is \\[ SE(\\bar{x}_1 - \\bar{x}_2) = \\sqrt{\\dfrac{s^2_1}{n_1} + \\dfrac{s^2_2}{n_2}} \\] where \\(n_i\\) is the sample size of sample \\(i\\) and \\(s_i\\) is the sample SD of population \\(i\\). For the sample proportion \\(\\hat{p}\\), the standard error is \\[ SE(\\hat{p}) = \\sqrt{\\dfrac{p(1-p)}{n}} \\] where \\(p\\) is the true population proportion. To estimate this SE we just use the sample proportion \\(\\hat{p}\\) in the SE formula. 1.3 Hypothesis testing Your null hypothesis \\(H_0\\) is a specific claim about a population parameter of interest. Examples include: \\(H_0: p = 0.5\\) (population proportion is 0.50, or 50%) \\(H_0: \\mu_1 - \\mu_2 = 0\\) (the mean of population 1 \\(\\mu_1\\) is equal to the mean of population 2 \\(\\mu_2\\)) The alternative hypothesis specifies another, more general, scenario for the population(s). Examples include: \\(H_0: p &gt; 0.5\\) (population proportion greater than 0.50) \\(H_0: \\mu_1 - \\mu_2 \\neq 0\\) (the two populations have difference means) You then use a test statistic to measure how far the data (e.g. sample statistic) deviates from what is expected assuming the null is true. Often these test stats take form of a standardized values so large absolute values indicate data that deviates a lot from what is expected if the null is true. For a hypothesis about one population mean \\(H_0: \\mu = \\mu_0\\), we use a t-test statistic: \\[ t = \\dfrac{\\bar{x} - \\mu_0}{s/\\sqrt{n}} \\] To test for a difference in two means, we use another t-test stat: \\[ t = \\dfrac{(\\bar{x}_1 - \\bar{x}_2) - 0}{\\sqrt{\\dfrac{s^2_1}{n_1} + \\dfrac{s^2_2}{n_2}}} \\] We then construct the sampling distribution of the test statistic under the assumption that the null hypothesis is true. Using this model, we compute a p-value by finding the probability of getting a test stat as, or more, extreme as the observed test stat. The p-value measures the probability of observing data as unusual, or more usual, than the observed data if the null is true. Small p-values indicate data that would rarely be seen if the null is true which means we have evidence (data) that supports the alternative hypothesis. If we use a t-test statistic, then we use a t-distribution to compute a p-value. If t is the test stat value then either command below will give a one-sided (\\(&lt;\\) or \\(&gt;\\)) p-value: &gt; pt(abs(t), df = , lower.tail = FALSE) # method 1 (gives right tail value) &gt; pt(-abs(t), df = ) # method 2 (gives left tail value) If your alternative is two-sided, you need to double the value given by either command above. The degrees of freedom depends on the type of estimate: for a one-sample mean problem, use \\(df = n-1\\) for a two-sample difference in means problem, let technology get the value! 1.4 Confidence Intervals A confidence interval for a population parameter gives us a range of likely values of the parameter. Most confidence intervals we use in this class are of the form: \\[ \\textrm{estimate} \\pm \\textrm{ margin of error} \\] The idea behind constructing a confidence interval starts with our estimate’s (statistic’s) sampling distribution and margin of error for some level of confidence: The sampling distribution tells us how an estimate varies from sample to sample around the true population parameter. The margin of error for a “C%” confidence interval is computed so that the estimate will be within the margin of error of the true parameter value C% of the time. Another “confidence” interpretation: of all possible samples, C% will yield a confidence interval (using the C% margin of error) that contains the true parameter value. Examples: E.g. for 95% confidence: 95% of all possible samples will give an estimate that is within the (95% level) margin of error of the truth. E.g. for 90% confidence: 90% of all possible samples will give 90% confidence interval that contains the truth. Normally distributed estimates: when a sampling distribution is roughly normally distributed, we can approximately say that 95% margin of error \\(\\approx 1.96 \\times SE\\) 90% margin of error \\(\\approx 1.65 \\times SE\\) 99% margin of error \\(\\approx 2.58 \\times SE\\) A higher level of confidence will lead to a larger margin of error: We need a larger margin of error to get a higher fraction of samples that close to the population parameter. The margin of errors given above are ballpark values. We will mostly be using a more trustworthy distribution in our class, the t-distribution, to compute how many SE’s we go to be a certain level of confidence: \\[ \\textrm{margin of error } = t^*_{df} \\times SE \\] where \\(t^*\\) is the \\((100-C)/2\\) percentile from the t-distribution with \\(df\\) degrees of freedom. Examples of confidence levels: for 95% confidence, \\(t^*\\) is the 2.5% percentile (or 97.5%) &gt; qt(.025, df= ) # for 95% confidence for 90% confidence, \\(t^*\\) is the 5% percentile (or 95%) &gt; qt(.05, df= ) # for 90% confidence Examples of degrees of freedom: for a one-sample mean problem, use \\(df = n-1\\) for a two-sample difference in means problem, let technology get the value! 1.5 Review activity (day 2) A Markdown of this activity with some code included is linked here. 1.5.1 General questions What is the difference between a parameter and a statistic? Give an example of a parameter and a statistic, using “common” notation for each. What is a random sample? What is statistical inference? Suppose you are told a 95% confidence interval for the proportion of registered voters for Trump just before the 2016 election is 0.38 to 0.44. What does this mean? What is the margin of error for this CI? What does “95% confidence” mean? A study found that participants who spent money on others instead of themselves had significantly lower blood pressure (two-sided p-value=0.012). What are the hypotheses for this test? What does the p-value mean? Inference methods rely on understanding the sampling distribution of the statistic that we used to estimate our unknown parameter. What does the sampling distribution of the sample mean look like? What does the standard error of the sample mean measure? How is the sampling distribution used in a hypothesis test? How is it used with confidence intervals? 1.5.2 Comparing two means The data set agstrat.csv contains a stratified random sample of 300 US counties Sampling units: counties Strata: region (North Central, Northeast, South, West) response: number of farms in 1992 (farms92) How can we answer the question: Do the average number of farms per county differ in the western and north central regions in 1992? Our basic plan for analysis looks like: Data: load and clean/manipulate (if needed) EDA: exploratory data analysis Inference: run tests/CIs and check assumptions! Conclusion: interpret results, avoid lots of stats jargon! 1.5.2.1 Data Read in the data and check variable names: &gt; agstrat&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/agstrat.csv&quot;) &gt; names(agstrat) (1a) How many counties were sampled from each region? &gt; table(agstrat$region) We want to focus on W and NC for our comparison. I will do a lot of data manipulation in this class using the R package dplyr. This package is already available on the mirage server, but if you are using a newly installed version of Rstudio on your computer you will need to install the package first. (See Appendix A.4). The filter command from the dplyr package selects rows from a data set that we want to keep (Section C.2.4). E.g. only rows from the W or NC regions. The argument used in this command is a logical statement and the row of a given case is kept in the data set when the statement is TRUE. &gt; library(dplyr) # need to load the package commands &gt; agstrat2 &lt;- filter(agstrat, region %in% c(&quot;W&quot;, &quot;NC&quot;)) &gt; table(agstrat2$region) Notice that filter it does not drop the unused levels (S and NE) from the region factor variable. We can use droplevels on the new data frame agstrat2 to get rid of these unused levels. We assign this new and improved data set the name agstrat2 again. (This overwrites the previous version without the dropped levels.) &gt; agstrat2 &lt;- droplevels(agstrat2) &gt; table(agstrat2$region) 1.5.2.2 EDA (2a) In 2-3 sentences, compare the distributions (shape, center, spread) of farms92 by region (W vs. NC). Note that the commands below give basic R commands for comparing distributions graphically and numerically. You do not necessarily need to use or show all output below when answering this type of question for homework, exams or reports. Graphical options (Section C.3.4) include boxplots: &gt; boxplot(farms92 ~ region, data = agstrat2, horizontal=TRUE, main=&quot;Number of farms (1992) by region&quot;, + ylab = &quot;region&quot;, xlab = &quot;# of farms per county in 1992&quot;) or faceted histograms from the ggplot2 package: &gt; library(ggplot2) # load ggplot2 package &gt; ggplot(agstrat2, aes(x=farms92)) + + geom_histogram() + + facet_wrap(~region) + + labs(title = &quot;Number of farms by region&quot;) Summary stats, by region, can be found using multiple tools in R (Section C.3.3). The tapply command is an option: &gt; tapply(agstrat2$farms92, agstrat2$region, summary) So is using the group_by and summarize combination in dplyr. Here we get the mean and SD of farms92 for both regions: &gt; agstrat2 %&gt;% + group_by(region) %&gt;% + summarize(mean(farms92), sd(farms92)) (2b) What county has the unusually high number of farms in the western region? How many farms do they have? Any ideas why it is so large? How do the summary stats change when this county is omitted? Here we find which row in agstrat2 has a farms92 value greater than 3000 (Section C.2.4): &gt; which(agstrat2$farms92 &gt; 3000) What county is this? We need to access row 118 in agstrat2. We use use [] subsetting or the slice command from dplyr: &gt; agstrat2[118,] # see row 118 &gt; slice(agstrat2, 118) # another way to see row 118 We can use the dplyr commands for (2a) with the slice command added to get stats without row 118 &gt; agstrat2 %&gt;% slice(-118) %&gt;% # slice removes row 118 + group_by(region) %&gt;% # then get stats by region + summarize(mean(farms92), sd(farms92), median(farms92), IQR(farms92)) 1.5.2.3 Inference (3a) What hypotheses are being tested with the t.test command below? What are the p-value and test statistics values for this test? How do you interepret the test statistic? How do you interpret the p-value? Are the assumptions met for this t-test? &gt; # run t test to compare mean number of farms &gt; t.test(farms92 ~ region, data = agstrat2) (3b) How and why do the test stat and p-value change when we omit row 118? Does your test conclusion change when omitting 118? Why? &gt; # redo without row 118 outlier &gt; t.test(farms92 ~ region, subset = -118, data = agstrat2) 1.5.2.4 Conclusion (4a) Do the average number of farms per county differ in the western and north central regions in 1992? If they do differ, by how much? Answer these question in context using numbers from the results above to support your conclusions. "],
["slr.html", "Chapter 2 Simple Linear Regression 2.1 The variables 2.2 The model form 2.3 Theory: Estimation 2.4 SLR model simulation 2.5 Inference for mean parameters 2.6 Inference for average or predicted response 2.7 Example: SLR model (day 3) 2.8 Checking model assumptions and fit 2.9 Example: SLR assumptions (day 4/5) 2.10 Transformations 2.11 Examples: Transformations (day 6) 2.12 \\(R^2\\) and ANOVA for SLR", " Chapter 2 Simple Linear Regression This chapter contains content from Sleuth Chapters 7 and 8. 2.1 The variables Suppose we have a quantitative response variable \\(y\\) that we want to relate to an explantory (aka predictor) variable \\(x\\). For now, we will assume that \\(x\\) is also quantitative. 2.2 The model form This section describes the SLR model for a particular population of interest. Another way to frame the model is that it describes a hypothetical data generating process (DGP) that was used to generate the sample of data that we have on hand. Let \\(Y_i\\) be the response from unit \\(i\\) that has explanatory (aka predictor) value \\(x_i\\). There are two equivalent ways to express the SLR model for \\(Y\\): Conditional normal model: Given a value \\(x_i\\), the response \\(Y_i\\) follows a normal model with mean and SD given below: \\[ Y_i \\mid x_i \\sim N(\\mu_{y\\mid x} = \\beta_0 + \\beta_1 x_i, \\sigma) \\] Mean + error: Statisticians are more likely to use a model specification that expresses \\(Y\\) as a function of the expected value/mean of \\(Y\\) plus an error term that models variation in responses around the mean: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\end{equation}\\] Both expressions of the SLR model above say the same thing: Linear Mean: \\(\\mu_{y\\mid x} = E(Y \\mid x) = \\beta_0 + \\beta_1 x\\) describes the population mean value of \\(Y\\) given a predictor value \\(x\\). This mean value varies linearly with \\(x\\) and the population parameters are \\(\\beta_0\\) and \\(\\beta_1\\). Constant SD: \\(SD(Y\\mid x)=\\sigma\\) describes the SD of \\(Y\\)’s in the population around a given mean value \\(\\mu_{y\\mid x}\\). The fact that this SD does not depend on the value of \\(x\\) is called the contant variance, or homoscedastic, assumption. Normality: The shape of population response values around \\(\\mu_{y\\mid x}\\) is described by a normal distribution model. Finally, one last assumption is made for the SLR model: Indepedence: Given a predictor value of \\(x\\), all responses \\(Y\\) occur independently of each other. There are a total of three parameters in the SLR model: the two mean parameters \\(\\beta_0\\) and \\(\\beta_1\\) the SD parameter \\(\\sigma\\) 2.2.1 Interpretation For a SLR model: \\(\\beta_0\\) is the mean response when the predictor value is 0 since \\(\\mu_{y \\mid 0} = \\beta_0 + \\beta_1(0) = \\beta_0\\). \\(\\beta_1\\) tells us how the mean response changes for a one unit increase in \\(x\\) 2.2.2 Example: Woodpecker nests We want to model nest depth (cm) in a tree cavity as a function of ambient air temperature (Celsius). Our SLR model for this relationships says that, given an ambient air temp \\(x_i\\), a randomly selected nest will have a depth \\(Y_i\\) that is modeled as \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\] This means that depth is normally distributed with a mean of \\(\\mu_{depth \\mid temp} = \\beta_0 + \\beta_1 (temp)\\) and a SD of \\(\\sigma\\). 2.3 Theory: Estimation Let’s consider taking a random sample of size \\(n\\) of responses and predictor values from our population (or DGP) for which the SLR model holds: \\((x_1, Y_1), \\dotsc, (x_n, Y_n)\\). The notation here, \\((x_i, Y_i)\\) implies that the predictor value \\(x_i\\) is fixed, but \\(Y_i\\) is a random variable that is generated from the SLR model described in Section 2.2. The estimation problem is that we need to use the sample of size \\(n\\) to estimate the SLR model parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\). Once we observe a sample of size \\(n\\), then we can use the SLR model to determine the probability of observing the sample. This probability, which depends on the actual model parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma\\), is called a likelihood function. We plug the observed data into this function, then find the parameters values that maximize the function using calculus. For a SLR model, this process yields the following maximum likelihood estimators (MLE) of our parameters: \\[ \\hat{\\beta}_1 = \\dfrac{\\sum_{i=1}^n (x_i - \\bar{x})(Y_i - \\bar{Y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\ \\ \\ \\ \\ \\hat{\\beta}_0 = \\bar{Y} - \\bar{\\beta}_1 \\bar{x} \\ \\ \\ \\ \\ \\hat{\\sigma} = \\sqrt{\\dfrac{\\sum_{i=1}^n (Y_i - \\hat{y}_i)^2}{n-2}} \\] where \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\\) is the predicted value of \\(y\\) given the value \\(x_i\\). 2.3.1 Sampling Distributions for SLR estimates The sampling distribution of a model estimate (\\(\\hat{\\beta}_0\\) or \\(\\hat{\\beta}_1\\)) is constructed by: fix a set of predictor values: \\(x_1, \\dotsc, x_n\\) for each fixed \\(x_i\\), generate a response \\(Y_i\\) from \\(N(\\beta_0 + \\beta_1 x_i, \\sigma)\\) compute the MLE’s \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) from the observed sample from (2) repeat 2-3 lots of times, then the distribution of the estimates from part (3) show the sampling distribution of the slope or intercept estimate. Using probability theory, we can show that the sampling distributions of both \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are approximately normal when \\(n\\) is “large enough” (thanks to the CLT): \\[ \\hat{\\beta}_i \\sim N(\\beta_i, SD(\\hat{\\beta}_i)) \\] Unbiased: We see that the expected value (mean) of \\(\\hat{\\beta}_i\\) is the parameter \\(\\beta_i\\), meaning it is an unbiased estimator. (It doesn’t systematically over- or under-estimate the parameter of interest.) Standard error: We end up estimating the SD in the sampling distribution given above. The SEs for each mean parameter estimate are \\[ SE(\\hat{\\beta}_1) = \\hat{\\sigma}\\sqrt{\\dfrac{1}{(n-1)s^2_x}} \\ \\ \\ \\ \\ SE(\\hat{\\beta}_0) = \\hat{\\sigma}\\sqrt{\\dfrac{1}{n} + \\dfrac{\\bar{x}^2}{(n-1)s^2_x}} \\] 2.4 SLR model simulation Download the Markdown of this activity: .Rmd. 2.4.1 Simulation function This chunk contains code that defines our function reg.sim that simulates \\(n\\) responses from a given regression model and given set of \\(n\\) predictor values \\(x\\). I’ve excluded it from our compiled document so see the .Rmd file to take a look at how this was created. 2.4.2 Run the function once Let’s use the function from (1) above. We will use the \\(n=12\\) temps (x-values) from the woodpeckers data and assume that the true model is: \\[\\mu(y \\mid x) = 20 - 0.4x\\] (red line below) with \\(\\beta_0=20\\), \\(\\beta_1=-0.4\\), and \\(\\sigma=2\\). In the code below I use the set.seed() command to “fix” the random number generator so I get the same answer each time this is run (so my answer in the handout is reproduced each time this file is compiled). &gt; wpdata&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; set.seed(77) &gt; reg.sim(x=wpdata$temp, beta0=20, beta1=-.4, sigma=2) ## $b0 ## [1] 20.8893 ## ## $b1 ## [1] -0.4682368 For this simulated sample (with seed of 77), the estimated regression line is \\(\\hat{\\mu}(y \\mid x) = 20.889 - 0.468x\\) (black line). 2.4.3 Simulated sampling distribution for \\(\\hat{\\beta}_1\\) We will now use the replicate command to generate 1000 different samples which create 1000 different estimates of \\(\\beta_1\\). A histogram of these estimates simulates the sampling distribution of estimated slope. What is the shape of the sampling distribution? Where is the distribution centered? How variable are these estimated slopes. &gt; set.seed(7) # just makes simulation reproducible &gt; slopes&lt;- replicate(1000,reg.sim(x=wpdata$temp, beta0=20, beta1=-.4, sigma=2,grph=F)$b1) &gt; hist(slopes); abline(v=-0.4,col=&quot;red&quot;, lwd=2) &gt; mean(slopes); sd(slopes) ## [1] -0.3974248 ## [1] 0.05227315 2.4.4 Are slope and intercept estimates correlated? In regression, it is not unusual to be interested in estimating a linear combination of our model parameters. An easy example of such a combination of parameters is the mean response for a given value \\(x_0\\) of the predictor: \\[ \\mu(y \\mid x=x_0) = \\beta_0 + \\beta_1x_0 \\] For a specific example, we may want to estimate the mean response (depth) for a temp of \\(x_0=5\\) degrees: \\(\\mu(y \\mid x=5) = \\beta_0 + \\beta_1 5\\). If we don’t know \\(\\beta_0\\) and \\(\\beta_1\\), then this mean parameter is a linear combination of two unknown parameters which we need to estimate. The natural estimate of this is just the estimated mean response: \\(\\hat{\\mu}(y \\mid x=x_0)=\\hat{\\beta}_0 + \\hat{\\beta}_1x_0\\). To assess how variable this estimate is (i.e. to get its SE) we need to understand how (if) the individual estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are correlated. We can use a simulation to look at this issue by generating 1000 samples from our model and plotting each \\((\\hat{\\beta}_0,\\hat{\\beta}_1)\\) pair for each sample. How are estimated slope and intercept associated? Any ideas why? &gt; set.seed(7) # this seed MUST match the seed used to get slopes! &gt; intercepts&lt;- replicate(1000,reg.sim(x=wpdata$temp, beta0=20, beta1=-.4, sigma=2,grph=F)$b0) &gt; plot(intercepts,slopes); abline(h=-.4,col=&quot;red&quot;); abline(v=20,col=&quot;red&quot;) &gt; title(&quot;Estimated slopes and intercepts&quot;) &gt; cor(intercepts, slopes) # correlation between estimates ## [1] -0.6928231 &gt; cov(intercepts, slopes) # covariance between estimates ## [1] -0.02869504 2.5 Inference for mean parameters In intro stats, you used \\(t\\) inference procedures for inference about population means since: (1) the sampling distribution of the sample mean was normally distributed and (2) we had to estimate its variability with a SE. The same goes for inference about the mean response parameters in a SLR model: \\[ t = \\dfrac{\\hat{\\beta}_i - \\beta_i}{SE(\\hat{\\beta}_i)} \\sim t_{df=n-2} \\] Use a t-distribution with \\(n-2\\) degrees of freedom for inference about the mean parameters. The degrees of freedom are calculated as the sample size \\(n\\) minus the number of terms in \\(\\mu_{y \\mid x}\\) that you have to estimate with the data. 2.5.1 Confidence Intervals To estimate either mean parameter with \\(C\\)% confidence, we have the general form \\[ \\hat{\\beta}_i \\pm t^* SE(\\hat{\\beta}_i) \\] where \\(t^*\\) is the \\((100-C)/2\\) percentile from the t-distribution with \\(df=n-2\\) degrees of freedom. 2.5.2 Hypothesis tests We can test the hypothesis \\[ H_0: \\beta_i = \\beta^*_i \\] with the following t-test statistic: \\[ t =\\dfrac{\\hat{\\beta}_i - \\beta^*_i}{SE(\\hat{\\beta}_i)} \\] where \\(\\beta^*\\) is our hypothesized value of \\(\\beta\\) (intercept or slope). The t-distribution with \\(n-2\\) degrees of freedom is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. The usual test results given by standard regression output tests whether a parameter value (intercept or slope) is equal to 0 vs. not equal to 0: \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with a test stat of \\[ t =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] 2.6 Inference for average or predicted response 2.6.1 Confidence intervals for \\(\\mu_{y \\mid x}\\) Here we are interested in estimating not just a \\(\\beta\\) with confidence, but we want to estimate the mean response for a given value of \\(x_0\\). For example, suppose we want to estimate the mean nest depth when the temp is \\(x_0=8\\) degrees. This mean parameter of interest is then: \\[ \\mu_{depth \\mid temp=8} = \\beta_0 + \\beta_1 (8) \\] Our parameter of interest is \\(\\mu_{y \\mid x_0} = \\beta_0 + \\beta_1 x_0\\) where \\(x_0\\) is known and \\(\\beta\\)’s need to be estimated. The natural estimator is just the fitted equation: \\[ \\hat{\\mu}_{y \\mid x_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 \\] As will any estimator, we can measure the variability of this estimator with a SE: \\[ SE(\\hat{\\mu}_{y \\mid x_0}) = \\hat{\\sigma} \\sqrt{\\dfrac{1}{n} + \\dfrac{(x_0 - \\bar{x})^2}{(n-1)s^2_x}} \\] Note! This SE depends on \\(\\pmb{x_0}\\)! It is miminized when \\(x_0\\) equals the mean predictor value \\(\\bar{x}\\) and it grows as \\(x_0\\) gets further from \\(\\bar{x}\\). Estimation is most precise in the “middle” of the predictor range and becomes less precise at the “edges” (where we usually have less data). A 95% confidence interval for the mean response \\(\\mu_{y \\mid x_0}\\) looks like \\[ \\hat{\\mu}_{y \\mid x_0} \\pm t^*_{df=n-2}SE(\\hat{\\mu}_{y \\mid x_0}) \\] 2.6.2 Prediction intervals for new cases To predict one individual’s future response \\(Y\\) for the predictor value \\(x_0\\), we just use the fitted equation: \\[ pred_{y \\mid x_0} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_0 \\] (We use \\(pred_{y \\mid x_0}\\) to remind us which predictor value as used for prediction.) Recall that the SLR model assumes the \\(Y\\) values when \\(x=x_0\\) are normally distributed with mean \\(\\mu_{y \\mid x_0}\\) and SD \\(\\sigma\\). The SE of this prediction at \\(x_0\\) takes into account (1) uncertainty in estimating the mean \\(\\mu_{y \\mid x_0}\\) and (2) variation in \\(Y\\)’s around the mean response (\\(\\sigma\\)): \\[ SE(pred_{y \\mid x_0}) = \\hat{\\sigma} \\sqrt{1 + \\dfrac{1}{n} + \\dfrac{(x_0 - \\bar{x})^2}{(n-1)s^2_x}} = \\sqrt{\\hat{\\sigma}^2 + SE(\\hat{\\mu}_{y \\mid x_0})^2 } \\] A 95% prediction interval for a future individual response at \\(x=x_0\\) looks like \\[ pred_{y \\mid x_0} \\pm t^*_{df=n-2}SE(pred_{y \\mid x_0}) \\] Predictions intervals feel and look similar to the mean response intervals above, but there is a very important conceptual difference: prediction means we are trying to “guess” at one individual response as opposed to the mean response of a large group of individuals. For example, if we want to understand nest depths for all nest build when temp is 8 degrees, then we care about estimating a fixed (but unknown) mean depth \\(\\mu_{depth \\mid temp=8}\\). If we see a bird starting to build a nest at 8 degrees, then we care about predicting this one, randomly determined depth \\(Y\\) using \\(pred_{depth \\mid temp=8}\\) and would use a prediction interval. 2.7 Example: SLR model (day 3) We will revisit the woodpecker nesting data first described in Section 2.2.2. 2.7.1 Load data Let’s suppose these 16 nests are a random sample of all nests in the collection region. The command head(dataname) produces a view of the first 5 rows of data. &gt; wpdata &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; head(wpdata) ## temp depth ## 1 -6 21.1 ## 2 -3 26.0 ## 3 -2 18.0 ## 4 1 19.2 ## 5 6 16.9 ## 6 10 18.1 &gt; dim(wpdata) ## [1] 12 2 2.7.2 EDA Start with univariate exploration: &gt; summary(wpdata) ## temp depth ## Min. :-6.00 Min. :10.50 ## 1st Qu.: 0.25 1st Qu.:12.03 ## Median :10.50 Median :16.85 ## Mean :11.00 Mean :16.36 ## 3rd Qu.:21.75 3rd Qu.:18.38 ## Max. :26.00 Max. :26.00 &gt; par(mfrow=c(1,2)) &gt; hist(wpdata$temp) &gt; hist(wpdata$depth) &gt; par(mfrow=c(1,1)) Graphically explore the (“bivariate”) relationship between temp and depth with a scatterplot using the command plot(y,x). Here is the plot version (with pch point character changed to give filled circles): &gt; plot(depth ~ temp, data=wpdata, pch=16, xlab=&quot;air temperature (C)&quot;, + ylab=&quot;nest depth (cm)&quot;, main=&quot;woodpeckers scatterplot&quot;) Here is the ggplot2 version with labs added to change labels and title: &gt; library(ggplot2) &gt; ggplot(wpdata, aes(x=temp, y = depth)) + geom_point() + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, title= &quot;woodpeckers scatterplot&quot;) 2.7.3 The least squares line (the estimated SLR model): You fit the linear model with mean \\(\\mu_{Y \\mid x} = \\beta_0 + \\beta_1 x\\) with the linear model function lm(y ~ x, data=). &gt; wood.lm &lt;- lm(depth~temp, data=wpdata) &gt; wood.lm ## ## Call: ## lm(formula = depth ~ temp, data = wpdata) ## ## Coefficients: ## (Intercept) temp ## 20.1223 -0.3422 We have mean parameter estimates: \\[ \\hat{\\beta}_0 = 20.122 \\ \\ \\ \\ \\hat{\\beta}_0 = -0.342 \\] The object wood.lm is called a linear model object in R. We can add the regression line from this object to an existing base R plot of the data using the abline command. &gt; plot(depth ~ temp, data=wpdata, pch=16, xlab=&quot;air temperature (C)&quot;, + ylab=&quot;nest depth (cm)&quot;, main=&quot;regression of depth on temp&quot;) &gt; abline(wood.lm) The ggplot2 package contains a geom_smooth geometry to add this SLR line: &gt; ggplot(wpdata, aes(x=temp, y = depth)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;, se = FALSE) + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, title= &quot;woodpeckers scatterplot&quot;) With geom_smooth, you need to specify the type of method used to relate \\(x\\) to \\(y\\). Adding se=FALSE removes a confidence interval for the mean trend. Notice that the aes aesthetic given in the ggplot function specifies the x and y variables to plot. The geom’s that follow this part of the command use this aes to create points and an estimated line. 2.7.4 Inference for coefficients The summary command is used for statistical inference for the intercept \\(\\beta_0\\) and slope \\(\\beta_1\\): &gt; summary(wood.lm) ## ## Call: ## lm(formula = depth ~ temp, data = wpdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8066 -1.3321 -0.6529 0.6811 4.8512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.12228 0.94024 21.401 1.11e-09 *** ## temp -0.34218 0.05961 -5.741 0.000188 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.335 on 10 degrees of freedom ## Multiple R-squared: 0.7672, Adjusted R-squared: 0.7439 ## F-statistic: 32.96 on 1 and 10 DF, p-value: 0.0001875 This output gives the mean parameter estimates in the Estimate column of the main table: \\[ \\hat{\\beta}_0 = 20.122 \\ \\ \\ \\ \\hat{\\beta}_0 = -0.342 \\] The estimated model SD \\(\\sigma\\) is given by Residual standard error: \\[ \\hat{\\sigma} = 2.335 \\] You should know how to verify (or find) the test stats and p-values for \\(\\beta_0\\) and \\(\\beta_1\\) given by the summary command if you are given the estimates and SEs. For the slope test \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\neq 0\\) we get a \\(t\\) test stat of &gt; (-0.34218 - 0)/0.05961 ## [1] -5.740312 The p-value for this two sided test is the probability of being above +5.741 and below -5.741, or double the probability below -5.741 (since the t-distribution is symmetric around 0): &gt; pt(-5.741,10) ## [1] 9.373105e-05 &gt; 2*pt(-5.741,10) ## [1] 0.0001874621 A 95% confidence interval for the slope \\(\\beta_1\\) is computed from the t distribution with 10 degrees of freedom: &gt; qt(.975,10) ## [1] 2.228139 &gt; -.34218 + c(-1,1)*qt(.975,10)*.05961 ## [1] -0.4749994 -0.2093606 &gt; confint(wood.lm) ## 2.5 % 97.5 % ## (Intercept) 18.0272874 22.2172802 ## temp -0.4749868 -0.2093679 The confint function gives the most accurate interval (no rounding error) but you need to know how to compute these CIs ``by hand.&quot; 2.7.5 Additional lm information The function lm creates a linear model object in R that has lots of information associated with it. Information includes the coefficient values, fitted values (\\(\\hat{y}_i\\)), and residuals (\\(y_i - \\hat{y}_i\\)): &gt; attributes(wood.lm) ## $names ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; ## ## $class ## [1] &quot;lm&quot; &gt; wood.lm$coefficients ## (Intercept) temp ## 20.1222838 -0.3421773 &gt; wood.lm$fitted.values # predicted y values (y-hat) ## 1 2 3 4 5 6 7 8 ## 22.17535 21.14882 20.80664 19.78011 18.06922 16.70051 16.35833 13.62091 ## 9 10 11 12 ## 12.93656 11.91003 11.56785 11.22567 &gt; wood.lm$residuals # residuals for each data point ## 1 2 3 4 5 6 ## -1.0753477 4.8511843 -2.8066384 -0.5801065 -1.1692199 1.3994894 ## 7 8 9 10 11 12 ## 0.4416667 -1.8209148 -1.9365602 0.1899718 3.2321491 -0.7256736 There are also functions that act on lm objects like &gt; fitted(wood.lm) ## 1 2 3 4 5 6 7 8 ## 22.17535 21.14882 20.80664 19.78011 18.06922 16.70051 16.35833 13.62091 ## 9 10 11 12 ## 12.93656 11.91003 11.56785 11.22567 &gt; resid(wood.lm) ## 1 2 3 4 5 6 ## -1.0753477 4.8511843 -2.8066384 -0.5801065 -1.1692199 1.3994894 ## 7 8 9 10 11 12 ## 0.4416667 -1.8209148 -1.9365602 0.1899718 3.2321491 -0.7256736 &gt; vcov(wood.lm) # variance and covariance matrix of beta estimates ## (Intercept) temp ## (Intercept) 0.88406062 -0.039081046 ## temp -0.03908105 0.003552822 2.7.6 broom package: Tidy lm output The broom package contains functions that convert model objects (like a lm object) into “tidy” data frames. The tidy command summarizes model results: &gt; library(broom) &gt; tidy(wood.lm) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 20.1 0.940 21.4 0.00000000111 ## 2 temp -0.342 0.0596 -5.74 0.000188 Notice that the output object is called a “tibble” which is a type of data frame in R. Here we can add confidence intervals for the model parameters to the output: &gt; tidy(wood.lm, conf.int=TRUE) ## # A tibble: 2 x 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 20.1 0.940 21.4 0.00000000111 18.0 22.2 ## 2 temp -0.342 0.0596 -5.74 0.000188 -0.475 -0.209 The augment command augments the data frame used to create a lm with predicted values and residuals from the lm model: &gt; wpdata.aug &lt;- augment(wood.lm) &gt; head(wpdata.aug) ## # A tibble: 6 x 9 ## depth temp .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.1 -6 22.2 1.22 -1.08 0.272 2.42 0.0544 -0.540 ## 2 26 -3 21.1 1.07 4.85 0.211 1.66 0.732 2.34 ## 3 18 -2 20.8 1.03 -2.81 0.194 2.23 0.215 -1.34 ## 4 19.2 1 19.8 0.900 -0.580 0.149 2.45 0.00632 -0.269 ## 5 16.9 6 18.1 0.737 -1.17 0.0996 2.43 0.0154 -0.528 ## 6 18.1 10 16.7 0.677 1.40 0.0840 2.41 0.0180 0.626 The .fitted column gives the fitted values \\(\\hat{y}_i\\) for each case and the .resid gives the residuals \\(y_i - \\hat{y}_i\\). 2.7.7 Inference for the mean and predicted response We can get a 95% confidence interval for the mean nest depth at 8 degrees Celsius \\(\\mu(depth\\mid temp=8)\\) with the predict command with interval type confidence specified: &gt; predict(wood.lm, newdata = data.frame(temp=8), interval = &quot;confidence&quot;, se.fit=T) ## $fit ## fit lwr upr ## 1 17.38487 15.83132 18.93841 ## ## $se.fit ## [1] 0.6972406 ## ## $df ## [1] 10 ## ## $residual.scale ## [1] 2.33453 The argument se.fit=T provides the SE of the mean response estimate. The estimated mean depth of all nests built at 8 degrees is 17.38 cm with a SE of 0.697 cm. We are 95% confident that the mean depth at 8 degrees is between 15.83 to 18.9 cm. You can verify the computation of the SE and CI using summary stats for temp and the estimated parameter values from the summary command: \\[ \\hat{\\mu}_{temp \\mid 8} = \\hat{\\beta}_0 + \\hat{\\beta}_1 (8) = 20.122 + (-0.342 )(8) = 17.385 \\] \\[ SE(\\hat{\\mu}_{depth \\mid 8}) = \\hat{\\sigma} \\sqrt{\\dfrac{1}{n} + \\dfrac{(x_0 - \\bar{x})^2}{(n-1)s^2_x}} = 2.335 \\sqrt{\\dfrac{1}{12} + \\dfrac{(8-11)^2}{(12-1)11.809^2}} = 0.6972 \\] &gt; nrow(wpdata) ## [1] 12 &gt; mean(wpdata$temp) ## [1] 11 &gt; sd(wpdata$temp) ## [1] 11.80909 &gt; mn.est.se &lt;- 2.33453*sqrt(1/12 + (8-11)^2/((12-1)*11.80909^2)) &gt; mn.est.se ## [1] 0.6972407 &gt; mn.est &lt;- 20.12228 -0.34218*8 &gt; mn.est ## [1] 17.38484 &gt; mn.est + c(-1,1)*qt(.975,10)*mn.est.se ## [1] 15.83129 18.93839 You can also include more than one predictor value temp in this function: &gt; predict(wood.lm, newdata = data.frame(temp=c(8,20)), interval = &quot;confidence&quot;, se.fit=T) ## $fit ## fit lwr upr ## 1 17.38487 15.83132 18.93841 ## 2 13.27874 11.35950 15.19798 ## ## $se.fit ## 1 2 ## 0.6972406 0.8613639 ## ## $df ## [1] 10 ## ## $residual.scale ## [1] 2.33453 We can get a 95% prediction interval for the depth of one depth constructed at 8 degrees Celsius \\(pred_{depth \\mid temp=8}\\) with the predict command with interval type prediction specified: &gt; predict(wood.lm, newdata = data.frame(temp=8), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 17.38487 11.95617 22.81356 We are 95% confident that a new nest built at 8 degrees will have a depth between 11.96 to 22.81 cm. R does not give us the SE for prediction \\(SE(pred(Y \\mid x_0)) = \\sqrt{\\hat{\\sigma}^2 + SE(\\hat{\\mu})^2}\\) so we need to compute it by hand if we want its value: \\[ SE(pred_{depth \\mid 8}) = \\sqrt{\\hat{\\sigma}^2 + SE(\\hat{\\mu})^2} = \\sqrt{2.335^2 + 0.6972^2} = 2.436 \\] &gt; se.pred &lt;- sqrt(2.33453^2 + 0.6972406^2) &gt; se.pred ## [1] 2.436427 The predicted depth of one nest build at 8 degress is 17.38 cm with a SE of 2.44 cm. The 95% prediction interval produced above can be verified as follows: &gt; yhat &lt;- 20.12228 -0.34218*8 &gt; yhat ## [1] 17.38484 &gt; yhat + c(-1,1)*qt(.975,10)*se.pred ## [1] 11.95614 22.81354 2.7.8 Adding confidence bands to a scatterplot The geom_smooth function in ggplot2 adds a 95% confidence interval for \\(\\mu_{y \\mid x}\\) around the estimated mean line: &gt; ggplot(wpdata, aes(x=temp, y = depth)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;) + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, title= &quot;woodpeckers scatterplot with mean confidence interval&quot;) Adding prediction interval bands takes slightly more work. First, create a new version of the data set that includes prediction intervals for each case in the data: &gt; wpdata.pred &lt;- data.frame(wpdata, predict(wood.lm, interval=&quot;prediction&quot;)) &gt; head(wpdata.pred) ## temp depth fit lwr upr ## 1 -6 21.1 22.17535 16.30939 28.04131 ## 2 -3 26.0 21.14882 15.42438 26.87325 ## 3 -2 18.0 20.80664 15.12396 26.48932 ## 4 1 19.2 19.78011 14.20554 25.35468 ## 5 6 16.9 18.06922 12.61459 23.52385 ## 6 10 18.1 16.70051 11.28483 22.11620 Then add a geom_ribbon layer to the previous plot, with ymin and ymax determined by the prediction interval’s lower (lwr) and upper (upr) bounds. The fill arguments in both layers below are not really needed, but they are used here to provide a legend label for the plot: &gt; ggplot(wpdata.pred, aes(x=temp, y = depth)) + + geom_point() + + geom_ribbon(aes(x=temp, ymin = lwr, ymax = upr, fill = &quot;prediction&quot;), alpha = .1) + + geom_smooth(method = &quot;lm&quot;, aes(fill = &quot;confidence&quot;), alpha = .4) + + labs(x = &quot;air temperature (C)&quot;, y= &quot;nest depth (cm)&quot;, + title= &quot;woodpeckers scatterplot&quot;, fill = &quot;Type&quot;) 2.7.9 Tools for displaying your model As described in the Formatting Tables in Markdown (Section D.5), you can use the package stargazer to create a nice table of model results in your pdf. The entire R chunk to do this in a pdf doc format is shown below (but not evaluated in this html book). You will need to add the R chunk option results='asis' to get the table formatted correctly. I also include the message=FALSE option in the chunk below that runs the library command to suppress the automatic message created when running the library command with stargazer. ```{r, results=&#39;asis&#39;, message=FALSE} library(stargazer) stargazer(wood.lm, header=FALSE, single.row = TRUE, title = &quot;SLR of depth on temp&quot;) ``` ```` The kable function (from the knitr package) works well in all output environments (e.g. pdf, html, word). The input to this function needs to be a data frame, so we can use the tidy version of our model summary: &gt; library(knitr) &gt; kable(tidy(wood.lm, conf.int = TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) 20.122 0.94 21.401 0 18.027 22.217 temp -0.342 0.06 -5.741 0 -0.475 -0.209 2.8 Checking model assumptions and fit Recall the SLR model assumptions from 2.2: \\[ Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\] Linearity: The mean response varies linearly with \\(x\\). Constant SD: \\(SD(Y\\mid x)=\\sigma\\) describes the SD of \\(Y\\)’s in the population around a given mean value \\(\\mu_{y\\mid x}\\). An equivalent statement of this assumption is that the model errors should not be associated with \\(x\\). Normality: The shape of population response values around \\(\\mu_{y\\mid x}\\) is described by a normal distribution model. Indepedence: Given a predictor value of \\(x\\), all responses \\(Y\\) occur independently of each other. An equivalent statement of this assumption is that the model errors are independent. 2.8.1 Residuals If all four model assumptions are met, then our model errors \\(\\epsilon_i\\)’s will be independent and distributed like \\(N(0,\\sigma)\\). Now we can’t actually “see” the model errors unless we know the true parameter values in the population since \\[ \\epsilon_i = y_i - (\\beta_0 + \\beta_1 x_i) \\] The closest thing we have to the model errors are the fitted model residuals computed using the estimated model parameters: \\[ r_i = y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\] Residuals for a fitted model are a diagnostic tool to help check whether a model fit to data seems to match the true model form that generated the data. 2.8.2 Residual plot: linearity and constant variance A residual plot is constructed by plotting \\(r_i\\) (y-axis) against \\(x_i\\) (x-axis). A horizontal reference line at \\(y=0\\) is usually added (since the mean residual value is always 0). Linearity: This assumption is met if, at each x-value, you see similar scatter of points (residuals) above and below the 0-reference line. Constant variance: This assumption is met if you see a similar magnitude of point scatter around the 0-reference line as you move along the x-axis. A residual plot that meets both these conditions is called a null plot. Watch out for curvature which suggests the mean function relating \\(y\\) and \\(x\\) may not be linear nonconstant variation which is seem in “fan” shaped plots outliers which can have a large influence on the fitted model 2.8.2.1 Example: Residual plot Let’s revisit the woodpecker nesting depth model and use the broom package to add residuals to the wpdata data frame (Section 2.7.6): &gt; # model fit above in Section 2.7 example &gt; wpdata&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; wood.lm&lt;- lm(depth~temp, data=wpdata) &gt; library(broom) &gt; wpdata.aug &lt;- augment(wood.lm) &gt; head(wpdata.aug) ## # A tibble: 6 x 9 ## depth temp .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21.1 -6 22.2 1.22 -1.08 0.272 2.42 0.0544 -0.540 ## 2 26 -3 21.1 1.07 4.85 0.211 1.66 0.732 2.34 ## 3 18 -2 20.8 1.03 -2.81 0.194 2.23 0.215 -1.34 ## 4 19.2 1 19.8 0.900 -0.580 0.149 2.45 0.00632 -0.269 ## 5 16.9 6 18.1 0.737 -1.17 0.0996 2.43 0.0154 -0.528 ## 6 18.1 10 16.7 0.677 1.40 0.0840 2.41 0.0180 0.626 The residual plot will put the predictor temp on the x-axis and .resid on the y-axis: &gt; library(ggplot2) &gt; ggplot(wpdata.aug, aes(x = temp, y = .resid)) + + geom_point() + + geom_hline(yintercept = 0, linetype= &quot;dashed&quot;) Don’t forget to use the augmented data frame wpdata.aug that contains the residuals. The layer geom_hline adds the horizontal reference line at 0. Interpretation: There are no majors trends seen in this residual plot. Generally, it is hard to prove or disprove model assumptions when we only observe 12 data points! 2.8.3 Residual normal QQ plot A normal QQ plot for a variable plots observed quartiles against the theoretical quartiles from a normal model. If these points follow a line then the data is approximately normal. Here is a general guide for interpreting non-linear trends that indicate a non-normal distribution: Concave up: the distribution is right skewed Concave down: the distribution is left skewed S-shaped: the distribution is symmetric but the tails are either too short (not enough variation) or too long (too much variation) to be normally distributed. More help! Check out this website for a deeper discussion of the interpretation of normal QQ plots We can check the normality assumption by plotting residuals with a normal QQ plot. You can also use a histogram of residuals to help interpret the normal QQ plot. 2.8.3.1 Example: Residual normal QQ plot Back to the augmented woodpecker data set. A histogram of the residuals shows a slightly right skewed distribution. &gt; hist(wpdata.aug$.resid) A quick way to get a normal QQ plot of residuals is to plug the lm object into the plot command and request plot number 2: &gt; plot(wood.lm, which = 2) Alternatively, you could use a ggplot. The aesthetic used for a QQ plot in a ggplot is sample = variable, then geom_qq and geom_qq_line are the layers used: &gt; ggplot(wpdata.aug, aes(sample = .resid)) + + geom_qq() + + geom_qq_line() Interpretation: The QQ plot also suggests a slightly longer right tail because it is (sort of) concave up. But, the QQ plot also clearly shows that this feature could just be due to two cases with residual values that are slightly higher than all others. Again, because there are only 12 data points these two cases could just be due to chance and we can conclude that there aren’t any strong indications of a non-normal distribution. 2.8.4 Independence Independence of errors is probably the hardest assumption to check because it often depends on how the data was collected. Two common scenarios that lead to data that violates this assumption are temporal correlation: This means correlation of errors because measurements were collected over time. Responses (and errors) measured close in time are more likely to be similar in value that responses measured further apart in time. For example, daily temperature readings in Northfield are temporally correlated. spatial correlation: This means correlation of errors because measurements were collected over a spatial region Responses (and errors) measured close together in space are more likely to be similar in value that responses measured further apart in space. For example, home values a sample of houses in St. Paul are likely spatially correlated since the value of a house is likely more similar to it’s neighbor than to a house across town. One method of checking for these two types of dependence is to plot the model residuals \\(r_i\\) (y-axis) against a variable that measures, or is associated, with time or space. For the time example, plot residuals against day of the year. For the spatial example, plot residuals against a categorical “neighborhood” variable. 2.8.5 Robustness against violations Robustness of a statistical method means the conclusions we make using the method aren’t all that senstive to assumptions used to construct the method. Can we trust our SLR model inferences if a model assumption is violated? Inference about \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\mu_{y \\mid x}\\) from a SLR model are only robust against violations of the normality assumption. If \\(n\\) is large enough, then the Central Limit Theorem tells us that t-tests and t-CIs are still accurate. SLR modeling is not robust against any other violations: Linearity: If your fitted mean model doesn’t match the form of the true mean then you will get incorrect inferences about predictor effects, mean responses, predicted responses, etc. Even your estimated model SD will be wrong! Constant variance and independence: If violated, the SE’s produced by the SLR model fit will not accuractely reflect the true uncertainty in our estimated parameters or predictions. Normality for prediction If the normality assumption is violated, then our prediction intervals will not capture the value of a new response “95% of the time”. (There is no CLT and “large n” to help us here when we are trying to predict one response!) 2.8.6 “Fixes” to violations Here are some suggestions if a particular assumption is violated. The first part to consider “fixing” is linearity. If the mean function is not correctly specified then that will likely cause the other assumptions to not hold. Linearity: transform one or both variables to a different scale (e.g. logarithm, square root, reciprocal), modify the mean function (e.g. add a quadratic term), try non-linear regression Constant variance: tranform the response variable, weighted regression Normality: transform the response variable Independence: add more predictors, use a model with correlated errors (e.g. mixed effects, time series, spatial, etc) 2.9 Example: SLR assumptions (day 4/5) 2.9.1 Drug offender sentences The data set DrugOffenses2.csv contains data on 24,011 individuals convicted on federal drug charges during 2013 and 2014. We will subset these individuals to look only at cases given non-life prison sentences (sentence2 &gt; 0), then look at the SLR of an individual’s sentence length (sent.nonlife) in months against their crimal history points (`CRIMPTS). 2.9.1.1 Basic EDA shows that both sentencing and points variables are right skewed, and there are non-responses in each variable. &gt; drug &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/DrugOffenses2.csv&quot;) &gt; dim(drug) ## [1] 24011 55 &gt; library(dplyr) &gt; drugPrison &lt;- filter(drug, sentence2 &gt; 0) &gt; par(mfrow=c(1,2)) &gt; hist(drugPrison$CRIMPTS, main=&quot;criminal points&quot;) &gt; hist(drugPrison$sent.nolife, main=&quot;sentence&quot;) &gt; par(mfrow=c(1,1)) &gt; summary(drugPrison$CRIMPTS) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 0.00 2.00 3.51 5.00 42.00 2012 &gt; summary(drugPrison$sent.nolife) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.03 21.00 48.00 65.41 87.00 960.00 45 2.9.1.2 Usual scatterplot The usual scatterplot suffers from overplotting when we have a large data sets or datasets with very discrete variables. It is hard to see where the majority of cases are in this plot and difficult to discern any trend in the response as we change the predictor variable. &gt; library(ggplot2) &gt; ggplot(drugPrison, aes(x=CRIMPTS, y=sent.nolife)) + + geom_point() + + geom_smooth(method= &quot;lm&quot;, se=FALSE) 2.9.1.3 Jittering and a loess smoother Two ways to minimize the effect of overplotting are to “jitter” the plotted points by adding a small amount of random noise. This can be done by using geom_jitter instead of geom_point use a more transparent plotting symbol. Then when points are overlapping, we will see darker colors which indicate a higher density of observations in that region of the graph. We do this by adding alpha= to the geom_jitter and make the alpha value less than 1 (smaller values mean more transparent points). We will also use a loess smoother line to the scatterplot to help reveal the true trend in sentencing as the point history changes. A smoother is a non-parameteric way to locally “average” the response as we change the predictor value. (Non-parametric means this isn’t a parameterized curve like a line or quadratic function, which means we don’t end up with a nice formula for \\(y\\) given \\(x\\) from such a model.) We fit the loess model to our plot by adding another layer of geom_smooth(method = &quot;loess&quot;, se=FALSE). In the commands below, the aes(color=) just adds a color legend to the plot. &gt; ggplot(drugPrison, aes(x=CRIMPTS, y=sent.nolife)) + + geom_jitter(alpha = .2) + + geom_smooth(method= &quot;lm&quot;, se=FALSE, aes(color=&quot;lm&quot;)) + + geom_smooth(method=&quot;loess&quot;, se=FALSE, aes(color=&quot;smoother&quot;)) 2.9.1.4 Residual plot and loess smoother The scatterplot and loess smoother suggest there is slight quadratic relationship between points history and sentence length, with a positive trend up to about 20 points and a negative trend after. We can see this too in the residual plot with loess smoother added. The variation around the regression line seems fairly similar for any points value up to about 20 points, and after 20 points there are fewer cases which seem to be slightly less variable. (Note that “similar variation” means the spread around the line is of similar magnitude for any value of x, not that the spread around the line is symmetric. As we see next, the spread around the line (residuals) is skewed right.) &gt; drug.lm &lt;- lm(sent.nolife ~ CRIMPTS, data=drugPrison) &gt; library(broom) &gt; drugPrison.aug &lt;- augment(drug.lm) &gt; ggplot(drugPrison.aug, aes(x=CRIMPTS, y=.resid)) + + geom_jitter(alpha = .2) + + geom_smooth(method= &quot;lm&quot;, se=FALSE, aes(color=&quot;lm&quot;)) + + geom_smooth(method=&quot;loess&quot;, se=FALSE, aes(color=&quot;smoother&quot;)) 2.9.1.5 Checking normality The residuals for this model are not normally distributed, there is a strong skew to the right. If the linear model fit the trend of the data, this non-normality would not be a concern for inference about the mean parameters \\(\\beta\\). It would only be a concern if we wanted to predict one individual’s sentence length with confidence given their criminal history points. &gt; par(mfrow=c(1,2)) &gt; hist(resid(drug.lm), main=&quot;Histogram of residuals&quot;) &gt; plot(drug.lm, which = 2) 2.9.2 Case study 15.2 - Global Warming Let’s look at the regression of global mean temperature deviation (Celsius) on year (1850 through 2010). The scatterplot suggests a curved relationship with time. Temperature deviation is the mean yearly temp minus the mean temp from all 161 years. &gt; library(Sleuth3) &gt; temps &lt;- case1502 &gt; summary(temps) ## Year Temperature ## Min. :1850 Min. :-0.6060 ## 1st Qu.:1890 1st Qu.:-0.3000 ## Median :1930 Median :-0.1740 ## Mean :1930 Mean :-0.1153 ## 3rd Qu.:1970 3rd Qu.: 0.0260 ## Max. :2010 Max. : 0.6200 &gt; ggplot(temps, aes(x = Year, y = Temperature)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;, se = FALSE) + + geom_smooth(method = &quot;loess&quot;, se = FALSE, linetype = &quot;dashed&quot;) 2.9.2.1 Independent residuals? After fitting our regression of Temperature on Year and Year^2 (due to curvature), a check of the residual plot (residuals vs. Year) shows no obvious signs of curvature in residuals so the quadratic mean function with time seems adequate. &gt; temps.lm &lt;- lm(Temperature ~ Year + I(Year^2), data=temps) &gt; temps.aug &lt;- augment(temps.lm) &gt; ggplot(temps.aug, aes(x = Year, y = .resid)) + + geom_point() + + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) The independence assumption will be violated for this model if residuals closer in time (x) are more similar than residuals further apart in time. This is hard to see in the basic residual plot above. To see this idea better, we can look at line plot of residuals vs. time: &gt; ggplot(temps.aug, aes(x = Year, y = .resid)) + + geom_point() + + geom_line() + + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) With this plot is it clear that if one year has a negative residual (lower than expected temps), the next year’s residual is often also negative. Same is true for positive residuals. This indicates that there is a temporal association in these residuals, which means that they are not indepedent. A SLR model is not appropriate for this data because the SEs will not accurately reflect the uncertainty in our estimates when independence is violated. 2.9.2.2 Autocorrelation The correlation statistic is used to assess the strength and direction of a linear relationship between two quantitative variables. The autocorrelation is used to assess how correlated values are for the same quantitative variable. Autocorrelation is computed by pairing responses by a lag amount. Lag of 1 means we are computing correlation for responses that differ by one spot in the vector of responses. For our time and temp data, this means looking at the correlation between responses that differ by a year. Lag 2 means looking at correlation between responses that differ by 2 years, and so on. We can use autocorrelation to verify that residuals closer together in time are more similar than those further apart in time: &gt; acf(resid(temps.lm)) &gt; acf(resid(temps.lm), plot=FALSE) ## ## Autocorrelations of series &#39;resid(temps.lm)&#39;, by lag ## ## 0 1 2 3 4 5 6 7 8 9 ## 1.000 0.592 0.446 0.336 0.380 0.313 0.270 0.261 0.272 0.287 ## 10 11 12 13 14 15 16 17 18 19 ## 0.236 0.164 0.083 -0.004 0.014 -0.007 -0.067 -0.022 -0.038 -0.069 ## 20 21 22 ## -0.065 -0.140 -0.162 Residuals that differ by one year have a correlation of 0.592, indicating a moderate positive correlation. This is what were able to see in the line plot above. Residuals that differ from 2-9 years have correlations between about 0.45 to 0.25. After years there is little to no autocorrelation between residuals. 2.10 Transformations Transformations of one or both variables in SLR is done to fix one, or both, of these assumption violations: linearity and constant variance. Tranformations are often explored via trial-and-error, use plots of transformed variables and residual plots from potential transformed SLR models to find the “best” SLR model. “Best” is determined by a model that most satisfies the SLR assumptions, not a model that yields the “strongest” relationship between variables. A transformation choice can also be determined from a theoretical model that we have from a specific application. 2.10.1 Transformation choices We need to look at non-linear functions of a variable. A linear transformation, like changing a measure from cm to feet would only change the numbers you see on the axes label and not the relative location of points on the plot. Common transformations to consider are the following: logarithms: Any log-base can be explored but base choice will only affect how you interpret a model, not affect the linearity or constant variance assumptions. E.g. if \\(\\log_2(y)\\) vs. \\(x\\) looks nonlinear, then \\(\\log_{10}(y)\\) vs. \\(x\\) will also be nonlinear. Logarithms are nice because they are easily interpretable in a SLR model (more to come). In R, &gt; log(2) # the natural log function (ln) ## [1] 0.6931472 &gt; log10(2) # base 10 ## [1] 0.30103 &gt; log2(2) # base 2 ## [1] 1 square root: A square root transformation is an “in between” no transformation and a logarithm transformation (see the Figure below) &gt; sqrt(4) # square root function ## [1] 2 2.10.2 Transformations in R We can visualize square root and logarithms in aggplot2 scatterplot by adding a layer that changes the scale of an axes. For example, scale_x_sqrt() will change the geom_point() scatterplot to a plot of \\(y\\) aganist \\(\\sqrt{x}\\) but the axes numeric labels on the x-axis will still measure \\(x\\). The layer scale_y_log10() will convert \\(y\\) to the base-10 logarithm. If you want a transformation that is not one of these two, then you can transform the variable in the aesthetic: ggplot(data, aes(x = 1/x, y = y)) would plot \\(y\\) against the reciprocal \\(1/x\\). You can apply transformations directly in the lm command too, e.g. lm(log(y) ~ log(x), data) fits the regression of \\(\\log(y)\\) against \\(\\log(x)\\). If a transformation involves a mathematics function (like power ^ or division /) on the right side of the formula symbol ~, then you need to use the “as is” operator function I(). E.g. the regression of \\(y\\) against the reciprocal \\(1/x\\) is lm(y ~ I(1/x), data). 2.10.3 Interpretation In general, models with transformed variables should be interpreted as a usual SLR but with the transformed variable scale. For example, the SLR for \\(\\sqrt{y}\\) against \\(\\sqrt{x}\\) has a mean function that looks like \\[ \\mu_{\\sqrt{y} \\mid x} = \\beta_0 + \\beta_1 \\sqrt{x} \\] so a one unit increase in the square root of \\(x\\) in associated with a \\(\\beta_1\\) change in the mean of the square root of \\(y\\). Models that use logarithms have a nicer interpretation. Section 2.10.4 has more review of logarithms but the three common SLR models that use logarithms are given here. Logarithmic model: The regression of \\(Y\\) on \\(\\log(x)\\) has the mean function \\[ \\mu(Y \\mid \\log(x)) = \\beta_0 + \\beta_1 \\log(x) \\] Multiplying \\(x\\) by a factor \\(m\\) is associated with a mean function change of \\[ \\mu(Y \\mid \\log(mx)) - \\mu(Y \\mid \\log(x)) = \\beta_1(\\log(m) + \\log(x)) - \\beta_1 \\log(x) = \\beta_1 \\log(m) \\] If base-2 is used for the tranformation of \\(x\\) then a doubling of \\(x\\) (so \\(m=2\\)) is associated with a change in mean \\(y\\) of \\(\\beta_1 \\log_2(2) = \\beta_1\\). Exponential model: The regression of \\(\\log(Y)\\) on \\(x\\) has the mean function \\[ \\mu(\\log(Y) \\mid x) = median(\\log(Y) \\mid x) = \\beta_0 + \\beta_1 x \\] Since the median of a logged-variable equals the log of the median of the variable, we can “untransform” this model and write it in terms of the median of \\(Y\\) (assuming here that log is the natural log): \\[ median(Y \\mid x) = e^{\\log(median(Y \\mid x))} = e^{median(\\log(Y) \\mid x)} = e^{\\beta_0}e^{\\beta_1 x} \\] Note that the mean of a logged-variable does not equal the log of the mean of the variable, so we can’t express the untransformed model in terms of the median of \\(Y\\). A one unit increase in \\(x\\) is associated with a \\(e^{\\beta_1}\\)-factor change in the median function since \\[ median(Y \\mid x+1) = e^{\\beta_0}e^{\\beta_1 (x+1)} = e^{\\beta_0}e^{\\beta_1 x}e^{\\beta_1} = median(Y \\mid x) e^{\\beta_1} \\] - - Power model: The regression of \\(\\log(Y)\\) on \\(\\log(x)\\) has the mean function \\[ \\mu(\\log(Y) \\mid \\log(x)) = median(\\log(Y) \\mid\\log(x)) = \\beta_0 + \\beta_1 \\log(x) \\] Using the same logic as the exponential model, inference on the untransformed scale of \\(Y\\) is about the median of \\(Y\\): \\[ median(Y \\mid x) = e^{\\log(median(Y \\mid x))} = e^{median(\\log(Y) \\mid x)} = e^{\\beta_0}e^{\\beta_1 \\log(x)} = e^{\\beta_0}(e^{\\log(x)})^{\\beta_1} = e^{\\beta_0}x^{\\beta_1} \\] An m-fold (multiplicative) change in \\(x\\) is associated with a \\(m^{\\beta_1}\\)-factor change in the median function since \\[ median(Y \\mid mx) = e^{\\beta_0}e^{\\beta_1 \\log(mx)} = e^{\\beta_0}(e^{\\log(x)})^{\\beta_1}(e^{\\log(m)})^{\\beta_1} = median(Y \\mid x) m^{\\beta_1} \\] 2.10.4 Review: Logarithms Let \\(b&gt;0\\) and \\(x&gt;0\\). The logarithm (base-\\(b\\)) of \\(x\\) is denoted \\(\\log_b(x)\\) and equal to \\[ \\log_b(x) = a \\] where \\(a\\) tells us what power we must raise \\(b\\) to to obtain the value \\(x\\): \\[ b^a = x \\] Easy examples are: \\(b=2\\), \\(x=8\\) and \\(a=3\\), \\[ \\log_2(8) = 3 \\] since \\(2^3 = 8\\). Or using base \\(b=10\\), then \\[ \\log_{10}(0.01) = -2 \\] since \\(10^{-2} = 0.01\\). Some basic facts logarithm facts are \\[ \\log_b(b) = 1 \\] since \\(b^1 = b\\) and \\[ \\log_b(1) = 0 \\] since \\(b^0 = 1\\). 2.10.4.1 Interpreting logged variables Multiplicative changes in \\(x\\) result in additive changes in \\(\\log_b(x)\\). If \\(m&gt;0\\), then \\[ \\log_b(mx) = \\log_b(m) + \\log_b(x) \\] For example, \\[ \\log_2(16) = \\log_2(2\\times 8) = \\log_2(2) + \\log_2(8) = 1 + 3 = 4 \\] 2.10.4.2 Inverse (i.e. reversing the log, getting rid of the log, …) The logarithm and exponential functions are inverses of one another. This means we can “get rid” of the log by calculating \\(b\\) raised to the logged-function: \\[ b^{\\log_b(x)} = x \\] This will be useful in regression when we have a linear relationship between logged-response \\(y\\) and a set of predictors. We need to For example, suppose we know that \\[ \\log_2(y) = 3 + 5x \\] To return this to an expression on the original (unlogged) scale of \\(y\\), we need take both sides raised to the base 2: \\[ 2^{\\log_2(y)} = 2^{3 + 5x} \\] Simplifying both sides gives \\[ y = 2^3 \\times 2^{5x} \\] 2.10.4.3 Logarithm Practice Questions (day 6) Solutions are posted on the class Moodle site. Write the following as the sum of two logarithms. Simplify as much as possible: \\(\\log_2(2x)\\) \\(\\log_2(0.5x)\\) \\(\\ln(2x)\\) where \\(\\ln\\) is the natural log (base-\\(e\\)) Write the following expressions in terms of \\(y\\), not \\(\\log(y)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3x\\) \\(\\log_{10}(y) = -2 + 0.4x\\) \\(\\ln(y) = 1 - 3x\\) Write the following expressions in terms of \\(y\\) and \\(x\\), not \\(\\log(y)\\) and \\(\\log(x)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3\\log_2(x)\\) \\(\\ln(y) = -2 + 0.4\\ln(x)\\) \\(\\ln(y) = 1 - 3\\log_2(x)\\) Logarithmic model: Regression of \\(Y\\) on \\(\\log(x)\\) obtains the following estimated mean of \\(Y\\): \\[ \\hat{\\mu}(Y \\mid x) = 1 - 3 \\log_2(x) \\] What is the change in estimated mean response if we double the value of \\(x\\)? What is the change in estimated mean response if we triple the value of \\(x\\)? What is the change in estimated mean response if we reduce the value of \\(x\\) by 20%? Exponential model: Regression of \\(\\log_2(Y)\\) on \\(x\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = -2 + 0.4x \\] Write the median in terms of \\(Y\\) instead of \\(\\log_2(Y)\\). Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 1 unit? What is the percent change in estimated median response if we increase \\(x\\) by 1 unit? What is the multiplicative change in estimated median response if we decrease \\(x\\) by 2 units? What is the percent change in estimated median response if we decrease \\(x\\) by 2 units? Power model: Regression of \\(\\log_2(Y)\\) on \\(\\log_2(x)\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = 1 -3\\log_2(x) \\] Write the median in terms of \\(Y\\) and \\(x\\) instead of \\(\\log\\)s. Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 50%? What is the percent change in estimated median response if we increase \\(x\\) by 50%? What is the multiplicative change in estimated median response if we reduce the value of \\(x\\) by 20%? What is the percent change in estimated median response if we reduce the value of \\(x\\) by 20%? 2.11 Examples: Transformations (day 6) A solution to these examples is found on the class Moodle page. 2.11.1 Cars 2004 This is a dataset with stats taken from 230 car makes and models from 2004. (1a) Is city MPG a linear function of car weight (lbs)? &gt; cars &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/cars2004.csv&quot;) &gt; summary(cars[, c(&quot;city.mpg&quot;,&quot;weight&quot;)]) ## city.mpg weight ## Min. :10.00 Min. :1850 ## 1st Qu.:16.00 1st Qu.:3185 ## Median :18.00 Median :3606 ## Mean :19.22 Mean :3738 ## 3rd Qu.:21.00 3rd Qu.:4237 ## Max. :60.00 Max. :7608 &gt; library(ggplot2) &gt; ggplot(cars, aes(x= weight, y = city.mpg)) + + geom_point() + + geom_smooth(method = &quot;lm&quot;, se = FALSE) + + geom_smooth(method = &quot;loess&quot;, se = FALSE, linetype = &quot;dashed&quot;, color=&quot;red&quot;) + + labs(title=&quot;City MPG vs. car weight&quot;) (1b) Explore some transformations: which gives the most linear relationship? Use layers of the type scale_x_log10() or scale_y_sqrt() to change a particular axes (1c) Fit the regression of 1/mpg on weight and check residuals 2.11.2 2005 Residential Energy Survey (RECS) RECS surveys households across the US. We are treating this sample of 4,382 households as an equally-weighted sample of US households. Our goal now is to model total energy costs (CostTotal) as a function of the size of the housing unit (SqftMeasure). (2a) Total energy cost vs. size With over 4,000 cases, we reduce point transparency with alpha to avoid overplotting. Assess the scatterplot in terms of (1) linearity, (2)constant variance, and (3) normal errors. &gt; energy &lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/EnergySurvey.csv&quot;) &gt; dim(energy) ## [1] 4382 16 &gt; summary(energy[,c(&quot;CostTotal&quot;,&quot;SqftMeasure&quot;)]) ## CostTotal SqftMeasure ## Min. : 57 Min. : 167 ## 1st Qu.: 1138 1st Qu.: 1056 ## Median : 1673 Median : 1848 ## Mean : 1841 Mean : 2284 ## 3rd Qu.: 2331 3rd Qu.: 3042 ## Max. :10346 Max. :11383 ## NA&#39;s :1 &gt; library(ggplot2) &gt; ggplot(energy, aes(x = SqftMeasure, y = CostTotal)) + + geom_point(alpha = .1) + + geom_smooth(method = &quot;loess&quot;, se = FALSE) (2b) Find a transformation that yields a linear model As done in example 1, use trial-and-error to find a transformation that yields a a linear relationship. Once this is found, we can fit a SLR model to the transformed variables. The basic transformations to consider (for \\(y\\), or \\(x\\), or both) are log10 (log base-10), sqrt, and inverse. (2c) Fit the “best” model from (2b) (2d) Once you find a model that “fits”, interpret your model parameters. What is the fitted model? How does cost change if house size is doubled? Get a CI for this effect. How does cost change if house size decreases by 10%? Get a CI for the effect. 2.12 \\(R^2\\) and ANOVA for SLR After we check the fit of a model and determine that there are no concerns with the SLR assumptions, we can summarize it with inference, interpretations and measures of “strength” of the association. One common measure of strength is called “R-squared”: \\[ R^2 = 1- \\dfrac{(n-2)\\hat{\\sigma}^2}{(n-1)s^2_y} \\] \\(R^2\\) measures the proportion of total variation in the response that is explained by the model. Total variation in \\(y\\) is measured, in part, by the sample SD of the response, which is a piece of the denominator above. The numerator measures the varation in \\(y\\) around the regression line (\\(\\hat{\\sigma}\\)), which is a measure of unexplained, or residual, variation. One minus the unexplained variation proportion gives us the value of \\(R^2\\). 2.12.1 Example: \\(R^2\\) Let’s revisit the woodpecker nest depth model: &gt; wpdata&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/woodpeckers.csv&quot;) &gt; library(ggplot2) &gt; ggplot(wpdata, aes(x=temp, y=depth)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + labs(x=&quot;air temperature (C)&quot;, y=&quot;nest depth (cm)&quot;, title=&quot;woodpeckers scatterplot&quot;) &gt; wood.lm&lt;- lm(depth~temp, data=wpdata) We get \\(R^2\\) in the Multiple R-squared entry in the lm summary output: &gt; summary(wood.lm) ## ## Call: ## lm(formula = depth ~ temp, data = wpdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8066 -1.3321 -0.6529 0.6811 4.8512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.12228 0.94024 21.401 1.11e-09 *** ## temp -0.34218 0.05961 -5.741 0.000188 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.335 on 10 degrees of freedom ## Multiple R-squared: 0.7672, Adjusted R-squared: 0.7439 ## F-statistic: 32.96 on 1 and 10 DF, p-value: 0.0001875 Here we have \\(R^2 = 0.7672\\), meaning the regression of depth on temp helps explain about 76.7% of the observed variation in depth. We can see how R computes this value by looking at the sample SD of \\(y\\), \\(s^2_y = 4.6133124\\) and the estimated model SD \\(\\hat{\\sigma} =2.3345298\\) &gt; sd(wpdata$depth) # sample SD of y ## [1] 4.613312 &gt; summary(wood.lm)$sigma # mode SD estimate ## [1] 2.33453 &gt; n &lt;- 12 &gt; (n-2)*summary(wood.lm)$sigma^2/((n-1)*sd(wpdata$depth)^2) # unexplained ## [1] 0.2327986 &gt; 1 - (n-2)*summary(wood.lm)$sigma^2/((n-1)*sd(wpdata$depth)^2) # explained ## [1] 0.7672014 2.12.2 ANOVA for SLR Analysis of Variance (ANOVA) decomposes the total variation in \\(y_i\\), \\(y_i - \\bar{y}\\), into a portion that is explained by a model, \\(\\hat{y}_i - \\bar{y}\\), and a portion that is unexplained, \\(y_i - \\hat{y}_i\\): \\[ y_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) \\] For row 2 in the wpdata, the total variation is show in red in the plot below while the explained variation is in blue and unexplained in green. Notice that the “explained” portion is what gets us closer to the actual response \\(y=26\\) (compared to just the mean response), which is what you would expect to see if temp is useful in explaining why we see variation in nest depths. The next step in ANOVA, is to total the squared variation distances across all \\(n\\) data points (squared because we don’t care if the differences are positive or negative). Some very useful mathematics is used to prove that the total squared variation of all \\(n\\) cases is equal to \\[ \\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2 \\] The three components are called: - total variation: \\(SST = \\sum_{i=1}^n (y_i - \\bar{y})^2 = (n-1)s^2_y\\) - regression (explained) variation: \\(SSreg = \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2\\) - residual (unexplained) variation: \\(SSR = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = (n-2)\\hat{\\sigma}^2\\) These sum of squares (SS) can also be used to compute \\(R^2\\) since \\[ R^2 = 1- \\dfrac{(n-2)\\hat{\\sigma}^2}{(n-1)s^2_y} = 1- \\dfrac{SSR}{SST} = \\dfrac{SSreg}{SST} \\] 2.12.3 Example: ANOVA Back to the nest depth model. The anova function extracts the sum of square values from our lm: &gt; anova(wood.lm) # ANOVA Table ## Analysis of Variance Table ## ## Response: depth ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## temp 1 179.61 179.61 32.956 0.0001875 *** ## Residuals 10 54.50 5.45 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 For this model we have - regression (explained) variation: from the temp row, \\(SSreg = 179.61\\) - residual (unexplained) variation: from the Residuals row, \\(SSR = 54.60\\) - total variation: adding the two SS values gives total variation \\(SST = SSreg + SSR = 179.61 + 54.60 = 234.21\\) The ANOVA SS can also give us the value of \\(R^2\\) for the model: \\[ R^2 = \\dfrac{179.61}{179.61 + 54.60} = 0.7668759 \\] "],
["mlr.html", "Chapter 3 Multiple Regression 3.1 The variables 3.2 The model form 3.3 Example: MLR fit and visuals 3.4 Categorical Predictors 3.5 Inference for MLR 3.6 ANOVA F-tests", " Chapter 3 Multiple Regression This chapter covers material from chapters 9-12 of Sleuth. 3.1 The variables Suppose we have a quantitative response variable \\(y\\) that we want to relate to \\(p\\) explantory variable (aka predictors, covariates) \\(x_1, \\dotsc, x_p\\). There is no restriction on the type of covariates, they can be both quantitative and categorical variables. 3.2 The model form This section describes the multiple linear regression (MLR) model for a particular population of interest. Another way to frame the model is that it describes a hypothetical data generating process (DGP) that was used to generate the sample of data that we have on hand. The major change in the MLR model compared to the SLR model is that now the mean function \\(\\mu_{y\\mid x_1, \\dotsc, x_p}\\) is a function of all covariates. The expression for \\(\\mu\\) must be linear with respect to the \\(\\beta\\) parameters even if we used a function of a predictor like \\(\\log(x)\\). The expression for \\(\\mu\\) is can even involve polynomial functions of \\(x\\) like \\(x^2\\) or interactions of predictors like \\(x_1 \\times x_p\\). In this section, we will describe the basic MLR with the simplest form it can take: \\[ \\mu_{y\\mid x_1, \\dotsc, x_p} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dotsm \\beta_p x_{p,i} \\] More complicated forms will be described later in this chapter. Let \\(Y_i\\) be the response from unit \\(i\\) that has explanatory (aka predictor) values \\(x_{1,i}, x_{2,i}, \\dotsc, x_{p,i}\\). There are two equivalent ways to express the SLR model for \\(Y\\): Conditional normal model: \\[ Y_i \\mid x_{1,i}, x_{2,i}, \\dotsc, x_{p,i}\\sim N(\\mu_{y\\mid x} = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dotsm \\beta_p x_{p,i}, \\sigma) \\] Mean + error: \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\dotsm \\beta_p x_{p,i} + \\epsilon_i \\ \\ \\ \\ \\ \\epsilon_i \\sim N(0, \\sigma) \\end{equation}\\] Both expressions of the MLR model above say the same thing: Linear Mean: \\(\\mu_{y\\mid x}\\) describes the population mean value of \\(Y\\) given all predictor values and it is linear with respect to the \\(\\beta\\) parametrs. (We still have a “linear” model even if we used \\(\\log(x)\\) or \\(x^2\\)!) Constant SD: \\(SD(Y\\mid x)=\\sigma\\) describes the SD of \\(Y\\)’s in the population around a given mean value \\(\\mu_{y\\mid x}\\). The fact that this SD does not depend on the value of \\(x\\) is called the contant variance, or homoscedastic, assumption. Normality: The shape of population response values around \\(\\mu_{y\\mid x}\\) is described by a normal distribution model. Indepedence: Given a predictor value of \\(x\\), all responses \\(Y\\) occur independently of each other. There are a total of \\(\\pmb{p+1}\\) parameters in this MLR model: the \\(p\\) mean parameters \\(\\beta_0, \\beta_1, \\dotsc, \\beta_p\\) the SD parameter \\(\\sigma\\) 3.2.1 Interpretation How a predictor influences the mean response is determined by the form of the \\(\\mu\\) function. Some common forms are discussed here. 3.2.1.1 Planar model The mean model described above models the relationship between \\(y\\) and all corvariates as a \\((p+1)-\\)dimensional plane: \\[ \\mu_{y\\mid x_1, \\dotsc, x_p} = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\dotsm \\beta_p x_{p} \\] When \\(p=1\\), we have a SLR and this “plane” simplifies to a 2-d line. When \\(p=2\\), the mean “surface” is a 3-D plane. The plot below displays an example of a sample of point triples \\((x_{1,i}, x_{2,i}, y_i)\\) that form a point “cloud” that is floating in the x-y-z coordinate system. The mean function is a plane that floats through the “middle” of the point cloud, hitting the mean value of \\(y\\) for each combination of \\(x_1\\) and \\(x_2\\). Notice that when we “fix” one of the predictor values, the trend between \\(y\\) and the other predictor is linear. E.g. Pick any place on the mean surface, then any place you “trace” along the surface will result in a line. How should we interpret the \\(\\beta\\)’s in our planar mean function? \\(\\beta_0\\) is the mean response when all predictor values are 0 since \\(\\mu_{y \\mid 0} = \\beta_0 + \\beta_1(0) + \\dotsm + \\beta_p (0)= \\beta_0\\). \\(\\beta_j\\) tells us how the mean response changes for a one unit increase in \\(x_j\\) holding all other predictors fixed. We can illustrate this for the the predictor \\(x_1\\). The mean function at \\(x_1 + 1\\), holding \\(x_2, \\dotsc, x_p\\) fixed, is \\[ \\begin{split} \\mu(y \\mid x_1+1, x_2, \\dotsc, x_p) &amp;= \\beta_0 + \\beta_1 (x_{1}+1) + \\beta_2 x_{2} + \\dotsm + \\beta_p x_p \\\\ &amp; = \\beta_0 + \\beta_1 x_{1} + \\beta_1 + \\beta_2 x_{2} + \\dotsm + \\beta_p x_p \\\\ &amp; = \\mu(y \\mid x_1, x_2, \\dotsc, x_p) + \\beta_1 \\end{split} \\] This shows that a 1 unit increase in \\(x_1\\) is associated with a \\(\\beta_1\\) change in the mean response holding all other predictors fixed. This holds in general too: a 1 unit increase in \\(x_j\\) is associated with a \\(\\beta_j\\) change in the mean response holding all other predictors fixed. 3.2.1.2 Quadratic model A model that incorporates polynomial functions of predictors, like \\(x^2, x^3\\), etc, is also a MLR model. Here is an example that says the mean of \\(y\\) is a quadratic function of \\(x_1\\) but a linear function of \\(x_2\\): \\[ \\mu_{y\\mid x_1, x_2} = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{1}^2 + \\beta_3 x_{2} \\] Notice that this model has two covariates \\(x_1\\) and \\(x_2\\) but four mean function parameters \\(\\beta_0 - \\beta_4\\) due to the extra quadratic term. An example of this model is visualized below (with all \\(\\beta\\)’s equal to 1). You see the quadratic relationship with \\(x_1\\) when you orient the \\(x_1\\) axis to be the left-to-right axes with the \\(x_2\\) axis coming out of the page. When these axes are flipped, you see a linear relationship. How should we interpret the \\(\\beta\\)’s in this quadratic mean function? \\(\\beta_0\\) is the mean response when all predictor values are 0. \\(\\beta_3\\) tells us how the mean response changes for a one unit increase in \\(x_2\\) holding \\(\\pmb{x_1}\\) fixed. \\(\\beta_1\\) and \\(\\beta_2\\) tell us how the mean response changes as a function of \\(x_1\\), but since it is quadratic the exact change in the mean response depends on the value of \\(x_1\\). E.g. the closer the \\(x_1\\) value is to the “top” or “bottom” of the quadratic curve, the smaller the changes in the mean response. We can illustrate this for the the predictor \\(x_1\\). The mean function at \\(x_1 + 1\\), holding \\(x_2\\) fixed, is \\[ \\begin{split} \\mu(y \\mid x_1+1, x_2) &amp;= \\beta_0 + \\beta_1 (x_{1}+1)+ \\beta_2 (x_{1}+1)^2 + \\beta_3 x_{2} \\\\ &amp; = \\beta_0 + \\beta_1 x_{1} + \\beta_1 + \\beta_2 x_{1}^2 + \\beta_2 2x_1 + \\beta_2 + \\beta_3 x_2 \\\\ &amp; = \\mu(y \\mid x_1, x_2) + \\beta_1 + \\beta_2(2x_1+1) \\end{split} \\] This shows that a 1 unit increase in \\(x_1\\) is associated with a \\(\\beta_1 + \\beta_2(2x_1+1)\\) change in the mean response holding all other predictors fixed. For example, if \\(x_1 = 1\\) the mean change is \\(\\beta_1 + 3\\beta_2\\). 3.2.1.3 Interactions A model that incorporates predictor interactions says that the effect of one predictor is dependent on the value of the other predictor, and vica versa. Here is an example that has the interaction of \\(x_1\\) and \\(x_2\\): \\[ \\mu_{y\\mid x_1, x_2} = \\beta_0 + \\beta_1 x_{1} + \\beta_2 x_{2} + \\beta_3 x_1x_{2} \\] An example of this model is visualized below (with all \\(\\beta\\)’s equal to 1). We obviously don’t see a planar surface. It’s a bit hard to see, but any “slice” you take from the surface along the \\(x_1\\) axis will create a linear function along the \\(x_2\\) axis. The trend, or steepness, of this line depends on the value you chose for \\(x_1\\). Switching the \\(x_1\\) and \\(x_2\\) variables results in the same observation. The effect of each predictor is linear, but its slope, or effect size, is a function of the other predictor. How should we interpret the \\(\\beta\\)’s in this interaction mean function? \\(\\beta_0\\) is the mean response when all predictor values are 0. \\(\\beta_1\\) tells us how the mean response changes for a one unit increase in \\(x_1\\) when \\(\\pmb{x_2=0}\\). \\(\\beta_2\\) tells us how the mean response changes for a one unit increase in \\(x_2\\) when \\(\\pmb{x_1=0}\\). \\(\\beta_3\\) is the interaction effect that tells us how the effect of \\(x_1\\) varies as a function of \\(x_2\\), and vice versa. We can illustrate this for the the predictor \\(x_1\\). The mean function at \\(x_1 + 1\\), holding \\(x_2\\) fixed, is \\[ \\begin{split} \\mu(y \\mid x_1+1, x_2) &amp;= \\beta_0 + \\beta_1 (x_{1}+1)+ \\beta_2 x_2+ \\beta_3 (x_1+1)x_{2} \\\\ &amp; = \\beta_0 + \\beta_1 x_{1} + \\beta_1 + \\beta_2 x_{2} + \\beta_3x_1x_2 + \\beta_3x_2 \\\\ &amp; = \\mu(y \\mid x_1, x_2) + \\beta_1 + \\beta_3x_2 \\end{split} \\] This shows that a 1 unit increase in \\(x_1\\) is associated with a \\(\\beta_1 + \\beta_3x_2\\) change in the mean response holding all other predictors fixed. For example, if \\(x_2 = 5\\) the mean change is \\(\\beta_1 + 5\\beta_3\\). Similary, a 1 unit increase in \\(x_2\\) is associated with a \\(\\beta_2 + \\beta_3x_1\\) change in the mean response holding all other predictors fixed. 3.3 Example: MLR fit and visuals 3.3.1 lm fit We fit a MLR model in R using the same command as a SLR model, but we add model predictors on the right-hand side of the formula. Examples include: planar model: lm(y ~ x1 + x2 + x3, data=, subset=) interaction model: lm(y ~ x1 + x2 + x1:x2 + x3, data=, subset=) or lm(y ~ x1*x2 + x3, data=, subset=) quadratic model: lm(y ~ x1 + I(x1^2) + x2, data=, subset=). This uses the “as is” operator I() that tells R that ^ is interpreted as a power rather than as its symbolic use in a formula (see ?formula for more details) Here is the multiple linear regression of brain weight (g) on gestation (days), body size (kg) and litter size from Case Study 9.2: &gt; library(Sleuth3) &gt; brain &lt;- case0902 &gt; brain.lm &lt;- lm(Brain ~ Gestation + Body + Litter, data=brain ) &gt; brain.lm ## ## Call: ## lm(formula = Brain ~ Gestation + Body + Litter, data = brain) ## ## Coefficients: ## (Intercept) Gestation Body Litter ## -225.2921 1.8087 0.9859 27.6486 The estimated mean function is \\[ \\hat{\\mu}(brain \\mid gest,body,litter) = -225.2921 + 1.8087 Gestation + 0.9859 Body + 27.6486 Litter \\] Holding gestation length and body weight fixed, increasing litter size by one baby increases estimated mean brain weight by 27.6g. But is this an appropriate model to use? This interpretation is meaningless if the model doesn’t fit the data! We need to check this with scatterplots and residual plots. 3.3.2 Graphics for MLR If we have \\(p\\) predictors in our model, then the MLR model can be viewed in a (at least) \\(p\\)-dimensional picture! Viewing this is difficult, if not impossible. The best we can do is look at 2-d scatterplots of \\(y\\) vs. all the predictor variables. (But, unfortunately, what we see in these 2-d graphs doesn’t always explain to us what we will “see” in the MLR model.) 3.3.2.1 Scatterplot matrix A scatterplot matrix plots all pairs of variables used in a model. The primary plots of interest will have the response \\(y\\) on the y-axis and the predictors on the x-axis. But the predictor plots (e.g. \\(x_1\\) vs. \\(x_2\\)) are useful to see if any predictors are related, which is a topic dicussed in more detail later in these notes. The basic scatterplot matrix command pairs takes in a data frame, minus any columns you don’t want plotted: &gt; names(brain) ## [1] &quot;Species&quot; &quot;Brain&quot; &quot;Body&quot; &quot;Gestation&quot; &quot;Litter&quot; &gt; # we want to omit column 1 (Species) from our graph: &gt; pairs(brain[,-1]) The top row shows scatterplots of the response Brain vs. the three predictors. All three indicate that transformations of all variables should be explored. A slightly nicer version that includes univariate density curves and correlation coefficients is made using ggpairs in the GGally package. This option fits a smoother curve to the scatterplots: &gt; library(GGally) &gt; ggpairs(brain, columns = c(&quot;Body&quot;,&quot;Gestation&quot;, &quot;Litter&quot;,&quot;Brain&quot;), + lower = list(continuous = &quot;smooth&quot;)) In this plot command, we select variables from brain in the columns argument with the response Brain listed last to make the lower row of the plot show the response Brain on the y-axis vs. all three predictors on the x-axes. Conclusion: transformations are likely needed. Since all variables have positive values, we can try logs first. We can’t use a scale argument in a scatterplot matrix to explore transformations. Instead, we can use the dplyr package to transform all variables (except Species) using the mutate_all function, then we plot using ggpairs again. Here we also added the ggpairs argument columnLabels to remind us what transformations were made: &gt; library(dplyr) &gt; brain %&gt;% select(-Species) %&gt;% # omit species + mutate_all(.funs = log) %&gt;% # apply the log function to all variables + ggpairs(columns = c(&quot;Body&quot;,&quot;Gestation&quot;, &quot;Litter&quot;,&quot;Brain&quot;), + lower = list(continuous = &quot;smooth&quot;), + columnLabels = c(&quot;log(Body)&quot;,&quot;log(Gestation)&quot;, &quot;log(Litter)&quot;,&quot;log(Brain)&quot;)) Does Litter need to be logged? If we don’t want to log all variables, we can use mutate_at instead of mutate_all: &gt; brain %&gt;% select(-Species) %&gt;% # omit species + mutate_at(.vars = c(&quot;Brain&quot;,&quot;Body&quot;,&quot;Gestation&quot;), .funs = log) %&gt;% # pick variables to log + ggpairs(columns = c(&quot;Body&quot;,&quot;Gestation&quot;, &quot;Litter&quot;,&quot;Brain&quot;), + lower = list(continuous = &quot;smooth&quot;), + columnLabels = c(&quot;log(Body)&quot;,&quot;log(Gestation)&quot;, &quot;Litter&quot;,&quot;log(Brain)&quot;)) 3.3.2.2 Jittered scatterplot: Jittered plots are useful when data points overlap (discrete variables like litter) and your sample size isn’t huge. Change the alpha transparency value with large data sets. Here we compare geom_point() against geom_jitter, using grid.arrange from the gridExtra package to put these plots side-by-side: &gt; base &lt;- ggplot(brain, aes(x=Litter, y=Brain)) + scale_y_log10() &gt; plotA &lt;- base + geom_point() + labs(title=&quot;unjittered&quot;) &gt; plotB &lt;- base + geom_jitter(width=.1) + labs(title=&quot;jittered&quot;) &gt; library(gridExtra) &gt; grid.arrange(plotA,plotB, nrow=1) The width=.1 amount specifies how much to jitter the points (we changed it here because the default amount didn’t display enough of a change). We can see the jittering most in the points clustered around Litter=0. 3.3.3 Residual plots for MLR If we have \\(p\\) predictors, then there are \\(p+1\\) residual plots to check: plot \\(r_i\\) against all predictors \\(x_1, \\dotsc, _p\\). The motivation for these plots is the same as SLR (residuals should not be related to the \\(x\\)’s) plot \\(r_i\\) against the fitted values \\(\\hat{y}_i\\). The motivation for this may be less clear, but the fitted values are simply a linear function of the predictor values: \\[ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1,i} + \\hat{\\beta}_2 x_{2,i} + \\dotsm + \\hat{\\beta}_p x_{p,i} \\] so the residuals should not be related to the fitted values if the model fits. All \\(p+1\\) plots should be checked. A quick starting point is the fitted value plot which is the first plot when plot-ing the lm. Here is the residuals vs. fitted plot for the untransformed variable model: &gt; # check residuals vs. fitted (y-hat) values: &gt; plot(brain.lm, which=1) This plot indicates non-linearity and non-constant variance. Here is the same residual plot for the model will all variables logged except Litter: &gt; brain.lm2 &lt;- lm(log(Brain) ~ log(Gestation) + log(Body) + Litter, data=brain) &gt; plot(brain.lm2, which=1) We can use the ggnostic function in GGally to get plots of residuals vs. all predictors: &gt; ggnostic(brain.lm2, columnsY = &quot;.resid&quot;) # from GGally The residual plots reveal some outliers that should be explored but the overall fit, while not perfect, is much better than the untransformed version. &gt; library(broom) &gt; library(knitr) &gt; kable(tidy(brain.lm2), digits=4) term estimate std.error statistic p.value (Intercept) 0.8234 0.6621 1.2437 0.2168 log(Gestation) 0.4396 0.1370 3.2095 0.0018 log(Body) 0.5745 0.0326 17.6009 0.0000 Litter -0.1104 0.0423 -2.6115 0.0105 The estimated median function for this model is \\[ \\widehat{median}(y \\mid x) = e^{0.8234}(Gest)^{0.4396}(Body)^{0.5745}e^{-0.1104(Litter)} \\] The effect of Gestation is interpreted as you would in a power model (both variables logged). For example, doubling gestational days is associated with an estimated $(2^{0.4396}-1)100%=$35.6% increase in median brain weight, holding body weight and litter size constant. The effect of Litter is interpreted as you would in an exponential model (with just the response logged). For example, after controlling for body size and gestation time, each additional offspring decreases estimated median brain weight by 10.5% (work is \\((e^{-0.1104}-1)100%=\\)). 3.3.4 EDA for interactions We visualize interactions by using a graphic that looks at the relationship between \\(y\\) and \\(x_1\\) while holding the value of \\(x_2\\) fixed (or almost fixed), and vice versa for flipping the role of \\(x\\) variables. An interaction may be needed if the relationship between \\(y\\) and \\(x_1\\) depends on the value of \\(x_2\\). 3.3.4.1 Predictors: one quantitative and one categorical For example, if \\(x_1\\) is quantitative and \\(x_2\\) is categorical, then we use facet_wrap (or facet_grid) to split the scatterplot of \\(y\\) vs. \\(x_1\\) by group of \\(x_2\\): &gt; ggplot(mydata, aes(x=x1, y=y)) + + geom_point() + + facet_wrap(~x2) 3.3.4.2 Predictors: both quantitative We use the same facet_wrap function, but we need to group the data into similar cases with respect to their \\(x_2\\) value, which is now assumed to be quantitative. One way to do this is to use the ntile(n=) function from dplyr package where n determines how many groups the data will be divided into. Is there an interaction between body weight and gestation?? Does the relationship between brain weight and gestation length change as we vary body weight? If yes, then we should include an interaction term between gestation and body weight. Here we hold body weight “fixed” by using the ntile command from the dplyr package to divide the data into chunks of animals with similar body weights. Here we pick n=4 for this function which divides the cases into 4 equal sized chunks based on the quartiles (4) of Body. In the plot below the cases in “1” are the lower 25% of body weights, “2” are the 25-50th percentile values of Body weights, etc. &gt; ggplot(brain, aes(x=Gestation, y=Brain)) + + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + scale_x_log10() + scale_y_log10() + + facet_wrap(~ ntile(Body, n=4)) Conclusion: the trend within each level of body weight is about the same. No obvious interactive effect of body weight and gestation length on brain weight. Of course we can always check significance by adding interaction term to model: &gt; brain.lm3 &lt;- lm(log(Brain) ~ log(Gestation) + log(Body) + Litter + log(Gestation):log(Body), data=brain) &gt; summary(brain.lm3) ## ## Call: ## lm(formula = log(Brain) ~ log(Gestation) + log(Body) + Litter + ## log(Gestation):log(Body), data = brain) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.95837 -0.29354 0.01594 0.27960 1.62013 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.64124 0.67137 0.955 0.342050 ## log(Gestation) 0.48758 0.14051 3.470 0.000797 *** ## log(Body) 0.69602 0.09268 7.510 3.91e-11 *** ## Litter -0.10050 0.04264 -2.357 0.020566 * ## log(Gestation):log(Body) -0.02678 0.01914 -1.399 0.165080 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4731 on 91 degrees of freedom ## Multiple R-squared: 0.9545, Adjusted R-squared: 0.9525 ## F-statistic: 477.4 on 4 and 91 DF, p-value: &lt; 2.2e-16 3.3.5 Quadratic models: Corn yields (exercise 9.15) This final example illustrates a quadratic model fit and scatterplot. Consider the corn yield data in textbook exercise 9.15. How is corn yield (measured bushels/acre) in a year related to the amount of rainfall (inches) in that summer? A linear model for yield against rainfall is not appropriate: &gt; corn &lt;- ex0915 &gt; summary(corn) ## Year Yield Rainfall ## Min. :1890 Min. :19.40 Min. : 6.800 ## 1st Qu.:1899 1st Qu.:29.95 1st Qu.: 9.425 ## Median :1908 Median :32.15 Median :10.500 ## Mean :1908 Mean :31.92 Mean :10.784 ## 3rd Qu.:1918 3rd Qu.:35.20 3rd Qu.:12.075 ## Max. :1927 Max. :38.30 Max. :16.500 &gt; ggplot(corn, aes(x=Rainfall, y=Yield)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + labs(title=&quot;SLR model&quot;) &gt; corn.lm1&lt;-lm(Yield ~ Rainfall, data =corn) &gt; plot(corn.lm1, which=1) We can add a quadratic term (using the I() operator): &gt; corn.lm2 &lt;- lm(Yield ~ Rainfall + I(Rainfall^2), data =corn) Alternatively, we can update model 1 (SLR) using the update command: on right side of formula the ~ . + newstuff says to add the newstuff to the old model formula which is denoted with the period . . &gt; corn.lm2 &lt;- update(corn.lm1, ~ . + I(Rainfall^2)) &gt; summary(corn.lm2) ## ## Call: ## lm(formula = Yield ~ Rainfall + I(Rainfall^2), data = corn) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.4642 -2.3236 -0.1265 3.5151 7.1597 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -5.01467 11.44158 -0.438 0.66387 ## Rainfall 6.00428 2.03895 2.945 0.00571 ** ## I(Rainfall^2) -0.22936 0.08864 -2.588 0.01397 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.763 on 35 degrees of freedom ## Multiple R-squared: 0.2967, Adjusted R-squared: 0.2565 ## F-statistic: 7.382 on 2 and 35 DF, p-value: 0.002115 &gt; plot(corn.lm2, which=1) This residual plot looks much better for this quadratic model compared to the linear model. We can visualize this quadratic model using the geom_smooth(method=&quot;lm&quot;) function but we have to specify this model form since the default is a SLR. This is done in the formula argument, using y and x to denote the x and y that you specify in the aes argument. &gt; ggplot(corn, aes(x=Rainfall, y=Yield)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, formula= y ~ x + I(x^2), se=FALSE) + + labs(title=&quot;quadratic fit!&quot;) The model fit is &gt; kable(tidy(corn.lm2), digits = 4) term estimate std.error statistic p.value (Intercept) -5.0147 11.4416 -0.4383 0.6639 Rainfall 6.0043 2.0389 2.9448 0.0057 I(Rainfall^2) -0.2294 0.0886 -2.5877 0.0140 so \\[ \\hat{\\mu}(yield \\mid rain) = -5.0147 + 6.0043(rain)- 0.2294(rain)^2 \\] An increase from 9 to 10 inches of rainfall is associated with a mean yield increase of 1.646 bushels per acre. \\[ 6.0043- 0.2294(2\\times 9 + 1) = 1.646 \\] An increase from 14 to 15 inches of rainfall is associated with a mean yield decrease of 0.648 bushels per acre. \\[ 6.0043- 0.2294(2\\times 14 + 1) = -0.648 \\] 3.4 Categorical Predictors Categorical predictors can be included in a regression model the same way that a quantitative predictor can. But since a categorical variable doesn’t have a numerical scaling, we don’t have a “linearity” assumption that needs to be met (though the three other assumptions still hold). A categorical variable is included by using one or more indicator variables (aka dummy variables) which indicate different levels of the variable. An indicator variable equals 1 to indicate the level of interest, and is 0 otherwise. The baseline level of an indicator variable is the level of the factor variable that doesn’t have an indicator variable made for it. R will create indicator variables for us in an lm, so there is no need to do this “by hand”. If a variable, which is stored as a factor in R, has \\(k\\) levels then we need \\(k-1\\) indicator variables. Consider the following examples to illustrate this idea. Suppose we are looking at how gender and education level are associated with income. If there are two genders in our data, here recorded as Female and Male, then we need one indicator to indicate one of the two levels. R will create an indicator for the second level, ordered alphabetically. If a case has Indicator_Male=1 then the case is Male but if the case has Indicator_Male=0 then the case is Female. So this one indicator variable can classify two levels and the baseline level is Female. Gender Indicator_Male Female 0 Male 1 Suppose we are looking at how number of farms per county in 1992 is related to number of farms in 1987 and region of the country. If there are four regions in our data, here recorded as NC, NE, S and W, then we need three indicators. R will create an indicator for all but the first level of NC. If a case has Indicator_NE=1 then the case is NE, if the case has Indicator_S=1 then the case is S, if the case has Indicator_W=1 then the case is W. If a case has all indicator values equal to 0, then the case is the baseline level of NC. So we need three indicators to classify the four region levels. Region Indicator_NE Indicator_S Indicator_W NC 0 0 0 NE 1 0 0 S 0 1 0 W 0 0 1 3.4.1 Interpretation: adding a categorical You add a categorical predictor x2 to the lm function just as you would any quantitative predictor: lm(y ~ x1 + x2, data). R automatically creates indicator variables for x2. When adding a categorical x2 which has, say levels A, B and C, the basic mean function form looks like: \\[ \\mu(Y \\mid x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_3 LevelB + \\beta_4 LevelC \\] The mean function for cases where \\(x_2=A\\) sets the indicators for levels B and C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=A) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (0) = \\beta_0 + \\beta_1 x_1 \\] The mean function for cases where \\(x_2=B\\) sets the indicator for level B equalt to 1 and the indicator for level C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=B) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (1) + \\beta_4 (0) = \\beta_0 + \\beta_1 x_1 + \\beta_3 \\] The mean function for cases where \\(x_2=C\\) sets the indicator for level C equalt to 1 and the indicator for level B equal to 0: \\[ \\mu(Y \\mid x_1, x_2=C) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (1) = \\beta_0 + \\beta_1 x_1 + \\beta_4 \\] Interpretation of indicator effects: \\(\\beta_3\\) is the difference between \\(\\mu(Y \\mid x_1, x_2=B)\\) and \\(\\mu(Y \\mid x_1, x_2=A)\\), so it measures the mean change between levels B and A, holding \\(x_1\\) fixed. \\(\\beta_4\\) is the difference between \\(\\mu(Y \\mid x_1, x_2=C)\\) and \\(\\mu(Y \\mid x_1, x_2=A)\\), so it measures the mean change between levels C and A, holding \\(x_1\\) fixed. \\(\\beta_3-\\beta_4\\) is the difference between \\(\\mu(Y \\mid x_1, x_2=B)\\) and \\(\\mu(Y \\mid x_1, x_2=C)\\), so it measures the mean change between levels B and C, holding \\(x_1\\) fixed. 3.4.1.1 Example: Agstrat Consider the agstrat data again, fitting the regression of square root of farms92 (number of farms/county in 1992) against square root of farms87 and region. &gt; agstrat&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/agstrat.csv&quot;) &gt; farms.lm &lt;- lm(sqrt(farms92) ~ sqrt(farms87) + region, data=agstrat) &gt; farms.lm ## ## Call: ## lm(formula = sqrt(farms92) ~ sqrt(farms87) + region, data = agstrat) ## ## Coefficients: ## (Intercept) sqrt(farms87) regionNE regionS regionW ## -0.9369 0.9798 -0.0826 0.7214 1.0100 The fitted model mean function is estimated as \\[ \\begin{split} \\hat{\\mu}(\\sqrt{farms92} \\mid \\sqrt{farms92}, region) &amp; = -0.9369 + 0.9798 \\sqrt{farms92} -0.0826(regionNE) + \\\\ &amp; 0.7214(regionS) + 1.0100(regionW) \\end{split} \\] For the baseline NC region, the estimated mean function is \\[\\begin{split} \\hat{\\mu}(\\sqrt{farms92} \\mid \\sqrt{farms92}, region=NC) &amp; = -0.9369 + 0.9798 \\sqrt{farms92} -0.0826(0) + \\\\ &amp; 0.7214(0)+ 1.0100(0) \\\\ &amp; = -0.9369 + 0.9798 \\sqrt{farms92} \\end{split} \\] For the S region, the estimated mean function is \\[\\begin{split} \\hat{\\mu}(\\sqrt{farms92} \\mid \\sqrt{farms92}, region=S) &amp; = -0.9369 + 0.9798 \\sqrt{farms92} -0.0826(0) + \\\\ &amp; 0.7214(1)+ 1.0100(0)\\\\ &amp; = -0.9369 + 0.9798 \\sqrt{farms92}+ 0.7214 \\\\ &amp; = -0.2155+ 0.9798 \\sqrt{farms92} \\end{split} \\] Interpretation \\(\\hat{\\beta}_3 = 0.7214\\) tells us that the mean square root farms92 is estimated to be 0.7214 units higher in the S compared to the NC region, holding farms87 constant. But the effect of farms87 is the same, regardless of region: holding region constant, a one unit increase in the square root of farms87 is associated with a 0.9798 unit increase in the mean square root of farms92. With one quatitative preditor and one categorical predictor, we can visual this data by coloring plot symbols by region. We can also add parallel lines that show how farms87 is related to farms92 in each region. We do this by adding the model’s fitted values (\\(\\hat{y}\\)’s) to the augmented data frame and plotting lines, by region, with these fitted values as the y-axis measurement. &gt; library(ggplot2) &gt; library(broom) &gt; agstrat.aug &lt;- augment(farms.lm) &gt; head(agstrat.aug) ## # A tibble: 6 x 10 ## sqrt.farms92. sqrt.farms87. region .fitted .se.fit .resid .hat .sigma ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26.9 29.3 NC 27.7 0.0895 -0.821 0.00977 0.906 ## 2 25.7 25.9 NC 24.4 0.0902 1.21 0.00992 0.905 ## 3 39.8 41.6 NC 39.9 0.121 -0.0895 0.0178 0.907 ## 4 34.1 35.7 NC 34.1 0.100 0.0269 0.0123 0.907 ## 5 21.2 22.0 NC 20.6 0.0965 0.569 0.0113 0.907 ## 6 24.1 26.4 NC 25.0 0.0898 -0.823 0.00983 0.906 ## # ... with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; &gt; ggplot(agstrat.aug, aes(x=sqrt.farms87., y=sqrt.farms92., color=region)) + + geom_point() + + geom_line(data=agstrat.aug, aes(y=.fitted), size=1) + + labs(title=&quot;Parallel line model&quot;, + x=&quot;square root of number of farms in 1987&quot;, + y=&quot;square root of number of farms in 1992&quot;) The difference of \\(\\hat{\\beta}_3 = 0.7214\\) is shown by the fact that the blue South line is always 0.7214 units above the red North Central line for any value of farms87. The small value of \\(\\hat{\\beta}_2 = -0.0826\\) for the NE indicator show that there is basically no difference between the NE and NC lines, which can be seen by the almost overlapping red and green lines. 3.4.2 Interpretation: adding a categorical interaction What if we thought that the effect of \\(x_1\\) depends on the level of the categorical variable \\(x_2\\). E.g. What if the slopes for each region in the above scatterplot were allowed different? To create a separate lines model we would add the interaction of \\(x_1\\) and \\(x_2\\): lm(y ~ x1 * x2, data). When adding a categorical x2 which has, say levels A, B and C, the basic mean function form looks like: \\[ \\mu(Y \\mid x_1, x_2) = \\beta_0 + \\beta_1 x_1 + \\beta_3 LevelB + \\beta_4 LevelC + \\beta_5 x_1LevelB + \\beta_6 x_1LevelC \\] The mean function for cases where \\(x_2=A\\) sets the indicators for levels B and C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=A) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (0) = \\beta_0 + \\beta_1 x_1 + \\beta_5 x_1(0) + \\beta_6 x_1(0) = \\beta_0 + \\beta_1 x_1 \\] The mean function for cases where \\(x_2=B\\) sets the indicator for level B equalt to 1 and the indicator for level C equal to 0: \\[ \\mu(Y \\mid x_1, x_2=B) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (1) + \\beta_4 (0) + \\beta_5 x_1(1) + \\beta_6 x_1(0) = (\\beta_0+ \\beta_3) + (\\beta_1+\\beta_5) x_1 \\] The mean function for cases where \\(x_2=C\\) sets the indicator for level C equalt to 1 and the indicator for level B equal to 0: \\[ \\mu(Y \\mid x_1, x_2=C) = \\beta_0 + \\beta_1 x_1 + \\beta_3 (0) + \\beta_4 (1) + \\beta_5 x_1(0) + \\beta_6 x_1(1)= (\\beta_0+ \\beta_4) + (\\beta_1+\\beta_6) x_1 \\] Interpretation of indicator effects: \\(\\beta_3\\) is the difference between \\(\\mu(Y \\mid x_1=0, x_2=B)\\) and \\(\\mu(Y \\mid x_1=0, x_2=A)\\) at the intercept where \\(x_1=0\\). \\(\\beta_4\\) is the difference between \\(\\mu(Y \\mid x_1=0, x_2=C)\\) and \\(\\mu(Y \\mid x_1=0, x_2=A)\\) at the intercept where \\(x_1=0\\). \\(\\beta_1\\) is the effect of \\(x_1\\) when \\(x_2 = A\\) (level A) \\(\\beta_1+\\beta_5\\) is the effect of \\(x_1\\) when \\(x_2 = B\\) (level B) \\(\\beta_5\\) measures how much more or less the effect of \\(x_1\\) is in level B compared to level A (difference in slopes of the two lines) \\(\\beta_1+\\beta_6\\) is the effect of \\(x_1\\) when \\(x_2 = C\\) (level C) \\(\\beta_6\\) measures how much more or less the effect of \\(x_1\\) is in level C compared to level A (difference in slopes of the two lines) 3.4.2.1 Example: Sleep This data sleep contains information on the sleep and physical characteristics of 62 species of mammals. Our goal is to see how the total amount of sleep (hours) that an animal gets in 24 hours is related to its body weight (kg) and the amount of danger (low danger to high danger) to which it is exposed. &gt; sleep &lt;- read.csv(&quot;http://math.carleton.edu/kstclair/data/sleep.csv&quot;) &gt; sleep %&gt;% select(TS, BodyWt, D) %&gt;% summary() ## TS BodyWt D ## Min. : 2.60 Min. : 0.005 Min. :1.000 ## 1st Qu.: 8.05 1st Qu.: 0.600 1st Qu.:1.000 ## Median :10.45 Median : 3.342 Median :2.000 ## Mean :10.53 Mean : 198.790 Mean :2.613 ## 3rd Qu.:13.20 3rd Qu.: 48.203 3rd Qu.:4.000 ## Max. :19.90 Max. :6654.000 Max. :5.000 ## NA&#39;s :4 Our EDA above shows that danger levelD is numerically coded, with a “1” indicating very low danger level up to “5” indicating very high danger. If we used this version in our model that forces a linear relationship, which means a constant change in TS for each bump up in danger level, which is a strong assumption to make given the categorical nature of this variable. Plus, this constant rate of change does not look to be the case based on EDA: &gt; boxplot(TS ~ D, data=sleep) We must make a factor version of this variable to correctly use it in our model. We will use the fct_recode command from the forcats package: &gt; table(sleep$D) # EDA counts for each level ## ## 1 2 3 4 5 ## 19 14 10 10 9 &gt; library(forcats) &gt; sleep$danger &lt;- fct_recode(factor(sleep$D), + &quot;very low&quot; = &quot;1&quot;, + &quot;low&quot; = &quot;2&quot;, + &quot;moderate&quot; = &quot;3&quot;, + &quot;high&quot; = &quot;4&quot;, + &quot;very high&quot; = &quot;5&quot;) &gt; table(sleep$danger) # check that we recoded correctly ## ## very low low moderate high very high ## 19 14 10 10 9 &gt; levels(sleep$danger) # very low is the first=baseline level ## [1] &quot;very low&quot; &quot;low&quot; &quot;moderate&quot; &quot;high&quot; &quot;very high&quot; Always do an EDA check (here, checking counts) when manipulating or recoding a variable. Our factor variable danger’s first level is very low which will be the baseline level for any model fit in R. When we add lm lines to a scatterplot with color coded points, then ggplot will fit separate lines for each color. Here we plot TS (total sleep) against the logged version of BodyWt since we don’t have a linear relationship on the untransformed scale. &gt; library(ggplot2) &gt; ggplot(sleep, aes(x=BodyWt, y=TS, color=danger)) + + geom_point() + + geom_smooth(method=&quot;lm&quot;, se=FALSE) + + scale_x_log10() + + labs(title=&quot;daily sleep vs. body weight by danger level&quot;) This plot suggests that there may be an interaction between danger and BodyWt since the lm lines have some variation in slope across danger levels. The model we will fit has ten parameters ($_0-_9) due to the four indicator variables that are also interacted with body weight: \\[\\begin{split} \\mu(TS \\mid body, danger) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 low + \\beta_3 moderate + \\beta_4 high + \\\\ &amp; \\beta_5 veryhigh + \\beta_6 \\log(body)low + \\beta_7 \\log(body)moderate + \\\\ &amp; \\beta_8 \\log(body)high + \\beta_9 \\log(body)veryhigh \\end{split} \\] &gt; sleep.lm &lt;- lm(TS ~ log(BodyWt)*danger, data=sleep) &gt; sleep.lm ## ## Call: ## lm(formula = TS ~ log(BodyWt) * danger, data = sleep) ## ## Coefficients: ## (Intercept) log(BodyWt) ## 13.86818 -0.58104 ## dangerlow dangermoderate ## -2.35924 -3.50087 ## dangerhigh dangervery high ## -4.22738 -7.03108 ## log(BodyWt):dangerlow log(BodyWt):dangermoderate ## -0.03446 -0.34437 ## log(BodyWt):dangerhigh log(BodyWt):dangervery high ## 0.16966 -0.09638 The estimated mean function is \\[\\begin{split} \\mu(TS \\mid body, danger) &amp;= 13.868 -0.581 \\log(body)-2.359 low -3.501 moderate -4.227 high - \\\\ &amp; 7.031 veryhigh -0.034 \\log(body)low -0.344\\log(body)moderate + \\\\ &amp; 0.170 \\log(body)high -0.096 \\log(body)veryhigh \\end{split} \\] There are many parameters to interpret, so here are a few examples: \\(\\beta_1\\) is the effect of body weight for species in the “very low” danger level. To see this, write out the mean functions for the two levels. For very low level: \\[\\begin{split} \\mu(TS \\mid body, danger=verylow) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (0) + \\beta_4 (0) + \\beta_5 (0) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(0) + \\beta_8 \\log(body)(0) + \\beta_9 \\log(body)(0) \\\\ &amp; = \\beta_0 + \\beta_1 \\log(body) \\end{split}\\] \\(\\beta_9\\) tells us how the effect of body weight differs for the very low and very high danger levels. Compare the mean response for very low to the mean response for the very high level: \\[\\begin{split} \\mu(TS \\mid body, danger=veryhigh) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (0) + \\beta_4 (0) + \\beta_5 (1) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(0) + \\beta_8 \\log(body)(0) + \\beta_9 \\log(body)(1) \\\\ &amp; = (\\beta_0 + \\beta_5) + (\\beta_1 + \\beta_9) \\log(body) \\end{split}\\] In the “very low” model, \\(\\beta_1\\) measures the effect of body weight on mean sleep while in the “very high” model, \\(\\beta_1 + \\beta_9\\) measures this effect. So the parameter \\(\\beta_9\\) tells us how this effect differs for the two danger levels. The small estimated value \\(\\hat{\\beta}_9 = -0.096\\), along with the separate lines scatterplot, suggest little difference in the effect of body weight in these two danger groups. The parameter function \\(\\beta_5 + \\beta_9 \\log(body)\\) tells us the difference in total sleep between the very high and very low danger levels at a given value of body weight. Recall that in this interaction model, the effect of danger should depend on the value of body weight. For example, for animals that are 1kg in weight, the parameter \\(\\beta_5 + \\beta_9\\log(1) = \\beta_5\\) measures the difference in mean total sleep for the very high and very low levels. The estimated parameter value of \\(\\hat{\\beta}_5 = -7.031\\) and the large difference shown in the scatterplot suggest that for 1 kg animals, that the average total sleep for animals with very high danger is about 7 hours less than for animals in very low danger. To determine if the effect of body weight on mean total sleep differs for “high” and “moderate” levels of danger, we would test the hypotheses \\[ H_0: \\beta_8 = \\beta_7 \\ \\ \\ \\ vs. \\ \\ \\ \\ H_A: \\beta_8 \\neq \\beta_7 \\] Again, write down the mean equations for these two levels. For “high” all dummy variables equal 0 expect high equals 1: \\[\\begin{split} \\mu(TS \\mid body, danger=high) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (0) + \\beta_4 (1) + \\beta_5 (0) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(0) + \\beta_8 \\log(body)(1) + \\beta_9 \\log(body)(0) \\\\ &amp; = (\\beta_0 + \\beta_4) + (\\beta_1 + \\beta_8) \\log(body) \\end{split} \\] for “moderate” all dummy variables equal 0 expect moderate equals 1. \\[\\begin{split} \\mu(TS \\mid body, danger=moderate) &amp;= \\beta_0 + \\beta_1 \\log(body) + \\beta_2 (0) + \\beta_3 (1) + \\beta_4 (0) + \\beta_5 (0) \\\\ &amp; + \\beta_6 \\log(body)(0) + \\beta_7 \\log(body)(1) + \\beta_8 \\log(body)(0) + \\beta_9 \\log(body)(0) \\\\ &amp; = (\\beta_0 + \\beta_3) + (\\beta_1 + \\beta_7) \\log(body) \\end{split}\\] In the “high” model, \\(\\beta_1 + \\beta_8\\) measures the effect of body weight on mean sleep while in the “moderate” model, \\(\\beta_1 + \\beta_7\\) measures this effect. Our null hypothesis would state that these two effects are equal: \\[ H_0: \\beta_1 + \\beta_8 = \\beta_1 + \\beta_7 \\] which simplifies to the hypothesis given above. 3.5 Inference for MLR Inference methods for individual \\(\\beta\\)-parameters, the mean function \\(\\mu\\) and predictions are all very similar to the inference methods outlined for SLR models in Sections 2.5 and 2.6. The biggest difference is that we now use a t-distribution with degrees of freedom equal to \\(\\pmb{n-(p+1)}\\) where \\(p+1\\) is equal to the number of \\(\\beta\\)’s in the mean function. To review: Confidence Intervals for \\(\\pmb{\\beta}_i\\): A \\(C\\)% confidence for \\(\\beta_i\\) has the form: \\[ \\hat{\\beta}_i \\pm t^* SE(\\hat{\\beta}_i) \\] where \\(t^*\\) is the \\((100-C)/2\\) percentile from the t-distribution with \\(df=n-(p+1)\\) degrees of freedom. Hypothesis tests We can test the hypothesis \\[ H_0: \\beta_i = 0 \\ \\ \\ \\ vs. \\ \\ \\ \\ H_A: \\beta_i \\neq 0 \\] with the following t-test statistic: \\[ t =\\dfrac{\\hat{\\beta}_i - 0}{SE(\\hat{\\beta}_i)} \\] The t-distribution with \\(n-(p+1)\\) degrees of freedom is used to compute the p-value that is appropriate for whatever \\(H_A\\) is specified. Interpretation of results: Recall the general interpretation for the planar model: \\(\\beta_i\\) is the effect of a one unit increase in \\(x_i\\), holding all other predictors constant. Testing \\(\\pmb{\\beta}_i =0\\) is the same as asking: Is the observed effect of \\(x_i\\) on \\(\\mu\\) statistically significant after accounting for all other terms in the model? For this reason, an individual t-test is only good for determing if \\(x_i\\) is needed in a model that contains all other terms. If you want to test the significance of multiple predictors at once, you need to conduct an F-test using ANOVA (Section 3.6. Inference for \\(\\pmb{\\mu(Y \\mid X_0)}\\) and \\(\\pmb{pred(Y \\mid X_0)}\\): Confidence and prediction intervals calculations are very similar to SLR in Section 3.5.1, except for the change in the degrees of freedom and a slightly more complex form for the SE of the estimated mean response \\(SE(\\hat{\\mu}(Y \\mid X_0))\\) which now involves more than one predictor. 3.5.1 Inference for a linear combination of \\(\\beta\\)’s One new inference method which is sometimes needed in MLR models, is to make inferences about a linear combination of \\(\\beta_i\\)’s. A linear combination is of the form: \\[ \\gamma = c_i \\beta_i + c_j \\beta_j \\] where \\(c_i\\) and \\(c_j\\) are known numbers. Some examples of a linear combination include: Suppose we fit the quadratic model in Section 3.2.1.2 and want to estimate the change in the mean response for a change in \\(x_1\\) from 9 to 10. This change is measured by the parameter \\[ \\gamma = \\beta_1 + \\beta_2(2\\times 9+1) = \\beta_1 + 19\\beta_2 \\] so \\(c_1 = 1\\) and \\(c_2 = 19\\). Suppose we fit the interaction model in Section 3.2.1.3 and want to estimate the change in the mean response for a 1 unit increase in \\(x_1\\), holding \\(x_2\\) constant at the value of -5. This change is measured by the parameter \\[ \\gamma = \\beta_1 + \\beta_3 (-5) = \\beta_1 -5 \\beta_3 \\] so \\(c_1 = 1\\) and \\(c_3 = -5\\). Any inference for this new \\(\\gamma\\) parameter will rely on an estimate, \\(\\hat{\\gamma}\\) and standard error \\(SE(\\hat{\\gamma})\\). Estimation: Just plug in our \\(\\beta\\) estimates! \\[ \\hat{\\gamma} = c_i \\hat{\\beta}_i + c_j \\hat{\\beta}_j \\] SE: This is more complex of a calculation because the estimates \\(\\hat{\\beta}_i\\) and \\(\\hat{\\beta}_i\\) are correlated. We can see this for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) in the SLR simulation in Section 2.4.4. We use probabilities rules for the linear combination of two correlated random variables to derive our SE as \\[ SE(\\hat{\\gamma}) = \\sqrt{c^2_i Var(\\hat{\\beta}_i) + c^2_j Var(\\hat{\\beta}_j) + 2c_i c_j Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) } \\] The variances (\\(Var\\)) values are the squared SE’s for each estimate, e.g. \\(Var(\\hat{\\beta}_i) = SE(\\hat{\\beta}_i)^2\\). The covariance (\\(Cov\\)) value measures how the two estiamtes co-vary together over many, many samples from the populations (just like SE tells us how each estimate varies by itself). Positive values of covariance means the two estimates are positively correlated, negative means negatively correlated. The magnitude of the covariance is a function of each estimate’s SE. In R, we will get these variance and covariance values from a MLR model’s estimated covariance matrix, which for a MLR planar model with two predictors will look like a 3x3 matrix with variance values on the diagonal and covariance values on the off-diagonals: \\[ \\begin{pmatrix} Var(\\hat{\\beta}_0) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_1) &amp; Cov(\\hat{\\beta}_0, \\hat{\\beta}_2) \\\\ Cov(\\hat{\\beta}_1, \\hat{\\beta}_0) &amp; Var(\\hat{\\beta}_1)&amp; Cov(\\hat{\\beta}_1, \\hat{\\beta}_2) \\\\ Cov(\\hat{\\beta}_2, \\hat{\\beta}_0)&amp; Cov(\\hat{\\beta}_2, \\hat{\\beta}_1)&amp; Var(\\hat{\\beta}_2) \\end{pmatrix} \\] Note that covariance is symmetric so that \\(Cov(\\hat{\\beta}_i, \\hat{\\beta}_j) = Cov(\\hat{\\beta}_j, \\hat{\\beta}_i)\\). Inference for \\(\\gamma\\) then looks like: Confidence interval for \\(\\pmb{\\gamma}\\): \\(\\hat{\\gamma} \\pm t^*_{df} SE(\\hat{\\gamma})\\) Hypothesis test for \\(\\pmb{H_0: \\gamma = \\gamma^*}\\): uses the test statistic \\[ t = \\dfrac{\\hat{\\gamma} - \\gamma^*}{SE(\\hat{\\gamma})} \\] and a p-value from a t-distribution with model degrees of freedom. 3.5.2 Example: Agstrat Let’s continue with inference for the parallel lines model for modeling the number of farms in 1992 as a function of region and number of farms in 1987 (Section 3.4.1.1). The mean model form is \\[ \\begin{split} \\mu(\\sqrt{farms92} \\mid \\sqrt{farms92}, region) &amp; = \\beta_0 + \\beta_1 \\sqrt{farms92} + \\beta_2(regionNE) + \\\\ &amp; \\beta_3(regionS) + \\beta_4(regionW) \\end{split} \\] &gt; agstrat&lt;- read.csv(&quot;http://people.carleton.edu/~kstclair/data/agstrat.csv&quot;) &gt; farms.lm &lt;- lm(sqrt(farms92) ~ sqrt(farms87) + region, data=agstrat) &gt; kable(tidy(farms.lm, conf.int=TRUE), digits = 3) term estimate std.error statistic p.value conf.low conf.high (Intercept) -0.937 0.191 -4.905 0.000 -1.313 -0.561 sqrt(farms87) 0.980 0.006 162.869 0.000 0.968 0.992 regionNE -0.083 0.219 -0.377 0.706 -0.514 0.349 regionS 0.721 0.123 5.884 0.000 0.480 0.963 regionW 1.010 0.170 5.933 0.000 0.675 1.345 Compare S to NC: \\(\\hat{\\beta}_3 = 0.721\\) tells us that the mean square root farms92 is estimated to be 0.721 units higher in the S compared to the NC region, holding farms87 constant. The t-test results in the summary output test \\(H_0: \\beta_3 = 0\\), which has a t-test statistic of \\(t = 0.721/0.123 = 5.884\\) which gives a p-value that is less than 0.001. Interpretation: Holding all other model terms constant (including other indicators for W and NE), there is a statistically significant difference between the mean number of farms in 1992 between South and North Central regions for any given value of farms in 1987. I am 95% confident that the average square root of farms in 1992 is anywhere from 0.48 to 0.963 units higher in the south than in the north central regions, holding the number of farms in 1987 fixed. Compare NE to NC: \\(\\hat{\\beta}_2 = -0.083\\) tells us that the mean square root farms92 is estimated to be 0.083 units lower in the NE compared to the NC region, holding farms87 constant. The t-test results in the summary output test \\(H_0: \\beta_2 = 0\\), which has a t-test statistic of \\(t = -0.377\\) which gives a p-value that is 0.706. Interpretation: Holding all other model terms constant (including other indicators for S and W), there is no statistically significant difference between the mean number of farms in 1992 between Northeast and North Central regions for any given value of farms in 1987. Compare S to W: In the parallel line model, we would like to know if there is a difference in the mean number of farms in 1992 in the S and W regions, after controlling for the number of farms in 1987. The question can be answered by comparing \\[ \\mu(farms92 \\mid farms87, region=S) - \\mu(farms92 \\mid farms87, region=W) = \\beta_3 - \\beta_4 \\] The hypotheses are then \\[ H_0: \\beta_3 - \\beta_4 = 0 \\ \\ vs. \\ \\ H_A: \\beta_3 - \\beta_4 \\neq 0 \\] Note that this is not the same as asking if each parameter equals 0. We just want to know if they are the same value, so we cannot use a t-test given in the summary output. This is a question about a linear combination of parameters: \\[ \\gamma = \\beta_3 - \\beta_4 \\] where \\(c_3 = 1\\) and \\(c_4 = -1\\). Our estimated parameter difference is \\[ \\hat{\\gamma} = \\hat{\\beta}_3 - \\hat{\\beta}_4 = 0.7214 - 1.0100 = -0.2886 \\] &gt; farms.lm ## ## Call: ## lm(formula = sqrt(farms92) ~ sqrt(farms87) + region, data = agstrat) ## ## Coefficients: ## (Intercept) sqrt(farms87) regionNE regionS regionW ## -0.9369 0.9798 -0.0826 0.7214 1.0100 &gt; # estimate of beta3 - beta4 (do S and W lines have same intercepts?) &gt; est &lt;- 0.7214 - 1.0100 &gt; est ## [1] -0.2886 We use the vcov(my.lm) command to get the estimated model’s covariance matrix: &gt; vcov(farms.lm) ## (Intercept) sqrt(farms87) regionNE regionS ## (Intercept) 0.036481631 -1.015899e-03 -0.0132540419 -0.013273838 ## sqrt(farms87) -0.001015899 3.619176e-05 0.0001884077 0.000189113 ## regionNE -0.013254042 1.884077e-04 0.0480149402 0.008949944 ## regionS -0.013273838 1.891130e-04 0.0089499443 0.015030978 ## regionW -0.013332531 1.912040e-04 0.0089608296 0.008964555 ## regionW ## (Intercept) -0.013332531 ## sqrt(farms87) 0.000191204 ## regionNE 0.008960830 ## regionS 0.008964555 ## regionW 0.028986383 The parts we need are \\(Var(\\hat{\\beta}_3) = 0.015030978\\), \\(Var(\\hat{\\beta}_4) = 0.028986383\\), and \\(Cov(\\hat{\\beta}_3, \\hat{\\beta}_4) = 0.008964555\\). The SE of our estimated difference is then \\[ SE(\\hat{\\gamma}) = \\sqrt{(1)^20.015030978 + (-1)^20.028986383 + 2(1)(-1)0.008964555} = 0.1615186 \\] &gt; # SE of this estimate &gt; se.est &lt;- sqrt(0.015030978 + (-1)^2*0.028986383 + 2*(-1)*0.008964555) &gt; se.est ## [1] 0.1615186 The test statistic is then \\(t = (-0.2886 - 0)/0.1615186 = -1.786791\\) which yields a 2-sided p-value of 0.075. This suggests that there is moderately weak evidence that there is a difference in means between the south and west, holding farms in 1987 fixed, but it is not statistically significant at the 5% level. &gt; # test stat &gt; t &lt;- (est - 0)/se.est &gt; t ## [1] -1.786791 &gt; 2*pt(t,df=295) ## [1] 0.07499793 3.5.3 Example: Sleep Let’s continue with inference for the separate lines model for modeling the amount of daily sleep for variety of animals as a function of their body size and danger level (Section 3.4.2.1). 3.6 ANOVA F-tests "],
["logistic.html", "Chapter 4 Logistic Regression", " Chapter 4 Logistic Regression "],
["poisson.html", "Chapter 5 Poisson Regression", " Chapter 5 Poisson Regression "],
["rrstudio.html", "A R and Rstudio A.1 Running Rstudio A.2 Installing R A.3 Installing Rstudio A.4 Installing R packages", " A R and Rstudio R is a free professional statistical software that is available for use on windows, mac and linux computers. R is a popular tool for researchers from many fields so acquiring basic R skills from our stats classes will be beneficial for this course and career plans! RStudio is a free software that provides a user-friendly interface with R. We will be running R through RStudio in our stats classes. R can be more challenging for a brand new user than other software (like Excel, SPSS, etc) because analyzes are done using written commands rather than using a drop down (point-and-click) menu. But R is very powerful because of the huge variety of statistical methods that it supports (due to the addition of free user contributed packages) and the user’s ability to customize their experience (graphics, new functions, data manipulation, etc). Because R is based on written commands that can be recorded in a variety of ways, it is easy for a user to reproduce, re-do or continue analyzes that were started at a different point in time. This is much harder to do when you are using a bunch of drop-down menu commands to run your analysis! We will emphasize reproducibility in this course by using R Markdown scripts (Section D) to complete our analyzes. A.1 Running Rstudio Browser: You can access an online version of Rstudio from https://mirage.mathcs.carleton.edu. If you are new to using Rstudio, I encourage you to use mirage rather than installing R/Rstudio on your computer. You can access mirage from anywhere on campus using Eduroam wireless if on a laptop, but from off campus you will first need to turn on Carleton’s VPN prior to logging in. Please install this software! Why use mirage? Any work you create (R scripts, Markdown, pdf, or word docs) are saved on your account located on this server (file path /Accounts/username). Mirage also has most R packages that we use in our classes preinstalled so you don’t need to install them before using them. Getting files off of Mirage: If you want to download a file from this account, check the button next to the file in the Files pane (lower right panel). (Don’t use the “File” dropdown menu for this!) Then select the More &gt; Export drop down menu from this pane and click Download. The file should be located in the default download location for your browser. Getting files onto Mirage: I will give you many files (usually .Rmd) in this class that you will want to upload, or add, to your mirage Rstudio account. To do this, download the file to your computer to a place you know about like Downloads or Desktop. Then in your Math245 project folder on mirage, click the upload button from the Files pane (lower right panel). (Don’t use the “File” dropdown menu for this!) Click Choose file and navigate to the location of the file on your computer. Click Open and Ok to upload this file to your mirage account. Personal computer: You can download R (Section A.2) and Rstudio (Section A.3) software onto your personal computer. You then open Rstudio to start an R session. You will need install certain R packages that we used in our class that aren’t part of the default package installation. See Section A.4. A.2 Installing R Follow the appropriate link below and complete the default installation. Windows: http://cran.r-project.org/bin/windows/base/ Mac: http://cran.r-project.org/bin/macosx/ A.3 Installing Rstudio Follow the link below and download the free RStudio Desktop Open Source Edition. Windows or Mac: http://www.rstudio.com/ide/download/ A.4 Installing R packages R packages provided added analysis tools that R users contribute to the R community. The default R installation only provides us with a fraction of the available packages. The most straightforward way to install a needed package in Rstudio is to click the Packages tab in the lower right pane. Click the Install button and start typing the package name into Packages field, then click Install to add the package to your available package library. You should only need to install a package once. Alternatively, you can install a package by typing an install.packages command into the Console window. For example, the following command installs the R data package Sleuth3 for The Statistical Sleuth textbook: &gt; install.packages(&quot;Sleuth3&quot;) "],
["renviron.html", "B The R enviroment B.1 Workspace B.2 Working directory B.3 Rstudio projects", " B The R enviroment B.1 Workspace Anything that you load or create in Rstudio (data, vectors, models) are called objects. You can see your current objects in your RStudio Environment pane (upper right). These objects, along with your command history, are contained in what R calls your “Workspace”. When you exit Rstudio, you may be asked if you want to save your workspace as a .RData file before exiting. I strongly encourage you not to do this since it can make it very hard to reproduce your workflow (and it can slow down Rstudio start up). Instead, use R markdown (Chapter D) to document your workflow so you can redo any analysis that you’ve previously done. To change Rstudio’s default startup and exit workspace behavior: From the Tools drop down menu (at the top), select Global Options… Under the General tab, uncheck the “Restore .RData into workspace at startup” In the “Save workspace to .RData on exit:” dropdown, select Never B.2 Working directory The default location that R looks for “stuff” or saves “stuff” is called your Working Directory. For example, the default location of this folder is typically your Documents folder for a Windows machine. You can run the getwd() command to see where your current working directory is located. For my desktop Windows computer, the default working directory location is &gt; getwd() [1] &quot;C:/Users/kstclair/Documents&quot; Or from the mirage server, &gt; getwd() [1] &quot;/Accounts/kstclair&quot; B.3 Rstudio projects RStudio (not standalone R) has a feature called Projects that make it easy to start RStudio in a particular working directory. You can create different Projects for different classes (or research projects). Your first task in Rstudio for Math 245 will be to create a Math 245 project: Find the Project button in the upper right-hand corner of Rstudio. Select New Project. Click New Directory from the New Project dialog box. Click on New project (again), then enter Math245 as your Directory name (no spaces). Use the Browse button to put this project in a good spot on your computer or accept the default location on mirage. Click on Create Project. Your Rstudio session should not change in looks all that much but check your computer should now contain a Math245 folder in the location you chose. This folder will contain a Math245.Rproj icon. Your working directory is now set to this Math245 folder. Check this with the getwd() command. Starting your Math245 project: Rstudio default settings are to start up your last project when you reopen Rstudio. So just opening Rstudio (or loggin onto mirage) usually opens your Math245 project. You will see your project name in the upper right-hand project button location. Alternatively, start your R project by double clicking on the project icon in your Math245 folder. You will know you are in You can also open or change projects with the drop-down project menu. "],
["rreview.html", "C R for basic data analysis C.1 Basics C.2 Data C.3 EDA C.4 Factor variables", " C R for basic data analysis C.1 Basics C.1.1 Quick Tips In the Console window: You type commands after the prompt &gt; and hit Enter to execute the command. The results of the command will be displayed below the command and preceded by a line counter enclosed in brackets (e.g. [1]). You can scroll back to, edit, and execute previous commands using the up arrow on your keyboard. If you prematurely hit enter before a command is completed to R’s satisfaction you will see a + prompt which asks you to finish the command. Simply complete the command after the + and hit Enter. If you get stuck in a cycle of + prompts you can get back to a fresh command prompt &gt; by hitting the Esc key. For example, try: &gt; 1+3 &lt;ENTER&gt; [1] 4 &gt; 1+ + 3 &lt;ENTER&gt; [1] 4 &gt; 1+ + &lt;ESC&gt; User interrupt requested For information about a command type ?commandname. C.1.2 Objects Everything in R is an object. You can assign a name to an object by using the assignment operator &lt;-. You can see the current list of objects in your R workspace by typing ls() or by looking in the Environment tab in Rstudio. You can remove objects from your workspace with the command rm(name) where name is the name of the object you want to delete. C.1.3 Vectors Vectors are a simple type of object and even single numbers (called scalars) are vectors with a length of 1. Here are a few ways to create vectors. The : operator creates a sequence of numbers that increase or decrease by 1. &gt; 3:10 &gt; 0:-3 &gt; 3:10 ## [1] 3 4 5 6 7 8 9 10 &gt; 0:-3 ## [1] 0 -1 -2 -3 The seq function also creates sequences of a given length or increment. &gt; seq(1,3,by=.5) ## [1] 1.0 1.5 2.0 2.5 3.0 &gt; seq(1,3,length=10) ## [1] 1.000000 1.222222 1.444444 1.666667 1.888889 2.111111 2.333333 ## [8] 2.555556 2.777778 3.000000 Use the combine function c() to create any particular arrangement: &gt; c(5,2,4,-1) ## [1] 5 2 4 -1 &gt; x &lt;- c(5,2,4,-1) &gt; x ## [1] 5 2 4 -1 &gt; y &lt;- c(x,0:3,x) &gt; y ## [1] 5 2 4 -1 0 1 2 3 5 2 4 -1 The rep command allows repetition: &gt; rep(0,5) ## [1] 0 0 0 0 0 &gt; rep(c(0,1),c(3,2)) ## [1] 0 0 0 1 1 You can also create vectors of characters (letters or words): &gt; c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;) ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &gt; c(&quot;abc&quot;,&quot;de&quot;) ## [1] &quot;abc&quot; &quot;de&quot; Logical (true/false) vectors do not use double quotes: &gt; c(T,T,F) ## [1] TRUE TRUE FALSE C.1.4 Arithmetic Here’s some basic commands: &gt; x &lt;- 1:4 &gt; x*2 ## [1] 2 4 6 8 &gt; x^2 ## [1] 1 4 9 16 &gt; (x-2)/3 ## [1] -0.3333333 0.0000000 0.3333333 0.6666667 Arithmetic involving vectors is done elementwise. &gt; y &lt;- c(-1,1,-1,1) &gt; x*y ## [1] -1 2 -3 4 &gt; x-y ## [1] 2 1 4 3 Special functions are available that work elementwise or on the entire vector. Here are a few: &gt; sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 &gt; log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 &gt; exp(x) ## [1] 2.718282 7.389056 20.085537 54.598150 &gt; sum(x) ## [1] 10 &gt; mean(x) ## [1] 2.5 &gt; sd(x) ## [1] 1.290994 &gt; sqrt(sum(x^2)) ## [1] 5.477226 More summary stats can be found with the commands min, max, median, quantile, summary, and var. You usually use the special functions above with numeric vectors (int or num), but you can use these functions with logical vectors too. The logical vector is coerced into a integer vector with 1 for TRUE and 0 for FALSE. &gt; y &lt;- c(T,T,F) &gt; y ## [1] TRUE TRUE FALSE &gt; sum(y) # number of TRUE&#39;s in y ## [1] 2 &gt; mean(y) # proportion of TRUE&#39;s in y ## [1] 0.6666667 C.1.5 Subsetting You can access one or more elements in a vector by specifying a certain index of elements within the vector: vector[index]. &gt; w&lt;-c(30,50,20,60,40,20) &gt; length(w) ## [1] 6 The first element of w: &gt; w[1] ## [1] 30 The first and third elements: &gt; w[c(1,3)] ## [1] 30 20 All elements except the first and third: &gt; w[-c(1,3)] ## [1] 50 60 40 20 The elements that are at most 40: &gt; w[w &lt;= 40] ## [1] 30 20 40 20 The position of these elements that are at most 40: &gt; which(w &lt;= 40) ## [1] 1 3 5 6 The mean of w and the mean of only the elements in w that are less than or equal to 4: &gt; mean(w) ## [1] 36.66667 &gt; mean(w[w &lt;= 40]) ## [1] 27.5 Expressions involving inequalities create a logical vector which is TRUE when the expression is true: &gt; w &lt;= 40 ## [1] TRUE FALSE TRUE FALSE TRUE TRUE &gt; w == 40 ## [1] FALSE FALSE FALSE FALSE TRUE FALSE So when a vector is indexed by a TRUE/FALSE vector only the TRUE entries will be displayed (and used in any command involving this vector). Here is the logical vector for entries in w that are not equal to 40: &gt; w != 40 ## [1] TRUE TRUE TRUE TRUE FALSE TRUE Here are the values of the entries of w excluding those equal to 40: &gt; w[w != 40] ## [1] 30 50 20 60 20 Here is the sum of the values of the entries of w excluding those equal to 40: &gt; sum(w[w != 40]) ## [1] 180 Adding a logical (T/F) vector tells you how many elements in the vector are equal to TRUE. Here is the number of entries in w that are not equal to 40: &gt; sum(w != 40) ## [1] 5 Finally, the vector operators | and &amp; mean OR and AND, respectively. We can find the entries in w that are less than 30 OR greater than 50 with &gt; (w &lt; 30) | (w &gt; 50) ## [1] FALSE FALSE TRUE TRUE FALSE TRUE We can find the entries that are at most 50 AND at least 30 with &gt; (w &gt;= 30) &amp; (w &lt;= 50) ## [1] TRUE TRUE FALSE FALSE TRUE FALSE C.2 Data C.2.1 Reading Data into R The most common way to read data into R is by storing it in a comma separated values (.csv) format. Non-textbook data files for this class will either be on my webpage http://people.carleton.edu/~kstclair/data. You can read a .csv file into R using its URL or file path (which is system dependent): &gt; mydata &lt;- read.csv(&quot;&lt;data file path&gt;/mydata.csv&quot;) Alternatively, you can download (then upload if using mirage) a needed data set to your data folder located in your Mathxxx folder. Once this is done, and your Mathxxx project is started, you can easily read the data set into R using the command &gt; mydata &lt;- read.csv(&quot;data/mydata.csv&quot;) You don’t need an extended file path name because your project should set your working directory to your Mathxxx folder and the data folder containing you .csv is a subfolder in this working directory. Many textbooks have R packages that contain data sets used in the book. Here I’ll use the SDaA packge used in my Math 255 (Sampling) course. Once you load this library you have automatic access to add textbook data files identified by the name given in the book. &gt; # install.packages(&quot;SDaA&quot;) # only run this once, ever &gt; library(SDaA) &gt; class(agstrat) ## [1] &quot;data.frame&quot; The object agstrat is called a data frame. The rest of this handout will explain how R data frames can be explored, used and changed. C.2.2 Investigating a Data Frame You can see an entire data frame by typing its name. You can see the first or last 5 rows of a data frame with the following commands: &gt; head(agstrat) ## county state acres92 acres87 acres82 farms92 farms87 farms82 ## 1 PIERCE COUNTY NE 297326 332862 319619 725 857 865 ## 2 JENNINGS COUNTY IN 124694 131481 139111 658 671 751 ## 3 WAYNE COUNTY OH 246938 263457 268434 1582 1734 1866 ## 4 VAN BUREN COUNTY MI 206781 190251 197055 1164 1278 1464 ## 5 OZAUKEE COUNTY WI 78772 85201 89331 448 483 527 ## 6 CLEARWATER COUNTY MN 210897 229537 213105 583 699 693 ## largef92 largef87 largef82 smallf92 smallf87 smallf82 region rn ## 1 54 54 42 58 67 48 NC 805 ## 2 14 13 14 42 36 38 NC 241 ## 3 20 19 16 175 186 184 NC 913 ## 4 23 17 9 56 66 55 NC 478 ## 5 6 5 5 56 49 48 NC 1028 ## 6 34 32 23 8 19 13 NC 496 ## weight ## 1 10.23301 ## 2 10.23301 ## 3 10.23301 ## 4 10.23301 ## 5 10.23301 ## 6 10.23301 &gt; tail(agstrat) ## county state acres92 acres87 acres82 farms92 farms87 farms82 ## 295 FRANKLIN COUNTY WA 670149 660813 632519 857 894 856 ## 296 LEA COUNTY NM 2149450 2220431 2178568 544 561 534 ## 297 THURSTON COUNTY WA 59890 56799 67628 811 806 856 ## 298 CARSON CITY (IC) NV 5361 17859 18780 28 37 34 ## 299 BANNOCK COUNTY ID 325338 358189 352306 588 655 617 ## 300 LA PLATA COUNTY CO 587339 613579 589167 709 682 625 ## largef92 largef87 largef82 smallf92 smallf87 smallf82 region rn ## 295 127 140 120 107 109 101 W 371 ## 296 208 205 191 59 67 63 W 259 ## 297 5 4 4 171 143 151 W 394 ## 298 3 2 2 15 15 17 W 295 ## 299 79 81 83 98 112 106 W 148 ## 300 67 79 66 25 39 33 W 112 ## weight ## 295 10.29268 ## 296 10.29268 ## 297 10.29268 ## 298 10.29268 ## 299 10.29268 ## 300 10.29268 You can get the dimensions (# rows by # columns) and variable names is a data frame with &gt; dim(agstrat) ## [1] 300 17 You can see the variable names with &gt; names(agstrat) ## [1] &quot;county&quot; &quot;state&quot; &quot;acres92&quot; &quot;acres87&quot; &quot;acres82&quot; &quot;farms92&quot; ## [7] &quot;farms87&quot; &quot;farms82&quot; &quot;largef92&quot; &quot;largef87&quot; &quot;largef82&quot; &quot;smallf92&quot; ## [13] &quot;smallf87&quot; &quot;smallf82&quot; &quot;region&quot; &quot;rn&quot; &quot;weight&quot; or variable names and types with the structure command &gt; str(agstrat) ## &#39;data.frame&#39;: 300 obs. of 17 variables: ## $ county : Factor w/ 261 levels &quot;ALEXANDER COUNTY&quot;,..: 180 115 254 241 175 37 186 94 243 212 ... ## $ state : Factor w/ 46 levels &quot;AL&quot;,&quot;AR&quot;,&quot;AZ&quot;,..: 27 13 32 20 44 21 37 10 22 14 ... ## $ acres92 : int 297326 124694 246938 206781 78772 210897 507101 332358 402202 535359 ... ## $ acres87 : int 332862 131481 263457 190251 85201 229537 552844 337990 396638 503582 ... ## $ acres82 : int 319619 139111 268434 197055 89331 213105 541015 355823 400466 513458 ... ## $ farms92 : int 725 658 1582 1164 448 583 321 986 1249 488 ... ## $ farms87 : int 857 671 1734 1278 483 699 371 1065 1251 518 ... ## $ farms82 : int 865 751 1866 1464 527 693 341 1208 1320 571 ... ## $ largef92: int 54 14 20 23 6 34 163 56 86 216 ... ## $ largef87: int 54 13 19 17 5 32 180 36 78 204 ... ## $ largef82: int 42 14 16 9 5 23 176 42 69 193 ... ## $ smallf92: int 58 42 175 56 56 8 10 90 42 16 ... ## $ smallf87: int 67 36 186 66 49 19 24 115 38 37 ... ## $ smallf82: int 48 38 184 55 48 13 16 132 28 24 ... ## $ region : Factor w/ 4 levels &quot;NC&quot;,&quot;NE&quot;,&quot;S&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ rn : int 805 241 913 478 1028 496 969 42 676 383 ... ## $ weight : num 10.2 10.2 10.2 10.2 10.2 ... You can view the data frame in Rstudio’s viewer window with &gt; View(agstrat) C.2.3 Accessing Data You can also access and edit information in a data frame by subscripting the data frame. Suppose you want to look at the variable farms92 (the number of farms per county in 1992). This variable is the 6th column in the data frame. You can access its contents with either command: &gt; agstrat[,6] ## [1] 725 658 1582 1164 448 583 321 986 1249 488 1308 657 974 780 ## [15] 293 201 362 309 500 530 491 305 1383 740 325 783 440 832 ## [29] 682 198 283 1000 547 953 771 979 427 963 545 942 949 544 ## [43] 822 955 1421 532 272 1669 308 401 171 480 1433 900 378 58 ## [57] 760 1216 1086 833 682 1280 1262 1029 1190 618 554 497 744 698 ## [71] 786 1305 1058 855 741 828 1140 509 759 1080 658 663 1447 1398 ## [85] 511 1529 623 742 386 468 738 759 746 680 792 327 777 205 ## [99] 1190 629 426 395 721 1367 659 249 550 440 438 74 668 147 ## [113] 488 1367 395 940 602 716 0 433 451 142 537 427 689 179 ## [127] 14 547 872 1444 549 345 235 406 40 705 169 394 219 774 ## [141] 74 561 290 414 781 1037 992 342 179 2760 56 315 49 1226 ## [155] 389 226 334 303 1152 403 2086 946 1342 612 407 986 199 17 ## [169] 704 1120 127 261 642 348 1360 297 404 114 1582 328 404 34 ## [183] 330 132 151 966 146 374 694 455 838 915 812 732 540 108 ## [197] 419 1609 417 560 1903 1956 270 433 617 910 298 288 456 199 ## [211] 507 772 476 113 440 110 1004 199 339 818 689 491 659 215 ## [225] 301 606 355 376 695 889 1234 532 195 711 515 1547 90 651 ## [239] 361 147 747 33 128 137 940 477 445 278 447 162 280 640 ## [253] 1579 29 1031 1006 320 849 1232 267 1441 23 850 612 733 451 ## [267] 179 641 233 661 495 195 508 3157 442 358 107 149 1696 1027 ## [281] 415 490 418 134 257 525 599 366 16 874 419 1054 1257 110 ## [295] 857 544 811 28 588 709 &gt; agstrat$farms92 ## [1] 725 658 1582 1164 448 583 321 986 1249 488 1308 657 974 780 ## [15] 293 201 362 309 500 530 491 305 1383 740 325 783 440 832 ## [29] 682 198 283 1000 547 953 771 979 427 963 545 942 949 544 ## [43] 822 955 1421 532 272 1669 308 401 171 480 1433 900 378 58 ## [57] 760 1216 1086 833 682 1280 1262 1029 1190 618 554 497 744 698 ## [71] 786 1305 1058 855 741 828 1140 509 759 1080 658 663 1447 1398 ## [85] 511 1529 623 742 386 468 738 759 746 680 792 327 777 205 ## [99] 1190 629 426 395 721 1367 659 249 550 440 438 74 668 147 ## [113] 488 1367 395 940 602 716 0 433 451 142 537 427 689 179 ## [127] 14 547 872 1444 549 345 235 406 40 705 169 394 219 774 ## [141] 74 561 290 414 781 1037 992 342 179 2760 56 315 49 1226 ## [155] 389 226 334 303 1152 403 2086 946 1342 612 407 986 199 17 ## [169] 704 1120 127 261 642 348 1360 297 404 114 1582 328 404 34 ## [183] 330 132 151 966 146 374 694 455 838 915 812 732 540 108 ## [197] 419 1609 417 560 1903 1956 270 433 617 910 298 288 456 199 ## [211] 507 772 476 113 440 110 1004 199 339 818 689 491 659 215 ## [225] 301 606 355 376 695 889 1234 532 195 711 515 1547 90 651 ## [239] 361 147 747 33 128 137 940 477 445 278 447 162 280 640 ## [253] 1579 29 1031 1006 320 849 1232 267 1441 23 850 612 733 451 ## [267] 179 641 233 661 495 195 508 3157 442 358 107 149 1696 1027 ## [281] 415 490 418 134 257 525 599 366 16 874 419 1054 1257 110 ## [295] 857 544 811 28 588 709 &gt; agstrat[,&quot;farms92&quot;] ## [1] 725 658 1582 1164 448 583 321 986 1249 488 1308 657 974 780 ## [15] 293 201 362 309 500 530 491 305 1383 740 325 783 440 832 ## [29] 682 198 283 1000 547 953 771 979 427 963 545 942 949 544 ## [43] 822 955 1421 532 272 1669 308 401 171 480 1433 900 378 58 ## [57] 760 1216 1086 833 682 1280 1262 1029 1190 618 554 497 744 698 ## [71] 786 1305 1058 855 741 828 1140 509 759 1080 658 663 1447 1398 ## [85] 511 1529 623 742 386 468 738 759 746 680 792 327 777 205 ## [99] 1190 629 426 395 721 1367 659 249 550 440 438 74 668 147 ## [113] 488 1367 395 940 602 716 0 433 451 142 537 427 689 179 ## [127] 14 547 872 1444 549 345 235 406 40 705 169 394 219 774 ## [141] 74 561 290 414 781 1037 992 342 179 2760 56 315 49 1226 ## [155] 389 226 334 303 1152 403 2086 946 1342 612 407 986 199 17 ## [169] 704 1120 127 261 642 348 1360 297 404 114 1582 328 404 34 ## [183] 330 132 151 966 146 374 694 455 838 915 812 732 540 108 ## [197] 419 1609 417 560 1903 1956 270 433 617 910 298 288 456 199 ## [211] 507 772 476 113 440 110 1004 199 339 818 689 491 659 215 ## [225] 301 606 355 376 695 889 1234 532 195 711 515 1547 90 651 ## [239] 361 147 747 33 128 137 940 477 445 278 447 162 280 640 ## [253] 1579 29 1031 1006 320 849 1232 267 1441 23 850 612 733 451 ## [267] 179 641 233 661 495 195 508 3157 442 358 107 149 1696 1027 ## [281] 415 490 418 134 257 525 599 366 16 874 419 1054 1257 110 ## [295] 857 544 811 28 588 709 If you just want the first two entries in farms92: &gt; agstrat[1:2,6] ## [1] 725 658 &gt; agstrat$farms92[1:2] ## [1] 725 658 The variable region is a categorical variable, or factor variable to R. We can see the levels of region with &gt; str(agstrat$region) ## Factor w/ 4 levels &quot;NC&quot;,&quot;NE&quot;,&quot;S&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... &gt; levels(agstrat$region) ## [1] &quot;NC&quot; &quot;NE&quot; &quot;S&quot; &quot;W&quot; So region has 4 levels called NC, NE, S, and W. Note that these levels are ordered alphabetically, which is typically done with factor variables from data sets that are read into R. C.2.4 Subsetting a Data Frame You can subset a data frame just as you can subset a vector (see the Basics handout). We might want to subset a data frame to extract certain columns (variables), or we may want to extract certain rows (observations), or some combination of both. Suppose you want a data frame that only contains the variables region and farms92: One way to do this is with the select command from the dplyr package: &gt; library(dplyr) &gt; agstrat2 &lt;- select(agstrat, region, farms92) &gt; str(agstrat2) ## &#39;data.frame&#39;: 300 obs. of 2 variables: ## $ region : Factor w/ 4 levels &quot;NC&quot;,&quot;NE&quot;,&quot;S&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ farms92: int 725 658 1582 1164 448 583 321 986 1249 488 ... Suppose you want a data frame that only contains data from the north central (NC) and west (W) regions. Here we use the dplyr command filter to specify the criteria that tells us that region should be either &quot;W&quot; or &quot;NC&quot;: &gt; agstrat3 &lt;- filter(agstrat, region %in% c(&quot;W&quot;, &quot;NC&quot;)) &gt; str(agstrat3) ## &#39;data.frame&#39;: 144 obs. of 17 variables: ## $ county : Factor w/ 261 levels &quot;ALEXANDER COUNTY&quot;,..: 180 115 254 241 175 37 186 94 243 212 ... ## $ state : Factor w/ 46 levels &quot;AL&quot;,&quot;AR&quot;,&quot;AZ&quot;,..: 27 13 32 20 44 21 37 10 22 14 ... ## $ acres92 : int 297326 124694 246938 206781 78772 210897 507101 332358 402202 535359 ... ## $ acres87 : int 332862 131481 263457 190251 85201 229537 552844 337990 396638 503582 ... ## $ acres82 : int 319619 139111 268434 197055 89331 213105 541015 355823 400466 513458 ... ## $ farms92 : int 725 658 1582 1164 448 583 321 986 1249 488 ... ## $ farms87 : int 857 671 1734 1278 483 699 371 1065 1251 518 ... ## $ farms82 : int 865 751 1866 1464 527 693 341 1208 1320 571 ... ## $ largef92: int 54 14 20 23 6 34 163 56 86 216 ... ## $ largef87: int 54 13 19 17 5 32 180 36 78 204 ... ## $ largef82: int 42 14 16 9 5 23 176 42 69 193 ... ## $ smallf92: int 58 42 175 56 56 8 10 90 42 16 ... ## $ smallf87: int 67 36 186 66 49 19 24 115 38 37 ... ## $ smallf82: int 48 38 184 55 48 13 16 132 28 24 ... ## $ region : Factor w/ 4 levels &quot;NC&quot;,&quot;NE&quot;,&quot;S&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ rn : int 805 241 913 478 1028 496 969 42 676 383 ... ## $ weight : num 10.2 10.2 10.2 10.2 10.2 ... Note one problem with this new data frame: the region variable still thinks it has 4 levels even though S and NE are not in this data frame &gt; levels(agstrat3$region) ## [1] &quot;NC&quot; &quot;NE&quot; &quot;S&quot; &quot;W&quot; &gt; table(agstrat3$region) ## ## NC NE S W ## 103 0 0 41 This could create a problem when we want to use the region variable in future analyzes. An easy solution exists using the droplevels command on the data frame &gt; agstrat3 &lt;- droplevels(agstrat3) &gt; table(agstrat3$region) ## ## NC W ## 103 41 C.2.5 Creating a data frame One way to create a data frame is to create vectors that will form the variables (columns), then binding them together in a data frame: &gt; x &lt;- 1:10 &gt; y &lt;- rep(c(&quot;a&quot;,&quot;b&quot;),c(5,5)) &gt; my.data &lt;- data.frame(x=x,y=y) &gt; str(my.data) ## &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ x: int 1 2 3 4 5 6 7 8 9 10 ## $ y: Factor w/ 2 levels &quot;a&quot;,&quot;b&quot;: 1 1 1 1 1 2 2 2 2 2 &gt; my.data ## x y ## 1 1 a ## 2 2 a ## 3 3 a ## 4 4 a ## 5 5 a ## 6 6 b ## 7 7 b ## 8 8 b ## 9 9 b ## 10 10 b C.2.6 Adding a new column to a data frame Suppose you want to add a variable called w to the data frame my.data. &gt; w &lt;- rnorm(10, mean=0, sd=1) &gt; my.data &lt;- data.frame(my.data,w=w) &gt; my.data ## x y w ## 1 1 a 1.5761753 ## 2 2 a -2.0851922 ## 3 3 a 0.9193717 ## 4 4 a 0.2210263 ## 5 5 a -1.4635998 ## 6 6 b -1.2354740 ## 7 7 b 1.1735090 ## 8 8 b -1.6561852 ## 9 9 b -0.3900635 ## 10 10 b 0.7313721 C.2.7 Missing Data The missing data value in R is NA. Any blank field (or NA field) in the .csv file will be recognized as a missing value when the data is read into R with the read.csv command. But suppose we have missing data in a data set we’ve entered by hand &gt; u &lt;- c(NA,2,3,4,5,NA,7,8,9,10) &gt; v &lt;- c(rep(NA,5), 1:5) &gt; my.data &lt;- data.frame(my.data,u=u, v=v) &gt; my.data ## x y w u v ## 1 1 a 1.5761753 NA NA ## 2 2 a -2.0851922 2 NA ## 3 3 a 0.9193717 3 NA ## 4 4 a 0.2210263 4 NA ## 5 5 a -1.4635998 5 NA ## 6 6 b -1.2354740 NA 1 ## 7 7 b 1.1735090 7 2 ## 8 8 b -1.6561852 8 3 ## 9 9 b -0.3900635 9 4 ## 10 10 b 0.7313721 10 5 We can see which entries in u are missing with the is.na command &gt; is.na(my.data$u) ## [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE or with the summary command: &gt; summary(my.data) ## x y w u v ## Min. : 1.00 a:5 Min. :-2.08519 Min. : 2.00 Min. :1 ## 1st Qu.: 3.25 b:5 1st Qu.:-1.40657 1st Qu.: 3.75 1st Qu.:2 ## Median : 5.50 Median :-0.08452 Median : 6.00 Median :3 ## Mean : 5.50 Mean :-0.22091 Mean : 6.00 Mean :3 ## 3rd Qu.: 7.75 3rd Qu.: 0.87237 3rd Qu.: 8.25 3rd Qu.:4 ## Max. :10.00 Max. : 1.57618 Max. :10.00 Max. :5 ## NA&#39;s :2 NA&#39;s :5 We can use the drop_na command from the tidyr package to create an NA-free version of our data frame. Applying it to the entire data frame returns only rows that have observations for all variables: &gt; library(tidyr) &gt; my.data.noNA &lt;- drop_na(my.data) &gt; my.data.noNA ## x y w u v ## 7 7 b 1.1735090 7 2 ## 8 8 b -1.6561852 8 3 ## 9 9 b -0.3900635 9 4 ## 10 10 b 0.7313721 10 5 There are times when you only want to remove NA’s for a limited number of variables. Add these variable names as arguments to the drop_na command to only remove rows with NA’s for those variables. Here we only remove NAs from u (rows 1 and 6): &gt; my.data.noNAu &lt;- drop_na(my.data, u) &gt; my.data.noNAu ## x y w u v ## 2 2 a -2.0851922 2 NA ## 3 3 a 0.9193717 3 NA ## 4 4 a 0.2210263 4 NA ## 5 5 a -1.4635998 5 NA ## 7 7 b 1.1735090 7 2 ## 8 8 b -1.6561852 8 3 ## 9 9 b -0.3900635 9 4 ## 10 10 b 0.7313721 10 5 Sometimes data sets (especially “read-world”&quot; data) do not use blank fields to indicate missing data. For example, perhaps an unrealistic value is given as filler for a missing data point, like -99 for a positive integer variable or 9999 for a smaller scale variable. The source where you find your data should tell you if special fields (like -99 or 9999) are used to indicate missing data. Once you determine what the missing data indicator is, you can import the data set using the read.csv command with the added argument na.strings = c(&quot;-99&quot;,&quot; &quot;). This argument tells R that missing data is coded either as an NA, a blank entry or as a -99 entry. &gt; mydata &lt;- read.csv(&quot;&lt;file path&gt;&quot;, na.strings = c(&quot;NA&quot;, &quot; &quot;, &quot;-99&quot;)) C.3 EDA We are using the agstrat data frame from the SDaA package (see Section 1.5.2.1). C.3.1 Categorical: The table command is useful when summarizing a categorical variable like region &gt; table(agstrat$region) ## ## NC NE S W ## 103 21 135 41 There are 103 counties in the north central region, 21 in the northeast, 135 in the south, and 41 in the west. We get a contingency table by entering two categorical variables &gt; table(agstrat$state,agstrat$region) ## ## NC NE S W ## AL 0 0 5 0 ## AR 0 0 9 0 ## AZ 0 0 0 1 ## CA 0 0 0 1 ## CO 0 0 0 5 ## CT 0 1 0 0 ## FL 0 0 4 0 ## GA 0 0 15 0 ## HI 0 0 0 2 ## IA 10 0 0 0 ## ID 0 0 0 5 ## IL 16 0 0 0 ## IN 9 0 0 0 ## KS 11 0 0 0 ## KY 0 0 15 0 ## LA 0 0 3 0 ## MA 0 1 0 0 ## MD 0 0 2 0 ## ME 0 1 0 0 ## MI 6 0 0 0 ## MN 9 0 0 0 ## MO 10 0 0 0 ## MS 0 0 6 0 ## MT 0 0 0 7 ## NC 0 0 16 0 ## ND 2 0 0 0 ## NE 12 0 0 0 ## NJ 0 1 0 0 ## NM 0 0 0 2 ## NV 0 0 0 2 ## NY 0 8 0 0 ## OH 4 0 0 0 ## OK 0 0 7 0 ## OR 0 0 0 4 ## PA 0 8 0 0 ## SC 0 0 4 0 ## SD 7 0 0 0 ## TN 0 0 6 0 ## TX 0 0 31 0 ## UT 0 0 0 4 ## VA 0 0 5 0 ## VT 0 1 0 0 ## WA 0 0 0 7 ## WI 7 0 0 0 ## WV 0 0 7 0 ## WY 0 0 0 1 So, for example, the data contains 5 counties in Alabama that are classified as southern. (Note that this isn’t a very interesting summary of two categorical variables, just an easy one to demonstrate the table command using this data set.) C.3.2 Quantitative: Basic summary stats commands are &gt; summary(agstrat$farms92) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 326.5 544.5 637.4 840.8 3157.0 &gt; mean(agstrat$farms92) ## [1] 637.3833 &gt; median(agstrat$farms92) ## [1] 544.5 &gt; sd(agstrat$farms92) ## [1] 448.2621 &gt; min(agstrat$farms92) ## [1] 0 &gt; max(agstrat$farms92) ## [1] 3157 We can explore which county(s) have the highest number of farms (3157) in 1992 with &gt; which(agstrat$farms92 == 3157) ## [1] 274 &gt; agstrat[274,] ## county state acres92 acres87 acres82 farms92 farms87 farms82 ## 274 HAWAII COUNTY HI 926607 1007287 1172448 3157 2810 2539 ## largef92 largef87 largef82 smallf92 smallf87 smallf82 region rn ## 274 55 60 58 1960 1602 1468 W 142 ## weight ## 274 10.29268 The 0.05 and 0.95 quantiles (i.e. 5th and 95th percentiles) of farms92 are &gt; quantile(agstrat$farms92, c(.05, .95)) ## 5% 95% ## 89.20 1441.15 meaning that 5% of counties have fewer than 89.2 farms and 95% of counties have fewer than 1441.15 farms. C.3.3 Quantitative grouped by a categorical Suppose we want to know the average number of farms per county for each region. The R function tapply(var, grp, fun) will apply the function fun to the variable var for each group in grp and produces a table of output (hence the t in tapply). Here is this command in action for the farms variable &gt; tapply(agstrat$farms92, agstrat$region, summary) ## $NC ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 58.0 489.5 738.0 750.7 968.5 1669.0 ## ## $NE ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 395.0 451.0 528.1 659.0 1367.0 ## ## $S ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 14.0 265.5 440.0 578.6 777.5 2760.0 ## ## $W ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 16.0 257.0 495.0 602.3 733.0 3157.0 The average number of farms per county in the northeast region is 528.1. The R package dplyr can also be used to get numerical summaries by groups using the group_by and summarize commands. Here we string together these two commands with the piping command %&gt;% to get the mean and standard deviation of farms92 for each level of region: &gt; library(dplyr) &gt; agstrat %&gt;% + group_by(region) %&gt;% + summarize(mean(farms92), sd(farms92)) ## # A tibble: 4 x 3 ## region `mean(farms92)` `sd(farms92)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 NC 751. 358. ## 2 NE 528. 359. ## 3 S 579. 472. ## 4 W 602. 558. The output produced by this string of commands is actually a type of a data frame called a tibble. C.3.4 Graphs R has very sophisticated graphing capabilities. This handout just gives you a summary of some of the most basic graphs. More complicated graphic features will be explained as needed in class. R has high-level plotting commands that create a complete graph, low-level commands which add to an existing graph, and a graphing window layout par command. Use the help command to see these options, e.g. ?hist for the histogram options. A bar graph of the categorical variable region is given by &gt; barplot(table(agstrat$region)) The southern region contains the most counties in our sample (135) and the north east region the fewest counties (21). We can add a label to the y-axis and a title to the plot by adding the arguments &gt; barplot(table(agstrat$region), ylab=&quot;count&quot;, main=&quot;Number of counties per region&quot;) A histogram and boxplot of farms92 are given by &gt; hist(agstrat$farms92, main = &quot;Number of farms per county in 1992&quot;) &gt; boxplot(agstrat$farms92, main = &quot;Number of farms per county in 1992&quot;) We can get a side-by-side boxplot of farms92 by region with &gt; boxplot(farms92 ~ region, data = agstrat, main = &quot;Number of farms per county in 1992&quot;) Suppose we want to look at the distribution of counties across regions grouped by counties with fewer than 500 farms vs. 500 or more farms. First we need to create a factor variable that identifies counties as having less or more than 500 farms: &gt; agstrat$farms500 &lt;- ifelse(agstrat$farms92 &lt; 500, &quot;fewer than 500 farms&quot;, &quot;500 or more farms&quot;) &gt; table(agstrat$farms500) ## ## 500 or more farms fewer than 500 farms ## 164 136 The we create the stacked bar graph for farms500 grouped by region using ggplot2: &gt; library(ggplot2) &gt; ggplot(agstrat, aes(x=region, fill = farms500)) + + geom_bar(position = &quot;fill&quot;) + + labs(y=&quot;proportion&quot;, fill = &quot;Number of farms&quot;, + title = &quot;Number of farms (categorized) by region&quot;) &gt; prop.table(table(agstrat$region, agstrat$farms500), 1) ## ## 500 or more farms fewer than 500 farms ## NC 0.7281553 0.2718447 ## NE 0.4285714 0.5714286 ## S 0.4444444 0.5555556 ## W 0.4878049 0.5121951 Of the 103 counties in the North Central region, about 72.8% have 500 or more farms. Of the 135 counties in the Southern region, about 44.4% have 500 or more farms. We can also use ggplot2 to create histograms of farms92 by region: &gt; ggplot(agstrat, aes(x=farms92)) + + geom_histogram() + + facet_wrap(~region) + + labs(title = &quot;Number of farms by region&quot;) We can also use the ggplot2 package to get side-by-side boxplots grouped by a third variable. Here we can compare the distribution of total farm acreage in 1992 (acres92) by region for counties that have fewer than 500 farms vs. 500 or more farms: &gt; ggplot(agstrat, aes(x = farms500, y=acres92)) + + geom_boxplot() + + facet_wrap(~region) + + labs(title = &quot;Farm acres by number of farms and region&quot;) The relationship between median acreage across the four regions looks similar regardless of how many farms are present in a county (with western counties having the highest acreage). But for all four regions, it looks like the median acreage is highest for counties with fewer than 500 farms. Counties with fewer farms may tend to have larger farms than counties with more (smaller) farms across all four regions. C.3.5 Reporting Results Homework and reports should be done using an R Markdown document in RStudio (see Section D). If you do need to copy a graph from Rstudio into another document, use the Copy Plot to Clipboard option in the Export menu. C.4 Factor variables Section C.2.3 showed how to determine the levels of a factor variable. There are many more things you may want to do with a categorical variable that is a factor type. Here are a few hints for manipulating a factor. C.4.1 Renaming factor levels The R package forcats has a fct_recode command to rename the levels of your factor variable &gt; library(forcats) &gt; mydata &lt;- data.frame(myfac=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)) &gt; table(mydata$myfac) ## ## a b c d e ## 1 1 2 1 1 &gt; mydata$new_myfac &lt;- fct_recode(mydata$myfac, + &quot;Aman&quot; = &quot;a&quot;, + &quot;Barb&quot; = &quot;b&quot;, + &quot;Chad&quot; = &quot;c&quot;, + &quot;Daryl&quot; = &quot;d&quot;, + &quot;Eliza&quot; = &quot;e&quot;) &gt; table(mydata$new_myfac) # check work ## ## Aman Barb Chad Daryl Eliza ## 1 1 2 1 1 C.4.2 Recode a categorical variable with many levels Suppose you have a variable var with response levels strongly agree, agree, disagree, and strongly disagree. You want to create a new version of this variable by combining all agree and all disagree answers. Here is one way to do this: &gt; mydata$new_var &lt;- ifelse(mydata$var %in% c(&quot;strongly agree&quot;, &quot;agree&quot;), &quot;agree&quot;, &quot;disagree&quot;) Any row in the dataset where var is in the set of responses listed (c(&quot;strongly agree&quot;, &quot;agree&quot;)) will be coded at agree in the newvar. All other responses (disagree, and strongly disagree) will be recoded as disagree in the newvar. If you have lots of levels that you want to collapse into fewer (or you just don’t want to use the ifelse command), then you should use the forcats package command fct_collapse. Here we have a variable called myfac that has levels a-e that we want to collapose into new groups low (just level a), mid (levels b and c) and high (levels d and e) &gt; mydata &lt;- data.frame(myfac=c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)) &gt; mydata$new_myfac &lt;- fct_collapse(mydata$myfac, + low = c(&quot;a&quot;), + mid = c(&quot;b&quot;,&quot;c&quot;), + high = c(&quot;d&quot;,&quot;e&quot;)) &gt; table(mydata$myfac,mydata$new_myfac) ## ## low mid high ## a 1 0 0 ## b 0 1 0 ## c 0 2 0 ## d 0 0 1 ## e 0 0 1 Just make sure that original factor levels of myfac are correctly spelled in the right-hand side of the assignment expressions. C.4.3 Converting some factor levels to NAs Sometimes you have too many levels to handle in a factor variable. Collapsing many levels into fewer is one solution (3.1), or we can create a version of the data that ignores the levels we don’t want to analyze. One way to do this is to turn those levels in NA (missing values) that R usually ignores. We can do this in the read.csv command (see section 1.3) or we can do this in the fct_collapse command or fct_recode Here we convert the d and e responses in myfac to missing values, while all other levels stay the same: &gt; mydata$try1 &lt;- fct_recode(mydata$myfac, NULL = &quot;d&quot;, NULL = &quot;e&quot;) &gt; summary(mydata$try1) ## a b c NA&#39;s ## 1 1 2 2 We can use similar syntax in the fct_collapse to both collapse levels and turn d and e into NA: &gt; mydata$try2 &lt;- fct_collapse(mydata$myfac, + low = c(&quot;a&quot;), + mid = c(&quot;b&quot;,&quot;c&quot;), + NULL = c(&quot;d&quot;,&quot;e&quot;)) &gt; summary(mydata$try2) ## low mid NA&#39;s ## 1 3 2 C.4.4 Changing the order of levels You can reorder the levels of a factor variable. Suppose newmyfac has responses that are ordered low, mid, and high. You can rearrange the order of these levels using the forcats package is fct_relevel command: &gt; table(mydata$new_myfac) # first check original order and exact spelling ## ## low mid high ## 1 3 2 &gt; mydata$new_myfac2 &lt;- fct_relevel(mydata$new_myfac, &quot;high&quot;,&quot;mid&quot;,&quot;low&quot;) &gt; table(mydata$new_myfac2) ## ## high mid low ## 2 3 1 C.4.5 Recode a numerically coded categorical variable Suppose you have a variable quant that is a categorical variable that was numerically coded (e.g. a 1=a, 2=b, 3=c, etc). You will need to convert this to a factor variable to analyze it correctly. Here is one way to do this: &gt; library(dplyr) &gt; mydata$quant &lt;- c(1,2,3,3,4,5) &gt; mydata$quant &gt; mydata$quant_fac &lt;- fct_recode(factor(mydata$quant), + &quot;a&quot; = &quot;1&quot;, + &quot;b&quot; = &quot;2&quot;, + &quot;c&quot; = &quot;3&quot;, + &quot;d&quot; = &quot;4&quot;, + &quot;e&quot; = &quot;5&quot;) &gt; mydata$quant_fac C.4.6 Recode a factor into a numeric There are times that a quantitative variable (like age) turns up as a factor after you read your data into R. This is due to at least one response in the column being a text response (non-numeric). R then defaults this column to the factor type. Suppose you’ve identified all character (text) entries in a variable that need to be either recoded into a number or turned into an NA to be ignored. You can use the readr package’s command parse_number to convert a factor variable into a numeric variable with a “best guess” at how to do this. For the ages variable with “over 90”, we see that parse_number strips away the “over” text and just leaves the number 90: &gt; library(readr) &gt; ages &lt;- factor(c(20, 18, 45, 34,&quot;over 90&quot;)) &gt; ages ## [1] 20 18 45 34 over 90 ## Levels: 18 20 34 45 over 90 &gt; new.ages &lt;- parse_number(as.character(ages)) &gt; new.ages ## [1] 20 18 45 34 90 For this version of ages, the function pulls the numbers that occur prior to the first character (-): &gt; ages &lt;- factor(c(20, 18, 45, 34,&quot;90-100&quot;)) &gt; ages ## [1] 20 18 45 34 90-100 ## Levels: 18 20 34 45 90-100 &gt; new.ages &lt;- parse_number(as.character(ages)) &gt; new.ages ## [1] 20 18 45 34 90 Rather than 90, we may want the entry to be the midpoint between 90 and 100: &gt; library(dplyr) &gt; ages2 &lt;- recode(ages, &#39;90-100&#39; = &quot;95&quot;) &gt; ages2 ## [1] 20 18 45 34 95 ## Levels: 18 20 34 45 95 &gt; new.ages &lt;- parse_number(as.character(ages2)) &gt; new.ages ## [1] 20 18 45 34 95 Finally, if there is no numeric value in an entry then parse_number will recode it automatically into an NA and give you a warning that lets you know it did this action: &gt; ages &lt;- factor(c(20, 18, 45, 34,&quot;way old&quot;)) &gt; ages ## [1] 20 18 45 34 way old ## Levels: 18 20 34 45 way old &gt; new.ages &lt;- parse_number(as.character(ages)) &gt; new.ages ## [1] 20 18 45 34 NA ## attr(,&quot;problems&quot;) ## # A tibble: 1 x 4 ## row col expected actual ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5 NA a number way old &gt; summary(new.ages) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 18.00 19.50 27.00 29.25 36.75 45.00 1 "],
["markdown.html", "D R Markdown D.1 How to write an R Markdown document D.2 Changing R Markdown chunk evaluation behavior D.3 Creating a new R Markdown document D.4 Extra: Graph formatting D.5 Extra: Table formatting", " D R Markdown An R Markdown (.Rmd) file will allow you to integrate your R commands, output and written work in one document. You write your R code and explanations in the .Rmd file, the knit the document to a Word, HTML, or pdf file. A basic R Markdown file has the following elements: Header: this is the stuff in between the three dashes --- located at the top of your .Rmd file. A basic header should specify your document title, author and output type (e.g. word_document). Written work: Write up your work like you would in any word/google doc. Formatting is done with special symbols. E.g. to bold a word or phrase, place two asterisks ** at the start and end of the word or phrase (with no spaces). To get section headers use one or more hash tags # prior to the section name. R code: Your R commands are contained in one or more chunks that contains one or more R commands. A chunk starts with three backticks (to the left of your 1 key) combined with {r} and a chunk ends with three more backticks. See the image below for an example of a chunk that reads in the data files HollywoodMovies2011.csv. A R chunk that reads in a data file Important!! A common error that students run into when first using R Markdown is forgetting to put the read.csv command in their document. An R Markdown document must contain all commands needed to complete an analysis. This includes reading in the data! Basically, what happens during the knitting process is that a fresh version of an Rstudio environment is created that is completely separate from the Rstudio you see running in front of you. The R chunks are run in this new environment, and if you will encounter a Markdown error if you, say, try to use the movies data frame without first including the read.csv chunk shown in Figure 1. D.1 How to write an R Markdown document Write your commands in R chunks, not in the console. Run chunk commands using the suggestions in the Hints section below. Knit your document often. This allows you to catch errors/typos as you make them. You can knit a .Rmd by pressing the Knit button at the top of the doc. You can change output types (e.g. switch from HTML to Word) by typing in the preferred doc type in the header, or by using the drop down menu option found by clicking the down triangle to the right of the Knit button. You can run a line of code in the R console by putting your cursor in the line and selecting Run &gt; Run Selected Line(s). You can run all commands in a chunk by clicking the green triangle on the right side of the chunk. URLs can be embeded between &lt; and &gt; symbols. The image below shows a quick scrolling menu that is available by clicking the double triangle button at the bottom of the .Rmd. This menu shows section headers and available chunks. It is useful for navagating a long .Rmd file. Quick scroll through Markdown document D.2 Changing R Markdown chunk evaluation behavior The default setting in Rstudio when you are running chunks is that the “output” (numbers, graphs) are shown “inline” within the Markdown Rmd. For a variety of reasons, my preference is to have commands run in the console. To see the difference between these two types of chunk evaluation option, you can change this setting as follows: Select Tools &gt; Global Options. Click the R Markdown section and uncheck (if needed) the option Show output inline for all R Markdown documents. Click OK. Now try running R chunks in the .Rmd file to see the difference. You can recheck this box if you prefer the default setting. D.3 Creating a new R Markdown document I suggest using old .Rmd HW file as a template for a new HW assignment. But if you want to create a completely new docment: Click File &gt; New File &gt; R Markdown…. A window like the one shown below should appear. The default settings will give you a basic Markdown (.Rmd) file that will generate an HTML document. Click OK on this window. Opening a Markdown document You should now have an “Untitled1” Markdown file opened in your document pane of Rstudio. Save this file, renamed as “FirstMarkdown.Rmd”, somewhere on your computer. (Ideally in a Math215 folder!) On Mirage, save the file in the default location (which is your account folder on the mirage server). Now click the Knit HTML button on the tool bar at the top of your Markdown document. This will generate a “knitted” (compiled) version of this document. Check that there is now an HTML file named “FirstMarkdown.html” in the same location as your “FirstMarkdown.Rmd” file. D.4 Extra: Graph formatting The markdown .Rmd for this graph formatting section is linked here: https://kstclair.github.io/Rmaterial/Markdown/Markdown_GraphFormatting.Rmd The data set Cereals contains information on cereals sold at a local grocery store. &gt; # load the data set &gt; Cereals &lt;- read.csv(&quot;http://math.carleton.edu/Stats215/RLabManual/Cereals.csv&quot;) D.4.1 Adding figure numbers and captions To add captions to the figures you make you need to add the argument fig.cap=&quot;my caption&quot; to your R chunk that creates the figure. If you have two or more figures created in the R chunk then give the fig.cap argument a vector of captions. If you are knitting to a pdf, you don’t need to add “Figure 1”, etc. numbering to the figure captions (they will be numbered automatically). For HTML and Word output types, you need to manually number figures. D.4.2 Resizing graphs in Markdown Suppose we want to create a boxplot of calories per gram grouped by cereal type and a scatterplot of calories vs. carbs per gram. Here are the basic commands without any extra formatting that create Figures 1 and 2: ```{r, fig.cap=\"Figure 1: Distributions of calories per gram by cereal type\"} boxplot(calgram ~ type, data=Cereals, main=\"Calories by type\", ylab=\"Calories per gram\") ``` Figure D.1: Distributions of calories per gram by cereal type ```{r, fig.cap=\"Figure 2: Calories vs. Carbs per gram\"} plot(carbsgram ~ calgram, data=Cereals, main=\"Carbs vs Calories\") ``` Figure D.2: Calories vs. Carbs per gram We can add fig.height and fig.width parameters to the Markdown R chunk to resize the output size of the graph. The size inputs used here are a height of 3.5 inches and a width of 6 inches. The command below creates Figures 3 and 4. ```{r, fig.height=3.5, fig.width=5, fig.cap=c(\"Figure 3: Distributions of calories per gram by cereal type\",\"Figure 4: Calories vs. Carbs per gram\")} boxplot(calgram ~ type, data=Cereals, main=\"Calories by type\", ylab=\"Calories per gram\") plot(carbsgram ~ calgram, data=Cereals, main=\"Carbs vs Calories\") ``` &gt; boxplot(calgram ~ type, data=Cereals, main=&quot;Calories by type&quot;, ylab=&quot;Calories per gram&quot;) Figure D.3: Distributions of calories per gram by cereal type &gt; plot(carbsgram ~ calgram, data=Cereals, main=&quot;Carbs vs Calories&quot;) Figure D.4: Calories vs. Carbs per gram D.4.3 Changing graph formatting in R You can use the par command to change R’s graphical parameter settings for plots that are not made from ggplot2. There are many options that can be changed, but one of the most useful is to change the layout of the graphical output display. The argument mfrow (multi-frame row) is given a vector c(nr, nc) that draws figures in an nr (number of rows) by nc (number of columns) array. We can arrange our two graphs in a 1 by 2 display (1 row, 2 columns) with the command: &gt; par(mfrow=c(1,2)) &gt; boxplot(calgram ~ type, data=Cereals, main=&quot;Calories by type&quot;, ylab=&quot;Calories per gram&quot;) &gt; plot(carbsgram ~ calgram, data=Cereals, main=&quot;Carbs vs Calories&quot;) Figure D.5: Distribution of calories per gram by cereal type and calories vs. carbs per gram. D.4.4 Hiding R commands You can omit R commands from your final document by adding echo=FALSE to your R chunk argument. Any output produced by your command (graphical or numerical) will still be displayed. For example, the following command creates Figure 6, a boxplot of carbs per gram by cereal type. ```{r, echo=FALSE, fig.cap=\"Figure 6: Distributions of calories per gram and shelf placement by cereal type\", fig.height=3, fig.width=4} boxplot(carbsgram ~ type, data=Cereals, main=\"Carbs by type\", ylab=\"Carbs per gram\") ``` Figure D.6: Distributions of calories per gram and shelf placement by cereal type D.4.5 Global changes in graph format The R chunk options that control graph sizes and output features (like echo) can be set globally for all R chunks either in the header (like with fig.caption) or in an opts_chunk$set() command at the start of the .Rmd file. I usually opt for setting global features with the opts_chunk command which you often see at the start of my .Rmd files. Any global settings, like echo or fig.height, can be overridden locally by changing them in individual chunks. D.4.6 Comments: Markdown is very sensitive to spaces, or lack-there-of. If you get odd formatting issues, try adding a spaces between R chunks, paragrahs, lists, section headers, etc. For example, you always need a space between an R chunk or text and a section header. D.5 Extra: Table formatting The markdown .Rmd for this table formatting section is linked here: https://kstclair.github.io/Rmaterial/Markdown/Markdown_TableFormatting.Rmd This handout gives some basic ways to format numerical output produced in your R chunks. Some of the methods mentioned below might only work when knitting to a PDF. Additional info about formatting text in R Markdown can be found online: http://rmarkdown.rstudio.com/authoring_basics.html http://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf http://rmarkdown.rstudio.com/pdf_document_format.html In homework or R appendices, I expect to see both the commands and output produced by those commands as your “work” for a problem. But in reports, as in any formal research paper, you should not include R commands or output (except for graphs). This handout is designed, primarily, to help you format numerical results to present into your written reports. The data set Cereals contains information on cereals sold at a local grocery store. &gt; # load the data set &gt; Cereals &lt;- read.csv(&quot;http://math.carleton.edu/Stats215/RLabManual/Cereals.csv&quot;) D.5.1 Hiding R commands and R output As mentioned in the graph formatting handout, adding the chunk option echo=FALSE will display output (like graphs) produced by a chunk but not show the commands used in the chunk. You can stop both R commands and output from being displayed in a document by adding the chunk option include=FALSE. As you work through a report analysis, you may initially want to see all of your R results as you are writing your report. But after you’ve summarized results in paragraphs or in tables, you can then use the include=FALSE argument to hid your R commands and output in your final document. If you ever need to rerun or reevaluate your R work for a report, you can easily recreate and edit your analysis since the R chunks used in your original report are still in your R Markdown .Rmd file. D.5.2 Markdown tables The Markdown language allows you to construct simple tables using vertical lines | to separate columns and horizontal lines - to create a header. Make sure to include at least one space before and after your Markdown table or it will not format correctly. I can’t find an easy way to attached an automatic table number and caption to this type of table, so I’ve simply written (and centered) the table number and caption by hand for the table below. Suppose we want to present the 5-number summary of calories per gram by cereal type. The tapply command can be used to obtain these numbers. &gt; tapply(Cereals$calgram, Cereals$type, summary) ## $adult ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 3.208 3.519 3.399 3.667 4.600 ## ## $children ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.636 3.931 4.000 4.028 4.074 4.483 We can construct a table of stats by type “by hand” using simple markdown table syntax in our .Rmd file that is shown below: Type | Min | Q1 | Median | Q3 | Max ---- | --- | --- | --- | --- | --- Adult | 2.0 | 3.2 | 3.5 | 3.7 | 4.6 Children | 3.6 | 3.9 | 4.0 | 4.1 | 4.5 The knitted table produced is shown below: Type Min Q1 Median Q3 Max Adult 2.0 3.2 3.5 3.7 4.6 Children 3.6 3.9 4.0 4.1 4.5 D.5.3 Markdown tables via kable The R package knitr contains a simple table making function called kable. You can use this function to, say, show the first few rows of a data frame: &gt; library(knitr) &gt; kable(head(Cereals), digits=3, caption=&quot;Table 1: Cereals data (first 6 cases)&quot;) &gt; library(knitr) &gt; kable(head(Cereals), digits=3, caption=&quot;Cereals data (first 6 cases)&quot;) Table D.1: Cereals data (first 6 cases) brand type shelf cereal serving calgram calfatgram totalfatgram sodiumgram carbsgram proteingram GM children bottom Lucky Charms 30 4.000 0.333 0.033 0.007 0.833 0.067 GM adult bottom Cheerios 30 3.667 0.500 0.067 0.007 0.733 0.100 Kellogs children bottom Smorz 30 4.000 0.667 0.067 0.005 0.833 0.033 Kellogs children bottom Scooby Doo Berry Bones 33 3.939 0.303 0.030 0.007 0.848 0.030 GM adult bottom Wheaties 30 3.667 0.333 0.033 0.007 0.800 0.100 GM children bottom Trix 30 4.000 0.500 0.050 0.006 0.867 0.033 Or you can use kable on a two-way table of counts or proportions: &gt; kable(table(Cereals$brand, Cereals$type), caption=&quot;Table 2: Cereal brand and type&quot;) Table D.2: Cereal brand and type adult children GM 4 11 Kashi 6 0 Kellogs 4 13 Quaker 1 2 WW 2 0 D.5.4 The pander package The R package pander creates simple tables in R that do not need any additional formatting in Markdown. The pander() function takes in an R object, like a summary table or t-test output, and outputs a Markdown table. You can add a caption argument to include a table number and title. Here is a table for the summary of calories per gram: &gt; library(pander) &gt; pander(summary(Cereals$calgram), caption=&quot;Table 3: Summary statistics for calories per gram.&quot;) Table 3: Summary statistics for calories per gram. Min. 1st Qu. Median Mean 3rd Qu. Max. 2 3.636 3.929 3.779 4.031 4.6 Pander can format tables and proportion tables. Here is the table for cereal type and shelf placement (Table 4), along with the distribution of shelf placement by cereal type (Table 5). &gt; my.table &lt;- table(Cereals$type,Cereals$shelf) &gt; pander(my.table,round=3, caption=&quot;Table 4: Cereal type and shelf placement&quot;) Table 4: Cereal type and shelf placement bottom middle top adult 2 1 14 children 7 18 1 &gt; pander(prop.table(my.table,1),round=3, caption=&quot;Table 5: Distribution of shelf placement by cereal type&quot;) Table 5: Distribution of shelf placement by cereal type bottom middle top adult 0.118 0.059 0.824 children 0.269 0.692 0.038 Here are t-test results for comparing mean calories for adult and children cereals (Table 6): &gt; pander(t.test(calgram ~ type, data=Cereals), caption=&quot;Table 6: Comparing calories for adult and children cereals&quot;) Table 6: Comparing calories for adult and children cereals (continued below) Test statistic df P value Alternative hypothesis -4.066 18.45 0.0006942 * * * two.sided mean in group adult mean in group children 3.399 4.028 Here are chi-square test results for testing for an association between shelf placement and cereal type (Table 7). Note that the simulate.p.value option was used to give a randomization p-value since the sample size criteria for the chi-square approximation was not met. &gt; pander(chisq.test(my.table, simulate.p.value = TRUE),caption=&quot;Table 7: Chi-square test for placement and type&quot;) Table 7: Chi-square test for placement and type Test statistic df P value 28.63 NA 0.0004998 * * * Here are the basic results for the regression of carbs on calories (Table 8). &gt; pander(lm(carbsgram ~ calgram, data=Cereals), caption=&quot;Table 8: Regression of carbs on calories&quot;) Table 8: Regression of carbs on calories Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.1021 0.0804 1.27 0.2111 calgram 0.1798 0.02108 8.528 1.264e-10 D.5.5 The stargazer package The stargazer package, like pander, automatically generates Markdown tables from R objects. The stargazer function has more formatting options than pander and can generate summary stats from a data frame table. It can also provide nicely formatted comparisons between 2 or more regression models. See the help file ?stargazer for more options. You will need to add the R chunk option results='asis' to get the table formatted correctly. I also include the message=FALSE option in the chunk below that runs the library command to suppress the automatic message created when running the library command with stargazer. When you give stargazer a data frame, it gives you summary stats for all numeric variables in the data frame (Table 10): ```{r, results='asis', message=FALSE} library(stargazer) stargazer(Cereals, type=\"html\", title=\"Table 9: Default summary stats using stargazer\") ``` Table 9: Default summary stats using stargazer Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max serving 43 36.953 10.542 27 30 50 60 calgram 43 3.779 0.517 2 3.6 4.0 5 calfatgram 43 0.490 0.261 0.000 0.328 0.600 1.034 totalfatgram 43 0.053 0.031 0.000 0.033 0.063 0.121 sodiumgram 43 0.005 0.002 0.000 0.003 0.006 0.007 carbsgram 43 0.782 0.116 0.280 0.767 0.850 0.920 proteingram 43 0.082 0.057 0.030 0.034 0.097 0.267 The default table type is &quot;latex&quot; which is the format you want when knitting to a pdf document. When knitting to an html document we need to change type to &quot;html&quot;. Unfortunately, there is no type that works nicely with Word documents so you would be better off using pander if you want a Word document. Note: When using the latex type and knitting to a pdf, you will get an annoying stargazer message about the creation of your latex table. Include the argument header=FALSE in the stargazer command to suppress this message when knitting to a pdf. You can subset the Cereals data frame to only include the variables (columns) that you want displayed. In Table 11 we only see calories and carbs. You can also edit the summary stats displayed by specifying them in the summary.stat argument. See the stargazer help file for more stat options. &gt; stargazer(Cereals[,c(&quot;calgram&quot;,&quot;carbsgram&quot;)], + type=&quot;html&quot;, + title=&quot;Table 10: Five number summary stats&quot;, + summary.stat=c(&quot;max&quot;,&quot;p25&quot;,&quot;median&quot;,&quot;p75&quot;,&quot;max&quot;)) Table 10: Five number summary stats Statistic Max Pctl(25) Median Pctl(75) Max calgram 5 3.6 3.9 4.0 5 carbsgram 0.920 0.767 0.800 0.850 0.920 The stargazer package was created to display results of statistical models. Here is the basic display for the regression of carbs on calories (Table 12). The argument single.row puts estimates and standard errors (in parentheses) in one row. There are many options that can be tweaked, like including p-values or confidence intervals. &gt; my.lm &lt;- lm(carbsgram ~ calgram, data=Cereals) &gt; stargazer(my.lm, type=&quot;html&quot;, + title=&quot;Table 11: Regression of carbs on calories&quot;, + single.row=TRUE) Table 11: Regression of carbs on calories Dependent variable: carbsgram calgram 0.180*** (0.021) Constant 0.102 (0.080) Observations 43 R2 0.639 Adjusted R2 0.631 Residual Std. Error 0.071 (df = 41) F Statistic 72.721*** (df = 1; 41) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 Table 13 adds the argument keep.stat to specify that only sample size and \\(R^2\\) should be included in the table. See the help file for more options to this argument. &gt; stargazer(my.lm, type=&quot;html&quot;, + title=&quot;Table 12: Regression of carbs on calories&quot;, + single.row=TRUE, + keep.stat=c(&quot;n&quot;,&quot;rsq&quot;)) Table 12: Regression of carbs on calories Dependent variable: carbsgram calgram 0.180*** (0.021) Constant 0.102 (0.080) Observations 43 R2 0.639 Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 "],
["mathreview.html", "E Math review E.1 Linear equations E.2 Logarithms E.3 Exercises", " E Math review E.1 Linear equations E.2 Logarithms Let \\(b&gt;0\\) and \\(x&gt;0\\). The logarithm (base-\\(b\\)) of \\(x\\) is denoted \\(\\log_b(x)\\) and equal to \\[ \\log_b(x) = a \\] where \\(a\\) tells us what power we must raise \\(b\\) to to obtain the value \\(x\\): \\[ b^a = x \\] Easy examples are: \\(b=2\\), \\(x=8\\) and \\(a=3\\), \\[ \\log_2(8) = 3 \\] since \\(2^3 = 8\\). Or using base \\(b=10\\), then \\[ \\log_{10}(0.01) = -2 \\] since \\(10^{-2} = 0.01\\). Some basic facts logarithm facts are \\[ \\log_b(b) = 1 \\] since \\(b^1 = b\\) and \\[ \\log_b(1) = 0 \\] since \\(b^0 = 1\\). E.2.1 Interpreting logged variables Multiplicative changes in \\(x\\) result in additive changes in \\(\\log_b(x)\\). If \\(m&gt;0\\), then \\[ \\log_b(mx) = \\log_b(m) + \\log_b(x) \\] For example, a doubling of \\(x=8\\) results in an increase in \\(\\log_2(8)\\) of one unit: \\[ \\log_2(16) = \\log_2(2\\times 8) = \\log_2(2) + \\log_2(8) = 1 + 3 = 4 \\] More generally if we use a base-2 logarithm, a doubling of \\(x\\) results in an additive increase in \\(\\log(x)\\) of 1 unit: \\[ \\log_2(2\\times x) = \\log_2(2) + \\log_2(x) = 1 + \\log_2(x) \\] E.2.2 Inverse (i.e. reversing the log, getting rid of the log, …) The logarithm and exponential functions are inverses of one another. This means we can “get rid” of the log by calculating \\(b\\) raised to the logged-function: \\[ b^{\\log_b(x)} = x \\] This will be useful in regression when we have a linear relationship between logged-response \\(y\\) and a set of predictors. For example, suppose we know that \\[ \\log_2(y) = 3 + 5x + \\epsilon \\] To return this to an expression on the original (unlogged) scale of \\(y\\), we need take both sides raised to the base 2: \\[ 2^{\\log_2(y)} = 2^{3 + 5x+ \\epsilon} \\] Simplifying both sides gives \\[ y = 2^3 \\times 2^{5x} \\times x^{\\epsilon} \\] E.2.3 Logarithms in R The R function log gives the natural logarithm (base-\\(e\\)): &gt; log(2) ## [1] 0.6931472 &gt; log(exp(1)) ## [1] 1 Other common logarithm bases are base-2 and base-10: &gt; log2(8) ## [1] 3 &gt; log10(100) ## [1] 2 General bases can be added as an argument: &gt; log(49, base = 7) ## [1] 2 E.3 Exercises Write the following as the sum of two logarithms. Simplify as much as possible: \\(\\log_2(2x)\\) \\(\\log_2(0.5x)\\) \\(\\ln(2x)\\) where \\(\\ln\\) is the natural log (base-\\(e\\)) Write the following expressions in terms of \\(y\\), not \\(\\log(y)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3x\\) \\(\\log_{10}(y) = -2 + 0.4x\\) \\(\\ln(y) = 1 - 3x\\) Write the following expressions in terms of \\(y\\) and \\(x\\), not \\(\\log(y)\\) and \\(\\log(x)\\). Simplify as much as possible: \\(\\log_2(y) = 1 - 3\\log_2(x)\\) \\(\\ln(y) = -2 + 0.4\\ln(x)\\) \\(\\ln(y) = 1 - 3\\log_2(x)\\) Logarithmic model: Regression of \\(Y\\) on \\(\\log(x)\\) obtains the following estimated mean of \\(Y\\): \\[ \\hat{\\mu}(Y \\mid x) = 1 - 3 \\log_2(x) \\] What is the change in estimated mean response if we double the value of \\(x\\)? What is the change in estimated mean response if we triple the value of \\(x\\)? What is the change in estimated mean response if we reduce the value of \\(x\\) by 20%? Exponential model: Regression of \\(\\log_2(Y)\\) on \\(x\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = -2 + 0.4x \\] Write the median in terms of \\(Y\\) instead of \\(\\log_2(Y)\\). Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 1 unit? What is the percent change in estimated median response if we increase \\(x\\) by 1 unit? What is the multiplicative change in estimated median response if we decrease \\(x\\) by 2 units? What is the percent change in estimated median response if we decrease \\(x\\) by 2 units? Power model: Regression of \\(\\log_2(Y)\\) on \\(\\log_2(x)\\) obtains the following estimated median of \\(Y\\): \\[ \\hat{median}(\\log_2(Y) \\mid x) = 1 -3\\log_2(x) \\] Write the median in terms of \\(Y\\) and \\(x\\) instead of \\(\\log\\)s. Simplify as much as possible. What is the multiplicative change in estimated median response if we increase \\(x\\) by 50%? What is the percent change in estimated median response if we increase \\(x\\) by 50%? What is the multiplicative change in estimated median response if we reduce the value of \\(x\\) by 20%? What is the percent change in estimated median response if we reduce the value of \\(x\\) by 20%? "],
["references.html", "References", " References "]
]
